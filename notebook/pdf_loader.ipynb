{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65281483",
   "metadata": {},
   "source": [
    "RAG Pipelines - Data Ingestion to Vector DB Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98d7c6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader,PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9f7129a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 PDF files to process\n",
      "\n",
      "Processing: NIPS-2017-attention-is-all-you-need-Paper.pdf\n",
      "  ✓ Loaded 11 pages\n",
      "\n",
      "Processing: NLP research paper for Healthcare.pdf\n",
      "  ✓ Loaded 17 pages\n",
      "\n",
      "Processing: Quen3 Embedding research paper.pdf\n",
      "  ✓ Loaded 14 pages\n",
      "\n",
      "Processing: RAG_research_paper.pdf\n",
      "  ✓ Loaded 21 pages\n",
      "\n",
      "Total documents loaded: 63\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0c90586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\\nof continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\\nsequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\\nthe matrix of outputs as:\\nAttention(Q,K,V ) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = ∑dk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\\nwhere headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈Rdmodel×dk , WK\\ni ∈Rdmodel×dk , WV\\ni ∈Rdmodel×dv\\nand WO ∈Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0,xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 ·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel )\\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 15], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+ n·d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate= d−0.5\\nmodel ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_stepstraining steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps= 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\nResidual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\n7'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0 ·1020\\nGNMT + RL [31] 24.6 39.92 2.3 ·1019 1.4 ·1020\\nConvS2S [8] 25.16 40.46 9.6 ·1018 1.5 ·1020\\nMoE [26] 26.03 40.56 2.0 ·1019 1.2 ·1020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0 ·1020\\nGNMT + RL Ensemble [31] 26.30 41.16 1.8 ·1020 1.1 ·1021\\nConvS2S Ensemble [8] 26.36 41.29 7.7 ·1019 1.2 ·1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.0 2.3 ·1019\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α= 0.6 [31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='IEEE REVIEWS IN BIOMEDICAL ENGINEERING 1\\nNatural Language Processing for Smart Healthcare\\nBinggui Zhou, Guanghua Yang \\x00 , Zheng Shi, and Shaodan Ma \\x00\\nAbstract—Smart healthcare has achieved signiﬁcant progress\\nin recent years. Emerging artiﬁcial intelligence (AI) technologies\\nenable various smart applications across various healthcare\\nscenarios. As an essential technology powered by AI, natural\\nlanguage processing (NLP) plays a key role in smart healthcare\\ndue to its capability of analysing and understanding human\\nlanguage. In this work, we review existing studies that concern\\nNLP for smart healthcare from the perspectives of technique and\\napplication. We ﬁrst elaborate on different NLP approaches and\\nthe NLP pipeline for smart healthcare from the technical point\\nof view. Then, in the context of smart healthcare applications\\nemploying NLP techniques, we introduce representative smart\\nhealthcare scenarios, including clinical practice, hospital man-\\nagement, personal care, public health, and drug development. We\\nfurther discuss two speciﬁc medical issues, i.e., the coronavirus\\ndisease 2019 (COVID-19) pandemic and mental health, in which\\nNLP-driven smart healthcare plays an important role. Finally,\\nwe discuss the limitations of current works and identify the\\ndirections for future works.\\nIndex Terms—Natural Language Processing, Smart Health-\\ncare, Artiﬁcial Intelligence, NLP Techniques, Healthcare Appli-\\ncations\\nI. I NTRODUCTION\\nS\\nMART healthcare is a healthcare system that exploits\\nemerging technologies, such as artiﬁcial intelligence (AI),\\nblockchain, big data, cloud/edge computing, and the internet\\nof things (IOT), for realizing various intelligent systems to\\nconnect healthcare participants and promote the quality of\\nhealthcare [1]. Major participants in smart healthcare can\\nbe classiﬁed into three categories, i.e., the public, health-\\ncare service providers, and third-party healthcare participants.\\nRelated to the participants, representative smart healthcare\\nscenarios include smart homes, smart hospitals, intelligent\\nresearch and development for life science, health management,\\npublic health, rehabilitation therapy, and etc. Fig. 1 shows the\\nmajor participants, emerging technologies, and representative\\nscenarios of smart healthcare.\\nNatural language processing (NLP) is a subﬁeld of com-\\nputer science and artiﬁcial intelligence that is concerned with\\nBinggui Zhou is with the School of Intelligent Systems Science and\\nEngineering, Jinan University, Zhuhai 519070, China; and also with the State\\nKey Laboratory of Internet of Things for Smart City and the Department of\\nElectrical and Computer Engineering, University of Macau, Macao 999078,\\nChina.\\nGuanghua Yang is with the School of Intelligent Systems Science and\\nEngineering, Jinan University, Zhuhai 519070, China.\\nZheng Shi is with the School of Intelligent Systems Science and Engineer-\\ning, Jinan University, Zhuhai 519070, China; and also with the State Key\\nLaboratory of Internet of Things for Smart City, University of Macau, Macao\\n999078, China.\\nShaodan Ma is with the State Key Laboratory of Internet of Things for\\nSmart City and the Department of Electrical and Computer Engineering,\\nUniversity of Macau, Macao 999078, China.\\n\\x00 Corresponding authors: Guanghua Yang (ghyang@jnu.edu.cn), Shaodan\\nMa (shaodanma@um.edu.mo).\\nthe automatic analysis, representation and understanding of\\nhuman language [2]. NLP has become a hot research area\\nand has attracted widespread attention from many research\\ncommunities in the past several years. As human language\\nis a general form of data entry for intelligent systems, NLP\\nenables machines to understand human language and interact\\nwith humans, making it essential to smart healthcare.\\nThe main manifestations of natural language are text and\\nspeech, where text encompasses text records, articles, book\\nchapters, dictionaries, and so forth, while speech occurs in\\nhuman-human and human-machine dialogues. NLP has been\\ndeveloped for several decades following the early origin of\\nartiﬁcial intelligence in the 1950s. Approaches to conduct\\nNLP are generally divided into three categories: rule-based\\napproaches, statistical approaches, and deep learning-based\\napproaches. From the 1950s to 1980s, NLP research mainly\\nfocused on rule-based approaches, which required expertise in\\nboth computer science and linguistics to design rules that ﬁt\\nhuman language. However, even well-designed rules are quite\\nlimited for covering human language due to its ﬂexibility and\\ncomplex patterns. Since the 1980s, statistical NLP systems\\nhave been designed by extracting features from corpora using\\nstatistical and machine learning algorithms and have gradually\\nreplaced rule-based NLP systems due to their superiority in\\nperformance and robustness. With the early application of\\nthe neural probabilistic language model [3] and the rapid\\ndevelopment of deep learning since 2013, neural NLP, by\\nusing neural networks and large corpora for automated feature\\nlearning, has dominated current research and achieved SOTA\\nperformance of many NLP tasks.\\nIn smart healthcare, NLP is applied to process text data and\\nis associated with human-machine/human-human communica-\\ntion. The text data can be classiﬁed into 2 categories: clinical\\ntext and other text data. Clinical text comes from all clinical\\nscenarios and mainly comprises of unstructured text records\\nfrom electronic health record (EHR) systems, including med-\\nical notes, diagnostic reports, electronic prescriptions, and\\netc. Other text data include all text that appears within other\\nhealthcare scenarios, e.g., surveys in population screening and\\narticles for evidence-based reference. Communication is com-\\nmon in all smart healthcare scenarios, such as patient-provider\\ncommunication in clinical inquiry and human-robot interaction\\nin rehabilitation therapy, accompanied by applications such\\nas machine translations and user interfaces for rehabilitation\\nrobots.\\nAs well recognized, research on and applications of NLP\\nfor smart healthcare have received intensive attention in recent\\nyears. However, no study has offered a well-organized sum-\\nmary of existing works in a systematic way. In this paper, we\\nﬁrst provide a systematic review of NLP for smart healthcare\\n© 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media,\\nincluding reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers\\nor lists, or reuse of any copyrighted component of this work in other works.\\narXiv:2110.15803v3  [cs.CL]  26 Sep 2022'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 1, 'page_label': '2', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='2 IEEE REVIEWS IN BIOMEDICAL ENGINEERING\\nMajor Participants\\nThird Parties\\ngovernment\\n medical insurance\\nService Providers\\nhealthcare\\ninstitutions\\nlife science\\ncompanies\\nSmart\\nHealthcare\\nPublic\\nhealthy people patients\\nEmerging Technologies\\nAI\\n cloud computing\\nIoT\\n blockchain\\nRepresentative Scenarios\\nsmart hospitals\\nintelligent R&D\\nfor life science\\npublic health\\npromotion\\nhealth monitoringa\\nb\\nc\\nFig. 1. Smart healthcare. a, major participants in smart healthcare include the public, healthcare service providers, and third-party healthcare participants.\\nb, example emerging technologies enable smart healthcare applications include artiﬁcial intelligence, blockchain, cloud computing, the internet of things, and\\netc. c, representative smart healthcare scenarios include intelligent research and development for life science, public health promotion, smart hospitals, health\\nmonitoring, and etc.\\nfrom both technical and application perspectives. After that,\\nwe discuss two speciﬁc medical issues, i.e., coronavirus dis-\\nease 2019 (COVID-19) pandemic and mental health, in which\\nNLP-driven smart healthcare plays an important role. Finally\\nwe discuss the limitations of existing works, identify the future\\ndirections of applying NLP to smart healthcare, and close the\\nreview with some conclusions.\\nII. NLP FOR SMART HEALTHCARE FROM TECHNICAL\\nPERSPECTIVE\\nNLP has been undergoing continuous development since the\\n1950s. Studies on NLP for smart healthcare have also been\\nconducted for decades and have attracted increased attention\\nin recent years with the advancement of artiﬁcial intelligence\\nand general NLP. To connect existing works from technical\\nperspective, in this section, we ﬁrst introduce the three kinds of\\nNLP approaches and their representative algorithms, and then\\nintroduce the NLP pipeline for smart healthcare to show how\\nNLP techniques are used in real smart healthcare applications.\\nA. Comparisons of different NLP approaches\\nThe mainstream NLP approaches can be classiﬁed into three\\ncategories, i.e., rule-based NLP, statistical NLP and neural\\nNLP, which have different characteristics. Below, we discuss\\nthe advantage and disadvantages of the three categories and\\nintroduce the representative algorithms of them.\\nRule-based NLP approaches, e.g., pattern matching [4]\\nand parsing [5], could be quite accurate in speciﬁc cases if\\ndedicated studies by experts are conducted. In addition, rule-\\nbased NLP approaches are easy to interpret and understand.\\nHowever, rules are normally too limited to cover all cases\\nconsidering the ﬂexibility and complex patterns of human lan-\\nguage. In addition, rule-based NLP requires expertise in both\\ncomputer science and linguistics to design appropriate rules to\\nﬁt human language, hindering it from large-scale applications.\\nCurrently, rule-based approaches have been widely considered\\nobsolete by academia [6], and are occasionally used for better\\npreprocessing nowadays [7].\\nIn general, statistical NLP is superior to rule-based NLP in\\nperformance and robustness. However, it also requires domain\\nexpertise to create handcrafted features, and is therefore lim-\\nited to taking full advantage of available data and providing\\nenough accuracy in complex applications. Although statistical\\nNLP requires intensive feature engineering, it is this direct\\nfeature design that makes it transparent and interpretable as\\nrule-based NLP. In addition, statistical NLP does not rely on\\nlarge-scale datasets or large amounts of computational power,\\nand thus is much more efﬁcient than neural NLP. Furthermore,\\nrepresentative statistical NLP models, such as bag-of-words\\n[8], TF-IDF [9], [10], and n-gram [11]–[13], have different\\ncharacteristics. Bag-of-words is easy to implement, but it\\nonly considers the frequencies of words in a sentence, which\\nneglects the importance and sequential order of these words.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ZHOU et al.: NATURAL LANGUAGE PROCESSING FOR SMART HEALTHCARE 3\\nThrough the inverse document frequency, TF-IDF improves\\nthe measurement of a word’s importance, but still does not\\ntake sequential order information into consideration. N-gram\\nconsiders n − 1 words before a word, which makes it more\\naccurate than bag-of-words but with higher computational\\ncomplexity (increases exponentially with n). It is worth men-\\ntioning that despite the dominance of deep learning in recent\\nyears, statistical NLP is still active in many healthcare studies\\nand applications.\\nRecent years have witnessed the success of neural NLP, who\\nhas shown better performance than both rule-based NLP and\\nstatistical NLP in applications with abundant available data.\\nHowever, neural NLP is often blamed for low interpretability\\nand dependence on expensive computing platforms. It is\\nalso worth noting that, compared with rule-based NLP and\\nstatistical NLP, neural NLP usually fails to achieve satisfac-\\ntory performance if limited data is available. Among neural\\nNLP models, recurrent neural network (RNN)-based models,\\nespecially long short-term memory (LSTM) [14]–[16]-based\\nmodels and gated recurrent unit (GRU)-based models [15],\\n[17], are more natural for processing sequential data such\\nas text and speech. They have the ability to remember his-\\ntorical information of the inputs, but suffer from gradient\\nvanishing/explosion, training issues and short-term memories.\\nConvolutional neural networks (CNN)-based models [18],\\n[19], combining with word embeddings, also show good\\nperformance in some tasks due to their ability in learning\\nlocal features and high computational efﬁciency which enables\\ndeep network architectures. Recently, graph neural network\\n(GNN)-based models have been applied to NLP-driven smart\\nhealthcare by incorporating knowledge from graph-structured\\nontology/entities [20]–[22]. When graphs are large in scale\\nor complex, GNN-based models are difﬁcult and costly to\\nimplement and train. Generally speaking, RNNs, CNNs, and\\nGNNs are all limited in tackling long-term dependencies in se-\\nquences. Through the self-attention mechanism, Transformer-\\nbased models [23], [24] are very efﬁcient in processing\\nlong sequences and support parallel training, but are lack\\nof ability in learning local features and position information.\\nWe have witnessed many combinations of the aforementioned\\nmodels for better feature extraction performance, including\\nCNN-LSTM networks [25], RNN-Attention networks [26],\\n[27], memory networks (MM) [28], [29], graph convolutional\\nnetworks (GCN) [30], CNN-LSTM-Attention networks [31],\\n[32], graph convolutional attention networks (GCAN) [33],\\n[34], etc. In addition, to further leverage large unlabelled\\ncorpora, pretraining, a very effective method, has been widely\\nexploited to obtain non-contextual or contextual embeddings\\n[35]. Word2vec [36]–[39], and GloVe (Global Vectors) [36],\\n[40], as representative algorithms of non-contextual embed-\\ndings, provide distributed dense vectors as word embeddings,\\nand outperform statistical algorithms such as bag-of-words and\\nn-gram. The non-contextual embedding for a word is static\\nand does not dynamically change as its context changes [35].\\nBased on the Transformer architecture, contextual embed-\\ndings, e.g., ELMo (Embeddings from Language Models) [41],\\nBERT (Bidirectional Encoder Representations from Trans-\\nformers) [42]–[45], and GPT (Generative Pre-Training) [46],\\nare developed to embed dynamic contextual information into\\nword embeddings, achieving outstanding performance than\\nother word embedding algorithms. It should be noted that these\\nmodels are typically huge and expensive to pre-train, which\\nsomehow constraints their broad application in healthcare.\\nThe comparisons of different NLP approaches and repre-\\nsentative algorithms are shown in Table I.\\nB. NLP pipeline for smart healthcare\\nAs shown in Fig. 2, there are three parts in an NLP pipeline\\nfor smart healthcare, i.e., preprocessing, feature extraction, and\\nmodelling. An NLP pipeline takes text or speech as illustrated\\nbefore as the input. After that, preprocessing is conducted\\nconsidering various inputs and their qualities to facilitate\\nfeature extraction and modelling. As the most important step,\\nfeature extraction is essential to NLP, which undoubtedly\\nexplains the attention it has received from researchers. Finally,\\nmodels for speciﬁc NLP tasks are built with the extracted\\nfeatures to yield the outputs accordingly.\\n1) Preprocessing: Preprocessing, including the procedures\\nof tokenization, stemming, lemmatization, stopword removal,\\nand etc., makes natural language normalized, machine-\\nreadable, and easy for postprocessing. Text preprocessing\\nmostly paves the way for feature extraction and modelling,\\nsince many NLP tasks require normalized text input to guar-\\nantee accuracy and efﬁciency due to signiﬁcant challenges\\ncoming from the ﬂexibility of natural languages and the\\nwide variety of morphological variants of medical terms in\\nmedical text [47]–[49]. However, with the development of\\nneural NLP, some text preprocessing procedures have become\\nunnecessary and may even cause problems. For example,\\nremoving stopwords may lead to the loss of informative\\ncontext information when using the BERT pre-trained model\\n[50]. As the preprocessing of speech, such as denoising, is\\ntypically regarded as a problem in signal processing, we do\\nnot discuss it in detail here.\\n2) Feature extraction: Apart from the increase in accessible\\ndigital data and the advances in computing platforms such\\nas graphics processing units, the development of NLP is\\nlargely attributed to the improvement in feature design or\\nfeature extraction methods. Both rule-based approaches and\\nstatistical approaches require expertise for rule design [4],\\n[5] or feature engineering [8]–[13]. For neural NLP, auto-\\nmated feature extraction via varieties of neural networks [14]–\\n[34] have greatly improved the efﬁciency of data utilization\\nand feature extraction. Automated feature engineering can be\\nconducted directly according to the downstream tasks using\\nsupervised learning, unsupervised learning or reinforcement\\nlearning. In addition, pretraining is also widely used in NLP\\nto automatically extract features from large unlabelled corpora\\nvia self-supervised learning in a generative, contrastive or\\ngenerative-contrastive manner [51] before the downstream\\ntasks begin. The extracted features, known as contextual or\\nnon-contextual embeddings, may encompass features such as\\nlexical meanings, syntactic features, semantic features, and\\neven pragmatics, which contribute to downstream tasks [35].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 3, 'page_label': '4', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='4 IEEE REVIEWS IN BIOMEDICAL ENGINEERING\\nTABLE I\\nCOMPARISONS OF DIFFERENT NLP APPROACHES AND REPRESENTATIVE ALGORITHMS .\\nNLP Approach Feature Extrac-\\ntion Method\\nAdvantages and Disadvantages Representative Algorithms\\nRule-based NLP rule design\\nadvantages :\\n- could be quite accurate in speciﬁc\\ncases;\\n- easy to interpret and understand\\ndisadvantages :\\n- rules are too limited to cover all\\ncases considering the ﬂexibility and\\ncomplex patterns of human language;\\n- require expertise in both computer\\nand linguistics to ﬁt human language\\npattern matching [4] and parsing [5]\\nStatistical NLP hand-crafted fea-\\nture engineering\\nadvantages :\\n- superior to rule-based NLP in\\nperformance and robustness;\\n- good interpretability\\ndisadvantages :\\n- require domain expertise to create\\nhandcrafted features;\\n- limited to taking full advantage of\\navailable data and providing enough\\naccuracy in complex applications\\nbag-of-words [8]:\\n- easy to implement;\\n- neglects the importance and sequential order of words\\nTF-IDF [9], [10]:\\n- improves the measurement of a word’s importance;\\n- does not take sequential order information into consideration\\nn-gram [11]–[13]:\\n- more accurate than bag-of-words;\\n- high computational complexity (increasing exponentially with n)\\nNeural NLP automated\\nfeature extraction\\nadvantages :\\n- better performance than both\\nrule-based NLP and statistical NLP\\nin applications with abundant\\navailable data\\ndisadvantages :\\n- low interpretability;\\n- dependence on expensive computing\\nplatforms;\\n- usually fail to achieve satisfactory\\nperformance if limited data is\\navailable\\n1) RNN-based models (e.g., LSTM [14]–[16] and GRUs [15], [17]):\\n- more natural for processing text and speech input;\\n- capable of remembering historical information of the inputs;\\n- suffer from gradient vanishing/explosion, training issues and\\nshort-term memories\\n2) CNN-based models [18], [19]:\\n- able to learn local features;\\n- high computational efﬁciency;\\n- limited in tackling long-term dependencies in sequences\\n3) GNN-based models [20]–[22]:\\n- efﬁcient in incorporating knowledge from graph-structured\\nontology/entities;\\n- limited in tackling long-term dependencies in sequences;\\n- difﬁcult and costly to implement and train with large-scale\\nor very complex graphs\\n4) Transformer-based models [23], [24]:\\n- efﬁcient in processing long sequences and parallel training;\\n- lack of ability in learning local features and position information\\n5) combinations: CNN-LSTM [25], RNN-Attention [26], [27],\\nMN [28], [29], GCN [30], CNN-LSTM-Attention [31], [32],\\nand GCAN [33], [34], etc.\\n6) non-contextual embedding-oriented pre-trained models\\n(word2vec [36]–[39], GloVe [36], [40]):\\n- outperform statistical algorithms;\\n- the non-contextual embedding for a word is static and will not\\ndynamically change as its context change\\n7) contextual embedding-oriented pre-trained models (ELMo [41],\\nBERT [42]–[45], GPT [46]):\\n- able to embed dynamic contextual information into word embeddings;\\n- outstanding performance than other word embedding algorithms;\\n- typically huge and expensive to pre-train'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ZHOU et al.: NATURAL LANGUAGE PROCESSING FOR SMART HEALTHCARE 5\\nInputs OutputsFeature ExtractionPreprocessing\\n• Text Classification\\n• Information Extraction\\n• Machine Translation\\n• Text Generation\\n• Information Retrieval\\n• Question Answering and Dialogue\\n• Knowledge Engineering\\n• Natural Language Understanding\\n• Causal Inference\\n• Speech Recognition\\n• Speech Synthesis\\n• …\\nModelling\\nFig. 2. The NLP pipeline for smart healthcare . There are three parts in an NLP pipeline for smart healthcare, i.e., preprocessing, feature extraction, and\\nmodelling. NLP takes text or speech as the input, followed by preprocessing to facilitate feature extraction and modelling. Features can be extracted with\\nvarious methods and models. Models for speciﬁc NLP tasks are ﬁnally built with the extracted features to yield the outputs.\\n3) Modelling: For various smart healthcare applications,\\ndifferent models should be built to accomplish various NLP\\ntasks, such as text classiﬁcation, information extraction, and\\nnatural language understanding. The extracted feature can be\\ndirectly processed by classiﬁers and regressors to yield outputs\\nfor simple tasks, e.g., medical text classiﬁcation [18], [52],\\nwhile further steps are required to complete complex tasks. In\\nthe following subsections, we ﬁrst introduce several text input-\\nbased NLP tasks according to their complexity. At the end of\\nthis section, we will introduce two speech-speciﬁc tasks, i.e.,\\nspeech recognition and speech synthesis.\\nInformation extraction. Information extraction (IE), a.k.a.\\ntext mining, enables harvesting information from text inputs,\\nand plays an important role in text analysis. Works related to\\ninformation extraction in smart healthcare focus on the ex-\\ntraction of diseases, drugs, events (mainly including temporal\\nexpressions, spatial expressions and participant information)\\nthrough name entity recognition [53], [54], relation extraction\\n[54]–[56], and event extraction [57] from medical text, includ-\\ning unstructured text in EHRs, articles, etc.\\nMachine translation . Machine translation (MT) aims to\\nautomatically translate text from one language to another\\n[58]. Currently, healthcare resources in various languages are\\nbecoming easily accessible as technologies evolve, and they\\nare all of great value in modern medical practice. Machine\\ntranslation therefore has drawn growing attention for building\\nbetter (multilingual) translation systems and further leveraging\\nmultilingual healthcare resources for other applications, either\\nto provide more accurate translations [59], [60] or to require\\nless time [60] than human translations.\\nText generation. Text generation (TG) automatically gener-\\nates text with given inputs while pursuing the goal of appearing\\nindistinguishable from human-written text. Speciﬁcally, there\\nare 3 kinds of inputs and corresponding subtasks in smart\\nhealthcare: text inputs (e.g., routine reports) associated with\\ntext summarization [61]–[63], question generation [64]–[66],\\ndialogue generation [67]–[69], and etc.; data inputs (e.g.,\\nneonatal intensive care data) connected with data-to-text [70];\\nand image inputs (e.g., medical images) related to image cap-\\ntioning [71], [72], visual question answering (VQA) [73]–[75],\\nand etc. Note that for data-to-text and image-to-text generation,\\na combination of NLP with data analysis or computer vision\\nis generally required, respectively.\\nInformation retrieval . Information retrieval (IR) obtains\\nmaterials that meet the query requirements from numerous\\ndocuments, and is a core of search engines for all applications.\\nTo ease the retrieval process [76], [77], improve the relevance\\nand diversity of the retrieval [78]–[80] or reduce the query\\ntime [81], current works aim to develop fast and efﬁcient\\ninformation retrieval methods to obtain useful retrieval from a\\nlarge collection of data sources, ranging from internal health\\ninformation system (HIS) systems and other digital documents\\nto online resources.\\nQuestion answering and dialogue systems . Question an-\\nswering (QA) involves automatically providing answers to\\nquestions raised by humans in a natural language. Question\\nanswering requires the machine to understand natural language\\nand infer the answers, making it highly dependent on natural\\nlanguage understanding and information retrieval. To date,\\nQA systems for healthcare have developed from information\\nretrieval based QA systems [82]–[85] and knowledge-based\\nQA systems [86]–[89] to hybrid QA systems [90], [91].\\nCompared to question answering, dialogue is also presented\\nin an interactive manner between humans and machines.\\nCommon dialogue systems in smart healthcare include task-\\noriented dialogue systems [92]–[94], and non-task-oriented\\n(a.k.a. chat-oriented) [95] dialogue systems, which assume\\ndifferent functions in various applications.\\nKnowledge engineering . Knowledge engineering (KE) is\\na ﬁeld within artiﬁcial intelligence that tries to construct and\\nuse knowledge-based systems [96]. It does not refer to a pure\\nNLP technique, but receives much attention in NLP for smart\\nhealthcare since medical text is one of the major sources\\nfor knowledge engineering. Within knowledge engineering,\\nknowledge acquisition and knowledge representation are cou-\\npling with information extraction, aiming at the acquisition\\nand representation of medical knowledge in a certain way,\\ne.g., knowledge graphs [97]–[99]. Besides, knowledge engi-\\nneering also concerns building knowledge-based systems to\\nexploit existing knowledge, such as knowledge-based ques-\\ntion answering (KBQA) systems [86]–[89], knowledge-based\\ninformation retrieval systems [100], text generation systems\\n[65], [101], etc.\\nNatural language understanding . Natural language un-\\nderstanding (NLU) focuses on machines’ comprehension of\\nhuman language in the form of unstructured text or speech.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='6 IEEE REVIEWS IN BIOMEDICAL ENGINEERING\\nMany of the aforementioned tasks, e.g., question answering,\\ninformation retrieval, require NLU to fully understand the in-\\nput queries. The difﬁculties of natural language understanding\\ncome from the diversity, ambiguity, and potential dependence\\nof natural language, making slow progress in natural language\\nunderstanding compared with other NLP techniques. After\\nyears of development in both general areas and smart health-\\ncare, the mainstream route of NLU is still to use various meth-\\nods to conduct slot ﬁlling and intent detection [102]–[104].\\nNLU is the core of multiple intelligent agents, assuming a\\nrole in understanding human intentions during human-machine\\ninteractions [102], [105], [106], medical queries [103], [104],\\netc.\\nCausal inference. Generally, causal inference is a discipline\\nconcerning the determination of actual effects of speciﬁc\\nthings, events or phenomena. Causal inference in NLP has\\nlong received insufﬁcient attention since the goal of classical\\nNLP applications is simply to make accurate predictions with\\nall available statistical correlations regardless of the underlying\\ncausal relationship [107]. Recently, with growing concerns\\nabout uninterpretable black box models, the importance of\\ncausal inference has gradually been recognized by NLP re-\\nsearchers, especially in the area of healthcare. Speciﬁcally, re-\\ncent advances of causal inference in NLP for smart healthcare\\nhave been made in uncovering causality from medical text\\n[108]–[110] and realizing reliable NLP-driven applications\\nwith discovered causal effects [108]–[110].\\nSpeech recognition and speech synthesis. Speech recogni-\\ntion (SR) aims to convert human speech into text information.\\nContrary to speech recognition, speech synthesis, a.k.a. text-to-\\nspeech (TTS), is concerned with representing text information\\nwith speech. Basically, SR-oriented and SS-oriented studies\\nattempt to build automatic computer systems for interconver-\\nsion between speech and text in the area of smart healthcare,\\nmaking human-machine interaction as natural and ﬂexible as\\nhuman-human interaction [111]. For speech recognition, these\\nefforts encompass the improvement in acoustic modelling\\n[112], [113], language modelling [114], and the whole system\\npipeline [115], [116] to enhance recognition accuracy. For\\nspeech synthesis, recent advancements have been made in\\ninvestigating and making synthesized speech natural [117],\\n[118], intelligible [119]–[123] and expressive [124]–[126],\\nwhich will help stimulate the enthusiasm of human-machine\\ninteraction [127].\\nIII. A PPLICATIONS OF NLP FOR SMART HEALTHCARE\\nNLP has been widely applied in smart healthcare and\\nhas brought dramatic improvements in many applications. As\\nshown in Fig. 3, a typical NLP-driven application is composed\\nof two parts: user interface (UI) and backend. The user\\nprovides text or speech input to the backend through the UI,\\nand then, the backend processes these inputs with the NLP\\nmodels and feeds the results back to the user by providing\\nspeciﬁc services through the UI. Knowledge bases are also\\nrequired at the backend for applications that essentially rely on\\nknowledge, for example, the aforementioned KBQA systems.\\nThe NLP techniques described in the previous section play a\\nkey role in both UI and backend.\\ntext\\n speech\\n speech\\nrecognition\\nmachine\\ntranslation\\nBackend\\nUser Interfaces\\nUser\\nSupporting \\nModels\\nKnowledge \\nBases\\nServices\\nInputs\\nFig. 3. Basic architecture of NLP-driven applications . A typical NLP-\\ndriven application is composed of user interface and backend, where the UI\\ntakes inputs from the user and feedback the results to the user, and the backend\\nprocesses these inputs with the NLP models with or without the knowledge\\nbases according to the speciﬁc task type.\\nThe UI enables information exchange between users and\\nintelligent systems through speech, text, etc. Easily accessible\\nUIs are critical for enhancing the experience of using intelli-\\ngent systems and realizing smart healthcare. Such user inter-\\nfaces can be implemented by using NLP techniques, especially\\nspeech recognition and natural language understanding.\\nAccording to their application scenarios, smart healthcare\\napplications employing NLP techniques can be classiﬁed into\\n5 major categories, i.e., clinical practice, hospital manage-\\nment, personal care, public health, and drug development. A\\nsummary of the applications and related NLP techniques is\\npresented in Table II. Below we introduce the ﬁve categories\\nin detail.\\nA. Clinical practice\\nClinical communication and data collection . Clinical\\ndata, including but not limited to demographics, medical\\nhistory, comorbidities, medical notes, physical examination\\nnotes, electronic recordings from medical devices, and clinical\\nlaboratory testing data and medical images [128], are the most\\nimportant data for diagnosis, treatment and even further retro-\\nspection. Patient-provider communication is an important way\\nto obtain ﬁrst-hand clinical data. When necessary, machine\\ntranslation may assist doctors in communicating with patients\\nwho speak different languages or have low literacy and limited\\nlevels of health education [129], [130]. Meanwhile, free text\\nnotes can be taken through speech recognition [131]–[133],\\nwhich can signiﬁcantly reduce medical staff’s time on labour-\\nintensive clinical documentation.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ZHOU et al.: NATURAL LANGUAGE PROCESSING FOR SMART HEALTHCARE 7\\nClinical decision support. Clinical decision support (CDS)\\nsystems can provide physicians with diagnosis and treatment\\nsuggestions, which play an increasingly important role in\\nclinical medicine with the surge of clinical cases and growing\\nconcerns regarding public healthcare. With the development of\\nquestion answering systems, clinical decision support based\\non question answering [84], [134], [135] has emerged and\\nbecome common, as it is closer to traditional patient-provider\\ncommunication. NLP techniques have shown great ability to\\nbuild clinical decision support systems by extracting various\\nuseful information for making diagnosis and therapeutic de-\\ncisions, such as family history information [136], entities and\\nrelations [137], [138], treatment and prognosis data [139],\\nclinical data concepts and features [140], and even causal\\nrelations [109], [110]. In addition, to ensure quality control\\nand future quality improvement, NLP can also contribute to\\nthe assessment of clinical procedures [141], [142], warning\\nof potentially harmful adverse drug events (ADEs) [143],\\ndisease symptoms [144], [145], and outcome-related causal\\neffects [146]. Finally, NLP is also powerful in enhancing\\nthe interpretability and reliability of clinical decision support\\nsystems for practical deployment, by providing supporting\\nevidence for diagnosis or treatment decisions in an evidence-\\nbased fashion [108], [147]–[150].\\nB. Hospital management\\nMedical resource allocation . Due to limited medical re-\\nsources, including hospital spaces, personnel, and materi-\\nals, efﬁcient resource allocation is critical in hospitals and\\nother medical facilities. By building patient triage systems,\\nmedical resources can attend to critical cases with priority\\nand enhance medical resource allocation effectiveness and\\nefﬁciency [151], [152]. Virtual assistants [153]–[155], hospital\\nautomation systems [156], [157] and collaborative robots\\n[158], [159] with voice control can further reduce the burden\\non medical staff, thereby improving hospital management\\nefﬁciency. There are also some interesting works that have\\nexplored the prediction of patient readmission to rearrange\\nmedical resources/interventions and reduce the readmission\\nrate [160]–[162]. In addition, by leveraging text generation\\ntechniques, part of text writing in healthcare, especially routine\\nreports, can be taken over by machines, freeing medical staff\\nfrom many administrative duties and making them available\\nfor direct patient care [70], [163].\\nData management . To manage large volumes of medical\\ndocumentation, text classiﬁcation, information extraction and\\ntext summarization can be used to generate category labels,\\ninformative keywords and simpliﬁed summaries [18], [52],\\n[62], [63], [164] for management, while information retrieval\\nsystems, especially those systems based on semantic search\\n[76], [165] and question answering [77], can be used in\\nhealthcare information systems to ease the retrieval process.\\nService quality control . Sentiment analysis with patient\\nexperience feedback will help hospitals improve their service\\nquality and patient experience. Such analysis required sub-\\nstantial personnel resources in the past, while NLP makes this\\nwork easier and greatly improves the efﬁciency of sentiment\\nanalysis [166]–[168].\\nC. Personal care\\nPersonal health assistants . Personal health assistants en-\\nable people to easily access useful medical information and\\nhealthcare services without visiting the healthcare institutions.\\nPersonal health assistants may incorporate several subsystems,\\nsuch as medical information access systems [169] and remote\\nhealthcare systems [170], for various purposes.\\nAssisting elderly individuals and disabled individuals .\\nNLP techniques can help elderly individuals and disabled\\nindividuals to greatly enhance their quality of life and so-\\ncial integration. V oice-controlled home automation systems\\nand robots may assist the elderly and the disabled in their\\ndaily lives [171], while robots (especially androids and other\\nrobots that communicate with people) can even encourage and\\naccompany them through social interactions [172], [173]. In\\naddition, NLP techniques are also of great value for providing\\nessential aids to people with various disabilities, e.g., speech\\nimpairments [122], [174]–[178], hearing loss [179], dyslexia\\n[180], or neurological disorders [119]–[121].\\nD. Public health\\nHealth knowledge popularization and medical educa-\\ntion. Health knowledge popularization and medical education\\nare essential public health interventions since they can im-\\nprove people’s health literacy and help them develop healthy\\nliving habits. Through knowledge engineering, accurate and\\ncomplete medical knowledge bases can be established to\\npromote the popularization of medical knowledge among the\\npopulation [86]–[89], [97]–[100], [181]. Speciﬁcally, people\\ncan easily access medical knowledge through question an-\\nswering systems [182], [183], information retrieval systems\\n[79], [81], and machine translation systems [1], [130], [184],\\n[185], facilitating the popularization and education of medical\\nknowledge. In addition, text generation techniques, such as\\nquestion generation and text summarization, can also be used\\nin medical education to generate medical case-based questions\\n[186] and construct simpliﬁed summaries [61].\\nPopulation screening. In addition to the health knowledge\\npopularization, population screening, which refers to the pro-\\ncess of assessing the prevalence of a disease or condition in\\na population or subgroup, is also an important intervention\\nfor delivering public health. The population screening starts\\nwith identifying target populations, followed by the screening\\ntest. After that, further actions such as further tests, advice, or\\ntreatment can be taken considering the screening results [187].\\nNLP can play two main roles in population screening. First,\\nNLP helps identify populations with higher health risk factors,\\nwhich may improve the efﬁciency of population screening\\n[188]. Second, NLP can also assist in the analysis of healthcare\\nquestionnaires and surveys [189], especially for open-ended\\nquestions.\\nE. Drug development\\nDrug discovery . NLP helps construct textual representa-\\ntions of biochemical entities for mapping the interactions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='8 IEEE REVIEWS IN BIOMEDICAL ENGINEERING\\nbetween diseases, drugs/chemical compounds, and biomacro-\\nmolecules (e.g., genes, proteins); predicting molecular prop-\\nerties; and designing novel molecules. Readers are referred to\\nthe comprehensive review by ¨Ozt¨urk et al [190] for a deeper\\nunderstanding of NLP methodologies for drug discovery.\\nPreclinical research. NLP techniques, especially informa-\\ntion extraction, are also able to identify the relations between\\nchemical structures and biological activity [191] and further\\nhelp researchers search for potentially effective chemical com-\\npounds, i.e., virtual screening [192], [193], in a huge chemical\\nspace. In addition, they are also applied in the prediction\\nof adverse drug reactions, including side effect prediction\\n[194], toxicity prediction [195], [196], and etc., in preclinical\\nresearch.\\nClinical research. Across the clinical research stage, NLP\\nmay enable efﬁcient clinical trial design [110], patient recruit-\\nment [197]–[199], clinical trial analytics [200], and etc.\\nDrug review and safety monitoring . Recently, the FDA\\nand other institutions have reported being interested in using\\nNLP for adverse drug event discovery and drug safety moni-\\ntoring [201]–[203], showing the full range of NLP’s key role\\nin drug development.\\nIV. NLP- DRIVEN SMART HEALTHCARE FOR SPECIFIC\\nMEDICAL ISSUES\\nNLP-driven smart healthcare plays an important role in\\nmany medical issues. In this section, we discuss how NLP-\\ndriven smart healthcare works in medical issues by taking two\\nspeciﬁc medical issues, i.e., COVID-19 pandemic and mental\\nhealth, as examples.\\nA. COVID-19 pandemic\\nWorldwide outbreak of COVID-19 has triggered an unprece-\\ndented global health crisis and has attracted much attention\\nfrom researchers [204]. No wonder, the COVID-19 pandemic\\nhas become one of the most inﬂuential medical issues over\\nthe past few years. In the COVID-19 pandemic, NLP-driven\\nsmart healthcare can be utilized for pandemic prevention,\\ndiagnosing, and drug development.\\nEarly forecasts of COVID-19 cases and pandemic knowl-\\nedge popularization are crucial to the prevention of the\\nCOVID-19 pandemic. In [205], an NLP module is embedded\\ninto an improved susceptible–infected model to build the\\nproposed hybrid AI model for COVID-19 prediction, showing\\nthat the forecasting accuracy of COVID-19 cases can be im-\\nproved by incorporating text inputs and with NLP techniques.\\nIn [206], the authors conclude that NLP techniques, e.g.,\\nNLP-aided information retrieval, literature-based discovery,\\nquestion answering and etc., can be applied to address the\\ninformation/knowledge needs of both researchers and the\\npublic in the COVID-19 pandemic.\\nIn clinical practice, NLP can be utilized to identify posi-\\ntively diagnosed COVID19 patients from free text narratives\\n[207], assess thoracic CT imaging reports [208], and identify\\nindividuals with the greatest risk of severe complications due\\nto COVID-19 [209], and provide COVID-19 testing advice\\n[210]. Such applications would be very useful to accelerate\\nthe diagnosis of COVID-19, mitigate its worst effects, and\\nalso reduce costs for combating the COVID-19 pandemic.\\nNLP has also been applied to drug development confronting\\nCOVID-19. In [211], the authors developed an NLP method\\nto automatically recognize the associations among potential\\ntargeted host organ systems, associated clinical manifestations\\nand pathways, and suggest potential drug candidates. NLP\\nmodels have also made great impacts in COVID-19 vaccine\\ndiscovery through protein interaction prediction, molecular\\nreaction modelling [212]. In addition, great opportunities\\nfor NLP can also be found in clinical design, regulatory\\ndecision-making, and pharmacovigilance [213]. These appli-\\ncations would signiﬁcantly reduce the time and cost of drug\\ndevelopment for COVID-19.\\nB. Mental health\\nThe mental health issues have received widespread and\\ncontinuously increasing attention for many years. Specially,\\nthe World Health Organization (WHO) claimed that the pan-\\ndemic and the resulting lockdowns, economic security, fear\\nand uncertainty would further cause devastating impacts on\\npeople’s mental health the world over in the past several\\nyears [214]. NLP-driven smart healthcare has great value in\\npredicting/diagnosing and treating mental health conditions.\\nNLP techniques have been applied to early predict or\\nidentify/screen various mental disorders, such as psychiatric\\nillness [215], late-life depression [216], severe mental illness\\n(schizophrenia, schizoaffective disorder and bipolar disorder)\\n[217]. In addition, some works have shown that NLP tech-\\nniques can predict risk-taking behaviours (e.g., suicide) with\\ngood discrimination [218], [219] so that early interventions\\ncan be taken to save lives. The data collected for such\\nanalysis may include text data such as social media posts,\\nscreening surveys, EHRs [220], and also speech data come\\nfrom narrative interviews [221], etc.\\nNLP techniques could also (automatically) provide effective\\npsychotherapeutic interventions through web-based psycho-\\neducational interventions, online counseling, etc., to augment\\ntherapist-based mental health interventions, showing potential\\nfuture opportunities for their integration into online men-\\ntal health tools [222]. For example, the insights of [223]\\ncould help improve counselor training and generate real-time\\ncounseling quality monitoring and answer suggestion support\\ntools. In addition, several mental health related areas that may\\nbeneﬁt from NLP techniques, including characterizing and\\nunderstanding mental disorders, measuring health outcomes,\\nstudying of social and occupational functioning, etc, were\\nshown in [219]. Speciﬁcally, [224] showed that the older\\nwould respond better to digital assistants employing a socially-\\noriented interaction style rather than the one with a task-\\noriented style, which is promising to promote mental health\\nin older adults by providing social interaction and company.\\nV. L IMITATIONS AND OUTLOOK\\nAlthough recent advancements in deep learning and neural\\nNLP have brought extraordinary enhancement to smart health-\\ncare, there are still some limitations that current methods have\\nyet to overcome.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ZHOU et al.: NATURAL LANGUAGE PROCESSING FOR SMART HEALTHCARE 9\\nTABLE II\\nAPPLICATIONS DRIVEN BY NLP IN ALL SMART HEALTHCARE SCENARIOS .\\nCategory Sub-Category Representative Applications Related Techniques\\nClinical Practice\\nclinical communication\\nand data collection\\npatient-provider communication [129], [130] machine translation\\nclinical documentation [131]–[133] speech recognition\\nclinical decision support\\nbuild QA-based clinical decision support systems [84], [134], [135] information extraction\\nbuild clinical decision support systems with extracted information: family\\nhistory information [136], entities and relations [137], [138], treatment and\\nprognosis data [139], clinical data concepts and features [140], causal relations\\n[109], [110]\\nquestion answering\\nhealthcare quality control: assess clinical procedures [141], [142], warning of\\nADE [143], disease symptoms [144], [145], and outcome-related causal effects\\n[146]\\ninformation extraction, causal in-\\nference\\nprovide supporting evidence for decisions under evidence-based fashion [108],\\n[147]–[150]\\ninformation retrieval, causal infer-\\nence\\nHospital Management\\nmedical resource allocation\\npatient triage [151], [152] information extraction\\nenable users to communicate and control intelligent systems through virtual\\nassistants [153]–[155], hospital automation systems [156], [157] and collabo-\\nrative robots [158], [159]\\nspeech recognition, natural lan-\\nguage understanding\\npredict and reduce readmission rate [160]–[162] information extraction\\nfree medical staff from routine text writing [70], [163] information extraction\\ndata management manage clinical documents [18], [52], [62], [63], [164] text generation\\nease the HIS retrieval process based on semantic search [76], [165] and question\\nanswering [77]\\ntext classiﬁcation, text summariza-\\ntion, information extraction\\nservice quality control improve service quality and patient experience [166]–[168] information retrieval, question an-\\nswering\\nPersonal Care\\npersonal health assistants access online medical information [169] information retrieval\\nenable remote healthcare [170] speech recognition\\nassisting the elderly and\\nthe disabled\\ndaily assistance [171] speech recognition, natural lan-\\nguage understanding\\nsocial interaction and company [172], [173] speech recognition, speech synthe-\\nsis\\nassist people with speech impairments [122], [174]–[178], hearing loss [179],\\ndyslexia [180], or neurological disorders [119]–[121]\\nspeech recognition, speech synthe-\\nsis\\nPublic Health\\nhealth knowledge\\npopularization and\\nmedical education\\nacquisition and representation of medical knowledge [86]–[89], [97]–[100],\\n[181]\\nknowledge engineering\\nease the access of medical knowledge [1], [79], [81], [130], [182], [184], [185]question answering, information\\nretrieval, machine translation\\ngenerate medical case-based questions [186] question generation\\nconstruct simpliﬁed summaries [61] text summarization\\npopulation screening identify target populations [188] information extraction\\nanalyse of healthcare questionnaire and surveys [189] information extraction\\nDrug Development\\ndrug discovery map the interactions between diseases, chemical compounds, and biomacro-\\nmolecules, predict molecular properties, and design novel molecules [190]\\ninformation extraction, information\\nretrieval, knowledge engineering\\npreclinical research drug screening [191]–[193] information extraction\\npredict adverse drug reactions: side effect prediction [194], and toxicity\\nprediction [195], [196]\\ninformation extraction\\nclinical research\\nclinical trial design [110] information extraction, causal in-\\nference\\npatient recruitment [197]–[199] information extraction\\nclinical trial analytics [200] information extraction\\ndrug review and safety\\nmonitoring\\nadverse drug events discovery and drug safety monitoring [201]–[203] information extraction\\nUnderstanding human language . Although substantial\\nefforts have been made to enable natural language understand-\\ning, the ﬂexibility of human language still makes full under-\\nstanding difﬁcult, especially when ambiguity in biomedical\\ntexts is encountered. Misunderstanding could lead to inaccu-\\nrate actions taken by robots, useless information returned by\\nengines, and even wrong decisions made by decision support\\nsystems, leading to economic loss, time wasting, and even\\nmore serious consequences.\\nInterpretability. Although applications that rely on neural\\nNLP to extract features and make decisions show excellent\\nperformance in real tasks, they are usually challenged by\\nusers due to their weakness in interpretability. Interpretability\\nis essential for smart healthcare applications, especially in\\nclinical scenarios that require quality assurance in cases of\\nlow conﬁdence. One of the major interpretability issues is\\nthat the learned features are usually not understood by hu-\\nmans. In addition, when tuning pre-trained language models\\nto downstream tasks, no enough intuitions on data for ﬁne-\\ntuning or types of applications can be given to guarantee good\\nperformance. Although efforts have been made to achieve\\ninterpretable NLP-driven applications, existing theories and\\nmethodologies are still not convincing and acceptable for\\nmany healthcare researchers and institutions. Before the inter-\\npretability issue is fully explored, the role of decision support\\nsystems in clinical practice can only be auxiliary from the\\nperspectives of medical ethics and practical application.\\nImplementation. There are still many issues concerning the\\nimplementation of NLP-driven applications in smart health-\\ncare. With the development of neural NLP, large deep neu-\\nral networks (e.g., pre-trained language models) have been\\nquickly migrated to smart healthcare. What followed are the\\nincreased requirements in computing power and training cost,\\nand the concerns about the reliability of neural NLP systems.\\nPatient privacy also prevents these models from achieving\\nmore prominent effects in smart healthcare for further practice.\\nThe consideration of medical ethics when applying such\\nsystems makes practical implementation more difﬁcult.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='10 IEEE REVIEWS IN BIOMEDICAL ENGINEERING\\nIn addition to tackling the aforementioned limitations, there\\nare some other directions to enhance existing NLP systems for\\nsmart healthcare.\\nCombining multiple NLP techniques . One direction to\\nenhance existing NLP systems can be the combination of\\nmultiple NLP techniques. For example, text generation can\\nwork as a data augmentation method for achieving comparable\\nresults in many applications with limited original data, such\\nas training QA systems [65], [85] and other clinically relevant\\ntasks [225], [226]. Through automatic question generation,\\nquestionnaires and surveys for population screening can be\\ngenerated from EHRs, which may outperform handcrafted\\nones. Machine translation has also proven beneﬁcial for var-\\nious text-based tasks by increasing the availability of mul-\\ntilingual healthcare information [227]–[229], implying the\\npossibility of improving the performance of current CDS\\nsystems. In addition, exploration of general knowledge and\\ndomain knowledge in the ﬁeld of NLP for smart healthcare\\ndeserves further attention and veriﬁcation.\\nEnd-to-end applications. Current NLP driven applications\\nfor smart healthcare usually focus on dealing with tasks step\\nby step and do not fully explore the feature extraction capa-\\nbility of advanced neural NLP for complex smart healthcare\\ntasks. A deeper integration of NLP techniques and healthcare\\napplications in an end-to-end manner can map the inputs\\nand outputs directly, signiﬁcantly simplify traditional pipelines\\nfor complex applications, eliminate the biases of intermediate\\ncomponents, and therefore achieve better performance. Taking\\npopulation screening as an example, although NLP has been\\napplied to identify populations and analyse screening test\\nresults in traditional screening procedures, NLP techniques can\\nbe further applied to build end-to-end population screening\\nsystems, with which the correlations between populations\\nand optimal actions can be found to improve the screening\\nperformance and the quality of healthcare. Another example\\nwould be reducing the readmission rate. As mentioned before,\\nsome works have revealed that NLP has the ability to predict\\npatient readmission, but further studies on providing appropri-\\nate interventions to reduce the readmission rate are not fully\\nconducted. We look forward to studies that integrate the two\\nparts to reveal every possibility for readmission rate reducing.\\nFew-shot learning and incorporating domain knowledge.\\nBy exploiting the learning capability of neural networks and\\nlarge available corpora, neural NLP has shown powerful ability\\nin learning language representations. However, for downstream\\ntasks or smart healthcare applications, there is still a long\\nway for NLP to go. Taking clinical decision support as an\\nexample, there are a lot of rare diseases with only a small\\nnumber of observations available for training a clinical deci-\\nsion support system to distinguish rare diseases from common\\ndiseases. This is a quite challenging task, especially when\\nthere are similar outcomes among some rare diseases and\\ncommon diseases. In addition, high-quality labelled data are\\nundoubtedly essential to guarantee task accuracy in developing\\npractical applications for smart healthcare. However, quality-\\ncontrolled annotation not only requires a large amount of\\ncost, but is also challenging due to the bias of experts’ level\\nof expertise. Therefore, even with well-learned pre-trained\\nlanguage models, few-shot learning algorithms and domain\\nknowledge are expected to be applied so that the ﬁne-tuned\\nmodels would be effective in learning from few rare disease\\nobservations or limited high-quality labelled data.\\nIncorporating multimodal and longitudinal data. Finally,\\nwe also anticipate future intelligent systems to utilize all avail-\\nable AI techniques, not only NLP, for practical applications\\nwith high accuracy and reliability. The past few years have\\nwitnessed the dominance of data-driven approaches in many\\napplications across various ﬁelds. NLP, computer vision, and\\nother machine learning algorithms can be applied to analyse\\nmedical text, medical images, electronic recordings (e.g., heart\\nsound), sensors data, laboratory results, and even genetic\\ninformation. With multimodal learning, useful information\\nextracted from these modalities can be combined together to\\nperfectly ﬁt the need for a complete and accurate analysis\\nof available healthcare data and patients’ health status. In\\naddition, all of these data and clinical events can be longi-\\ntudinal, where time series analysis can be applied to extract\\nlong-term dependencies and improve health care delivery. By\\ncombining these techniques to analyse multimodal and lon-\\ngitudinal data, future intelligent systems would become more\\npowerful and reliable for patients, physicians, and healthcare\\ninstitutions for applications such as 24/7 health monitoring,\\nchronic-condition management, healthy lifestyle promotion,\\nand precision medicine.\\nVI. C ONCLUSION\\nIn the context of smart healthcare, NLP takes text or speech\\nas the input in various scenarios involving humans and ma-\\nchines, and realizes the functions of analysing and understand-\\ning human language. In this paper, we review existing studies\\nconcerning NLP for smart healthcare from the perspectives\\nof technique and application. We elaborate on different NLP\\napproaches and the NLP pipeline for smart healthcare from the\\ntechnical point of view. Table I provides the comparisons of\\ndifferent NLP approaches and their representative algorithms.\\nVarious text-oriented and speech-oriented NLP tasks are elab-\\norated to conclude existing methodologies for tackling such\\ntasks. By introducing smart healthcare applications employing\\nNLP techniques in various smart healthcare scenarios (in-\\ncluding clinical practice, hospital management, personal care,\\npublic health, and drug development), we show the strength\\nand possibility of NLP for delivering smart healthcare. Table II\\nprovides a detailed list of representative applications in smart\\nhealthcare and their related NLP techniques. We further dis-\\ncuss two speciﬁc medical issues, i.e., COVID-19 pandemic and\\nmental health, in which NLP-driven smart healthcare plays an\\nimportant role. After that, we discuss the limitations of current\\nworks across understanding human language, interpretability,\\nand implementation of NLP systems for smart healthcare.\\nFinally, we identify several directions for future works, notably\\ncombining multiple NLP techniques, developing end-to-end\\napplications, few-shot learning, and incorporating multimodal\\nand longitudinal data.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ZHOU et al.: NATURAL LANGUAGE PROCESSING FOR SMART HEALTHCARE 11\\nREFERENCES\\n[1] S. Tian, W. Yang, J. M. L. Grange, P. Wang, W. Huang, and Z. Ye,\\n“Smart healthcare: Making medical care more intelligent,” Global\\nHealth Journal, vol. 3, no. 3, pp. 62–65, Sep. 2019.\\n[2] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent trends in\\ndeep learning based natural language processing,” arXiv:1708.02709\\n[cs], Nov. 2018.\\n[3] Y . Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural prob-\\nabilistic language model,” Journal of Machine Learning Research ,\\nvol. 3, no. Feb, pp. 1137–1155, 2003.\\n[4] J. Crim, R. McDonald, and F. Pereira, “Automatically annotating\\ndocuments with normalized gene lists,” BMC Bioinformatics , vol. 6,\\nno. 1, p. S13, May 2005.\\n[5] J. Vilares, M. A. Alonso, and M. Vilares, “Extraction of complex\\nindex terms in non-English IR: A shallow parsing based approach,”\\nInformation Processing & Management, vol. 44, no. 4, pp. 1517–1537,\\nJul. 2008.\\n[6] L. Chiticariu, Y . Li, and F. R. Reiss, “Rule-based information extraction\\nis dead! Long live rule-based information extraction systems!” in\\nProceedings of the 2013 Conference on Empirical Methods in Natural\\nLanguage Processing . Seattle, Washington, USA: Association for\\nComputational Linguistics, Oct. 2013, pp. 827–832.\\n[7] N. Kang, B. Singh, Z. Afzal, E. M. van Mulligen, and J. Kors,\\n“Using rule-based natural language processing to improve disease\\nnormalization in biomedical text,” Journal of the American Medical\\nInformatics Association : JAMIA , vol. 20, Oct. 2012.\\n[8] W.-H. Weng, K. B. Wagholikar, A. T. McCray, P. Szolovits, and H. C.\\nChueh, “Medical subdomain classiﬁcation of clinical notes using a\\nmachine learning-based natural language processing approach,” BMC\\nMedical Informatics and Decision Making , vol. 17, p. 155, Dec. 2017.\\n[9] D. Dessi, R. Helaoui, V . Kumar, D. R. Recupero, and D. Riboni, “TF-\\nIDF vs word embeddings for morbidity identiﬁcation in clinical notes:\\nAn initial study,” arXiv:2105.09632 [cs], Mar. 2020.\\n[10] O. Ozyegen, D. Kabe, and M. Cevik, “Word-level text highlighting\\nof medical texts forTelehealth services,” arXiv:2105.10400 [cs] , May\\n2021.\\n[11] M. Rahimian, J. L. Warner, S. K. Jain, R. B. Davis, J. A. Zerillo, and\\nR. M. Joyce, “Signiﬁcant and distinctive n-grams in oncology notes:\\nA text-mining method to analyze the effect of OpenNotes on clinical\\ndocumentation,” JCO Clinical Cancer Informatics, no. 3, pp. 1–9, Dec.\\n2019.\\n[12] A. Yazdani, R. Safdari, A. Golkar, and S. Rostam Niakan Kalhori,\\n“Words prediction based on N-gram model for free-text entry in\\nelectronic health records,” Health Information Science and Systems ,\\nvol. 7, Feb. 2019.\\n[13] V . Yip, M. Mete, U. Topaloglu, and S. Kockara, “Concept discovery for\\npathology reports using an N-gram model,” Summit on Translational\\nBioinformatics, vol. 2010, pp. 43–47, Mar. 2010.\\n[14] M. Beeksma, S. Verberne, A. van den Bosch, E. Das, I. Hendrickx,\\nand S. Groenewoud, “Predicting life expectancy with a long short-term\\nmemory recurrent neural network using electronic medical records,”\\nBMC Medical Informatics and Decision Making , vol. 19, no. 1, p. 36,\\nFeb. 2019.\\n[15] A. N. Jagannatha and H. Yu, “Bidirectional RNN for medical event\\ndetection in electronic health records,” in Proceedings of the 2016\\nConference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies . San\\nDiego, California: Association for Computational Linguistics, Jun.\\n2016, pp. 473–482.\\n[16] C. Liu, H. Sun, N. Du, S. Tan, H. Fei, W. Fan, T. Yang, H. Wu, Y . Li,\\nand C. Zhang, “Augmented LSTM framework to construct medical\\nself-diagnosis android,” in 2016 IEEE 16th International Conference\\non Data Mining (ICDM) , Dec. 2016, pp. 251–260.\\n[17] Y .-S. Zhao, K.-L. Zhang, H.-C. Ma, and K. Li, “Leveraging text\\nskeleton for de-identiﬁcation of electronic medical records,” BMC\\nMedical Informatics and Decision Making , vol. 18, no. Suppl 1, p. 18,\\nMar. 2018.\\n[18] M. Hughes, I. Li, S. Kotoulas, and T. Suzumura, “Medical text\\nclassiﬁcation using convolutional neural networks,” Studies in Health\\nTechnology and Informatics, vol. 235, pp. 246–250, 2017.\\n[19] X. Li, H. Wang, H. He, J. Du, J. Chen, and J. Wu, “Intelligent diagnosis\\nwith chinese electronic medical records based on convolutional neural\\nnetworks,” BMC Bioinformatics, vol. 20, no. 1, p. 62, Feb. 2019.\\n[20] Y . Li, B. Qian, X. Zhang, and H. Liu, “Graph neural network-based\\ndiagnosis prediction,” Big Data, vol. 8, no. 5, pp. 379–390, Oct. 2020.\\n[21] Z. Sun, H. Yin, H. Chen, T. Chen, L. Cui, and F. Yang, “Disease\\nprediction via graph neural networks,” IEEE Journal of Biomedical\\nand Health Informatics , vol. 25, no. 3, pp. 818–826, Mar. 2021.\\n[22] T. Wu, Y . Wang, Y . Wang, E. Zhao, and Y . Yuan, “Leveraging graph-\\nbased hierarchical medical entity embedding for healthcare applica-\\ntions,” Scientiﬁc Reports, vol. 11, no. 1, p. 5858, Mar. 2021.\\n[23] T. Mayer, E. Cabrio, and S. Villata, “Transformer-based argument\\nmining for healthcare applications,” in ECAI 2020 - 24th European\\nConference on Artiﬁcial Intelligence, Santiago de Compostela / Online,\\nSpain, Aug. 2020.\\n[24] D. Zhang, J. Thadajarassiri, C. Sen, and E. Rundensteiner, “Time-\\naware transformer-based network for clinical notes series prediction,”\\nin Machine Learning for Healthcare Conference . PMLR, Sep. 2020,\\npp. 566–588.\\n[25] S. Tokala, V . Gambhir, and A. Mukherjee, “Deep learning for social\\nmedia health text classiﬁcation,” in Proceedings of the 2018 EMNLP\\nWorkshop SMM4H: The 3rd Social Media Mining for Health Applica-\\ntions Workshop & Shared Task . Brussels, Belgium: Association for\\nComputational Linguistics, Oct. 2018, pp. 61–64.\\n[26] E. Choi, M. T. Bahadori, J. A. Kulas, A. Schuetz, W. F. Stewart, and\\nJ. Sun, “RETAIN: An interpretable predictive model for healthcare\\nusing reverse time attention mechanism,” arXiv:1608.05745 [cs], Feb.\\n2017.\\n[27] J. Chu, W. Dong, K. He, H. Duan, and Z. Huang, “Using neural\\nattention networks to detect adverse medical events from electronic\\nhealth records,” Journal of Biomedical Informatics , vol. 87, pp. 118–\\n130, Nov. 2018.\\n[28] P. Chakraborty, F. Wang, J. Hu, and D. Sow, “Explicit-blurred\\nmemory network for analyzing patient electronic health records,”\\narXiv:1911.06472 [cs, stat] , Jul. 2020.\\n[29] J. Song, Y . Wang, S. Tang, Y . Zhang, Z. Chen, Z. Zhang, T. Zhang,\\nand F. Wu, “Local–Global memory neural network for medication\\nprediction,” IEEE Transactions on Neural Networks and Learning\\nSystems, vol. 32, no. 4, pp. 1723–1736, Apr. 2021.\\n[30] H.-J. Yoon, J. Gounley, M. T. Young, and G. Tourassi, “Information ex-\\ntraction from cancer pathology reports with graph convolution networks\\nfor natural language texts,” in 2019 IEEE International Conference on\\nBig Data (Big Data) , Dec. 2019, pp. 4561–4564.\\n[31] R. Cai, B. Zhu, L. Ji, T. Hao, J. Yan, and W. Liu, “An CNN-LSTM\\nattention approach to understanding user query intent from online\\nhealth communities,” in 2017 IEEE International Conference on Data\\nMining Workshops (ICDMW), Nov. 2017, pp. 430–437.\\n[32] B. Tang, X. Wang, J. Yan, and Q. Chen, “Entity recognition in chinese\\nclinical text using attention-based CNN-LSTM-CRF,” BMC Medical\\nInformatics and Decision Making , vol. 19, no. 3, p. 74, Apr. 2019.\\n[33] E. Choi, Z. Xu, Y . Li, M. W. Dusenberry, G. Flores, Y . Xue, and A. M.\\nDai, “Learning the graphical structure of electronic health records with\\ngraph convolutional transformer,” arXiv:1906.04716 [cs, stat] , Jan.\\n2020.\\n[34] J. Wang, X. Chen, Y . Zhang, Y . Zhang, J. Wen, H. Lin, Z. Yang, and\\nX. Wang, “Document-level biomedical relation extraction using graph\\nconvolutional network and multihead attention: Algorithm development\\nand validation,” JMIR Medical Informatics , vol. 8, no. 7, p. e17638,\\nJul. 2020.\\n[35] X. Qiu, T. Sun, Y . Xu, Y . Shao, N. Dai, and X. Huang, “Pre-trained\\nmodels for natural language processing: A survey,” arXiv:2003.08271\\n[cs], Apr. 2020.\\n[36] A. L. Beam, B. Kompa, A. Schmaltz, I. Fried, G. Weber, N. P. Palmer,\\nX. Shi, T. Cai, and I. S. Kohane, “Clinical concept embeddings learned\\nfrom massive sources of multimodal medical data,” arXiv:1804.01486\\n[cs, stat], Aug. 2019.\\n[37] X. Cai, J. Gao, K. Y . Ngiam, B. C. Ooi, Y . Zhang, and X. Yuan, “Med-\\nical concept embedding with time-aware attention,” in Proceedings of\\nthe 27th International Joint Conference on Artiﬁcial Intelligence , ser.\\nIJCAI’18. Stockholm, Sweden: AAAI Press, Jul. 2018, pp. 3984–\\n3990.\\n[38] M. Kholghi, L. De Vine, L. Sitbon, G. Zuccon, and A. Nguyen, “The\\nbeneﬁts of word embeddings features for active learning in clinical\\ninformation extraction,” in Proceedings of the Australasian Language\\nTechnology Association Workshop 2016 , Melbourne, Australia, Dec.\\n2016, pp. 25–34.\\n[39] Y . Zhang, Q. Chen, Z. Yang, H. Lin, and Z. Lu, “BioWordVec,\\nimproving biomedical word embeddings with subword information and\\nMeSH,” Scientiﬁc Data, vol. 6, no. 1, p. 52, May 2019.\\n[40] S. Dubois, N. Romano, D. C. Kale, N. Shah, and K. Jung, “Effective\\nrepresentations of clinical notes,” arXiv:1705.07025 [cs, stat] , Aug.\\n2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='12 IEEE REVIEWS IN BIOMEDICAL ENGINEERING\\n[41] Q. Jin, B. Dhingra, W. W. Cohen, and X. Lu, “Probing biomedical\\nembeddings from language models,”arXiv:1904.02181 [cs], Apr. 2019.\\n[42] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang,\\n“BioBERT: A pre-trained biomedical language representation model\\nfor biomedical text mining,” Bioinformatics, vol. 36, no. 4, pp. 1234–\\n1240, Feb. 2020.\\n[43] K. Huang, J. Altosaar, and R. Ranganath, “ClinicalBERT: Modeling\\nclinical notes and predicting hospital readmission,” arXiv:1904.05342\\n[cs], Nov. 2020.\\n[44] L. Rasmy, Y . Xiang, Z. Xie, C. Tao, and D. Zhi, “Med-BERT: Pre-\\ntrained contextualized embeddings on large-scale structured electronic\\nhealth records for disease prediction,” npj Digital Medicine , vol. 4,\\nno. 1, pp. 1–13, May 2021.\\n[45] Y . Li, S. Rao, J. R. A. Solares, A. Hassaine, R. Ramakrishnan,\\nD. Canoy, Y . Zhu, K. Rahimi, and G. Salimi-Khorshidi, “BEHRT:\\nTransformer for electronic health records,” Scientiﬁc Reports, vol. 10,\\nno. 1, p. 7155, Apr. 2020.\\n[46] E. T. R. Schneider, J. V . A. de Souza, Y . B. Gumiel, C. Moro, and E. C.\\nParaiso, “A GPT-2 language model for biomedical texts in portuguese,”\\nin 2021 IEEE 34th International Symposium on Computer-Based\\nMedical Systems (CBMS) , Jun. 2021, pp. 474–479.\\n[47] A. Akkasi, E. Varo ˘glu, and N. Dimililer, “ChemTok: A new rule based\\ntokenizer for chemical named entity recognition,” BioMed Research\\nInternational, vol. 2016, 2016.\\n[48] H.-J. Dai, P.-T. Lai, Y .-C. Chang, and R. T.-H. Tsai, “Enhancing of\\nchemical compound and drug name recognition using representative\\ntag scheme and ﬁne-grained tokenization,”Journal of Cheminformatics,\\nvol. 7, no. Suppl 1 Text mining for chemistry and the CHEMDNER\\ntrack, p. S14, 2015.\\n[49] H. Liu, T. Christiansen, W. A. Baumgartner, and K. Verspoor, “Bi-\\noLemmatizer: A lemmatization tool for morphological processing of\\nbiomedical text,” Journal of Biomedical Semantics , vol. 3, p. 3, Apr.\\n2012.\\n[50] Y . Qiao, C. Xiong, Z. Liu, and Z. Liu, “Understanding the Behaviors\\nof BERT in Ranking,” arXiv:1904.07531 [cs], Apr. 2019.\\n[51] X. Liu, F. Zhang, Z. Hou, Z. Wang, L. Mian, J. Zhang, and J. Tang,\\n“Self-supervised learning: Generative or contrastive,” IEEE Transac-\\ntions on Knowledge and Data Engineering , pp. 1–1, 2021.\\n[52] Y . Wang, S. Sohn, S. Liu, F. Shen, L. Wang, E. J. Atkinson, S. Amin,\\nand H. Liu, “A clinical text classiﬁcation paradigm using weak supervi-\\nsion and deep representation,” BMC Medical Informatics and Decision\\nMaking, vol. 19, no. 1, p. 1, Jan. 2019.\\n[53] S. Hassanpour and C. P. Langlotz, “Information extraction from multi-\\ninstitutional radiology reports,” Artiﬁcial intelligence in medicine ,\\nvol. 66, pp. 29–39, Jan. 2016.\\n[54] N. Perera, M. Dehmer, and F. Emmert-Streib, “Named entity recog-\\nnition and relation detection for biomedical information extraction,”\\nFrontiers in Cell and Developmental Biology , vol. 8, p. 673, Aug.\\n2020.\\n[55] A. Thillaisundaram and T. Togia, “Biomedical relation extraction\\nwith pre-trained language representations and minimal task-speciﬁc\\narchitecture,” in Proceedings of The 5th Workshop on BioNLP Open\\nShared Tasks. Hong Kong, China: Association for Computational\\nLinguistics, Nov. 2019, pp. 84–89.\\n[56] S. ˇZitnik, M. ˇZitnik, B. Zupan, and M. Bajec, “Sieve-based relation\\nextraction of gene regulatory networks from biological literature,”BMC\\nBioinformatics, vol. 16, no. Suppl 16, p. S1, Oct. 2015.\\n[57] P. Jindal and D. Roth, “Extraction of events and temporal expressions\\nfrom clinical narratives,” Journal of Biomedical Informatics , vol. 46,\\npp. S13–S19, Dec. 2013.\\n[58] A. Garg and M. Agarwal, “Machine translation: A literature review,”\\narXiv:1901.01122 [cs], Dec. 2018.\\n[59] A. B ´erard, Z. M. Kim, V . Nikoulina, E. L. Park, and M. Gall ´e, “A\\nmultilingual neural machine translation model for biomedical data,” in\\nProceedings of the 1st Workshop on NLP for COVID-19 (Part 2) at\\nEMNLP 2020 . Online: Association for Computational Linguistics,\\nDec. 2020.\\n[60] K. Kirchhoff, A. M. Turner, A. Axelrod, and F. Saavedra, “Application\\nof statistical machine translation to public health information: A feasi-\\nbility study,” Journal of the American Medical Informatics Association\\n: JAMIA, vol. 18, no. 4, pp. 473–478, 2011.\\n[61] M. Afzal, F. Alam, K. M. Malik, and G. M. Malik, “Clinical Con-\\ntext–Aware biomedical text summarization using deep neural network:\\nModel development and validation,” Journal of Medical Internet Re-\\nsearch, vol. 22, no. 10, p. e19810, Oct. 2020.\\n[62] J. Lopez, “Automatic summarization of medical conversations, a\\nreview,” in TALN-RECITAL 2019-PFIA 2019 . Toulouse, France:\\nATALA, Jul. 2019, pp. 487–498.\\n[63] G. Manas, V . Aribandi, U. Kursuncu, A. Alambo, V . L. Shalin,\\nK. Thirunarayan, J. Beich, M. Narasimhan, and A. Sheth, “Knowledge-\\ninfused abstractive summarization of clinical diagnostic interviews:\\nFramework development study,” JMIR Mental Health , vol. 8, no. 5,\\np. e20865, May 2021.\\n[64] I. Pistol, D. Trandab ˘at,, and M. R ˘aschip, “Medi-test: Generating tests\\nfrom medical reference texts,” Data, vol. 3, no. 4, p. 70, Dec. 2018.\\n[65] S. Shen, Y . Li, N. Du, X. Wu, Y . Xie, S. Ge, T. Yang, K. Wang,\\nX. Liang, and W. Fan, “On the generation of medical question-answer\\npairs,” arXiv:1811.00681 [cs], Dec. 2019.\\n[66] W. Wang, T. Hao, and W. Liu, “Automatic question generation for\\nlearning evaluation in medicine,” in Advances in Web Based Learning\\n– ICWL 2007 , ser. Lecture Notes in Computer Science, H. Leung,\\nF. Li, R. Lau, and Q. Li, Eds. Berlin, Heidelberg: Springer, 2008, pp.\\n242–251.\\n[67] D. Li, Z. Ren, P. Ren, Z. Chen, M. Fan, J. Ma, and M. de Rijke, “Semi-\\nsupervised variational reasoning for medical dialogue generation,”\\nProceedings of the 44th International ACM SIGIR Conference on\\nResearch and Development in Information Retrieval , pp. 544–554, Jul.\\n2021.\\n[68] S. Lin, P. Zhou, X. Liang, J. Tang, R. Zhao, Z. Chen, and L. Lin,\\n“Graph-evolving meta-learning for low-resource medical dialogue gen-\\neration,” arXiv:2012.11988 [cs], Dec. 2020.\\n[69] W. Yang, G. Zeng, B. Tan, Z. Ju, S. Chakravorty, X. He, S. Chen,\\nX. Yang, Q. Wu, Z. Yu, E. Xing, and P. Xie, “On the generation of\\nmedical dialogues for COVID-19,” arXiv:2005.05442 [cs], Jun. 2020.\\n[70] S. Pauws, A. Gatt, E. Krahmer, and E. Reiter, “Making effective use of\\nhealthcare data using data-to-text technology,” arXiv:1808.03507 [cs],\\nAug. 2018.\\n[71] V . Kougia, J. Pavlopoulos, and I. Androutsopoulos, “A survey on\\nbiomedical image captioning,” arXiv:1905.13302 [cs], May 2019.\\n[72] Y . Xiong, B. Du, and P. Yan, “Reinforced transformer for medical\\nimage captioning,” in Machine Learning in Medical Imaging , ser.\\nLecture Notes in Computer Science, H.-I. Suk, M. Liu, P. Yan, and\\nC. Lian, Eds. Cham: Springer International Publishing, 2019, pp.\\n673–680.\\n[73] X. He, Z. Cai, W. Wei, Y . Zhang, L. Mou, E. Xing, and P. Xie,\\n“Towards visual question answering on pathology images,” inProceed-\\nings of the 59th Annual Meeting of the Association for Computational\\nLinguistics and the 11th International Joint Conference on Natural\\nLanguage Processing (Volume 2: Short Papers) . Online: Association\\nfor Computational Linguistics, Aug. 2021, pp. 708–718.\\n[74] L.-M. Zhan, B. Liu, L. Fan, J. Chen, and X.-M. Wu, “Medical visual\\nquestion answering via conditional reasoning,” in Proceedings of the\\n28th ACM International Conference on Multimedia, ser. MM ’20. New\\nYork, NY , USA: Association for Computing Machinery, Oct. 2020, pp.\\n2345–2354.\\n[75] B. Afrae, D. Yousra, A. Imane, B. A. Mohamed, and A. B. Abdel-\\nhakim, “A new visual question answering system for medical images\\ncharacterization,” in Proceedings of the 4th International Conference\\non Smart City Applications , ser. SCA ’19. New York, NY , USA:\\nAssociation for Computing Machinery, Oct. 2019, pp. 1–7.\\n[76] H. Wu, G. Toti, K. I. Morley, Z. M. Ibrahim, A. Folarin, R. Jackson,\\nI. Kartoglu, A. Agrawal, C. Stringer, D. Gale, G. Gorrell, A. Roberts,\\nM. Broadbent, R. Stewart, and R. J. Dobson, “SemEHR: A general-\\npurpose semantic search system to surface semantic data from clinical\\nnotes for tailored care, trial recruitment, and clinical research,” Journal\\nof the American Medical Informatics Association : JAMIA , vol. 25,\\nno. 5, pp. 530–537, Jan. 2018.\\n[77] J. Gobeill, A. Gaudinat, E. Pasche, D. Vishnyakova, P. Gaudet,\\nA. Bairoch, and P. Ruch, “Deep question answering for protein anno-\\ntation,” Database: The Journal of Biological Databases and Curation ,\\nvol. 2015, Sep. 2015.\\n[78] A. Montazeralghaem, R. Rahimi, and J. Allan, “Relevance ranking\\nbased on query-aware context analysis,” Advances in Information\\nRetrieval, vol. 12035, pp. 446–460, Mar. 2020.\\n[79] B. Xu, H. Lin, Y . Lin, Y . Ma, L. Yang, J. Wang, and Z. Yang,\\n“Improve biomedical information retrieval using modiﬁed learning to\\nrank methods,”IEEE/ACM Transactions on Computational Biology and\\nBioinformatics, vol. 15, no. 6, pp. 1797–1809, Nov. 2018.\\n[80] J. Urbain, O. Frieder, and N. Goharian, “Passage relevance models for\\ngenomics search,” BMC Bioinformatics , vol. 10, no. Suppl 3, p. S3,\\nMar. 2009.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ZHOU et al.: NATURAL LANGUAGE PROCESSING FOR SMART HEALTHCARE 13\\n[81] S. Mohan, N. Fiorini, S. Kim, and Z. Lu, “A fast deep learning\\nmodel for textual relevance in biomedical information retrieval,” in\\nProceedings of the 2018 World Wide Web Conference, ser. WWW ’18.\\nRepublic and Canton of Geneva, CHE: International World Wide Web\\nConferences Steering Committee, Apr. 2018, pp. 77–86.\\n[82] D. Hristovski, D. Dinevski, A. Kastrin, and T. C. Rindﬂesch, “Biomedi-\\ncal question answering using semantic relations,” BMC Bioinformatics,\\nvol. 16, no. 1, p. 6, Jan. 2015.\\n[83] V . Vinod, S. Agrawal, V . Gaurav, P. R, and S. Choudhary, “Multilingual\\nmedical question answering and information retrieval for rural health\\nintelligence access,” arXiv:2106.01251 [cs], Jun. 2021.\\n[84] M. A. H. Zahid, A. Mittal, R. C. Joshi, and G. Atluri, “CLINIQA:\\nA machine intelligence based clinical question answering system,”\\narXiv:1805.05927 [cs], May 2018.\\n[85] P. Wang, T. Shi, and C. K. Reddy, “Text-to-SQL generation for question\\nanswering on electronic medical records,” in Proceedings of The Web\\nConference 2020. Taipei Taiwan: ACM, Apr. 2020, pp. 350–361.\\n[86] Z. Jiang, C. Chi, and Y . Zhan, “Research on medical question an-\\nswering system based on knowledge graph,” IEEE Access, vol. 9, pp.\\n21 094–21 101, 2021.\\n[87] H. Liu, Q. Hu, Y . Zhang, C. Xing, and M. Sheng, “A knowledge-\\nbased health question answering system,” in Smart Health, ser. Lecture\\nNotes in Computer Science, H. Chen, D. D. Zeng, E. Karahanna, and\\nI. Bardhan, Eds. Cham: Springer International Publishing, 2017, pp.\\n286–291.\\n[88] D. Demner-Fushman and J. Lin, “Answering clinical questions with\\nknowledge-based and statistical techniques,” Computational Linguis-\\ntics, vol. 33, no. 1, pp. 63–103, Mar. 2007.\\n[89] R. M. Terol, P. Mart ´ınez-Barco, and M. Palomar, “A knowledge based\\nmethod for the medical question answering problem,” Computers in\\nBiology and Medicine , vol. 37, no. 10, pp. 1511–1521, Oct. 2007.\\n[90] Z. Liu, E. Peng, S. Yan, G. Li, and T. Hao, “T-know: A knowledge\\ngraph-based question answering and infor-mation retrieval system for\\ntraditional chinese medicine,” in Proceedings of the 27th International\\nConference on Computational Linguistics: System Demonstrations .\\nSanta Fe, New Mexico: Association for Computational Linguistics,\\nAug. 2018, pp. 15–19.\\n[91] E. Mutabazi, J. Ni, G. Tang, and W. Cao, “A review on medical textual\\nquestion answering systems based on deep learning approaches,”\\nApplied Sciences, vol. 11, no. 12, p. 5456, Jan. 2021.\\n[92] H. Shim, D. Lowet, S. Luca, and B. Vanrumste, “Building blocks\\nof a task-oriented dialogue system in the healthcare domain,” in\\nProceedings of the Second Workshop on Natural Language Processing\\nfor Medical Conversations . Online: Association for Computational\\nLinguistics, Jun. 2021, pp. 47–57.\\n[93] Z. Wei, Q. Liu, B. Peng, H. Tou, T. Chen, X. Huang, K.-f. Wong,\\nand X. Dai, “Task-oriented dialogue system for automatic diagnosis,”\\nin Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics (Volume 2: Short Papers) . Melbourne,\\nAustralia: Association for Computational Linguistics, Jul. 2018, pp.\\n201–207.\\n[94] L. Xu, Q. Zhou, K. Gong, X. Liang, J. Tang, and L. Lin, “End-to-end\\nknowledge-routed relational dialogue system for automatic diagnosis,”\\narXiv:1901.10623 [cs], Mar. 2019.\\n[95] H. Kawata, K. Ookawara, M. Muta, S. Masuko, and J. Hoshino,\\n“Lifestyle agent: The chat-oriented dialogue system for lifestyle man-\\nagement,” in Entertainment Computing – ICEC 2017 , ser. Lecture\\nNotes in Computer Science, N. Munekata, I. Kunita, and J. Hoshino,\\nEds. Cham: Springer International Publishing, 2017, pp. 396–399.\\n[96] R. Studer, V . R. Benjamins, and D. Fensel, “Knowledge engineering:\\nPrinciples and methods,” Data & Knowledge Engineering , vol. 25,\\nno. 1, pp. 161–197, Mar. 1998.\\n[97] T. Goodwin and S. M. Harabagiu, “Automatic generation of a qualiﬁed\\nmedical knowledge graph and its usage for retrieving patient cohorts\\nfrom electronic medical records,” in 2013 IEEE Seventh International\\nConference on Semantic Computing , Sep. 2013, pp. 363–370.\\n[98] A. Rossanez, J. C. dos Reis, R. d. S. Torres, and H. de Ribaupierre,\\n“KGen: A knowledge graph generator from biomedical scientiﬁc\\nliterature,” BMC Medical Informatics and Decision Making , vol. 20,\\nno. 4, p. 314, Dec. 2020.\\n[99] M. Rotmensch, Y . Halpern, A. Tlimat, S. Horng, and D. Sontag,\\n“Learning a health knowledge graph from electronic medical records,”\\nScientiﬁc Reports, vol. 7, no. 1, p. 5994, Jul. 2017.\\n[100] H. Wang, Q. Zhang, and J. Yuan, “Semantically enhanced medical\\ninformation retrieval system: A tensor factorization based approach,”\\nIEEE Access, vol. 5, pp. 7584–7593, 2017.\\n[101] Y . Pan, Q. Chen, W. Peng, X. Wang, B. Hu, X. Liu, J. Chen, and\\nW. Zhou, “MedWriter: Knowledge-aware medical text generation,” in\\nProceedings of the 28th International Conference on Computational\\nLinguistics. Barcelona, Spain (Online): International Committee on\\nComputational Linguistics, Dec. 2020, pp. 2363–2368.\\n[102] A. Stoica, T. Kadar, C. Lemnaru, R. Potolea, and M. D ˆıns ¸oreanu,\\n“Intent detection and slot ﬁlling with capsule net architectures for a\\nromanian home assistant,” Sensors, vol. 21, no. 4, p. 1230, Jan. 2021.\\n[103] A. Neuraz, L. C. Llanos, A. Burgun, and S. Rosset, “Natural language\\nunderstanding for task oriented dialog in the biomedical domain in a\\nlow resources context,” arXiv:1811.09417 [cs], Nov. 2018.\\n[104] C. Zhang, N. Du, W. Fan, Y . Li, C.-T. Lu, and P. S. Yu, “Bringing\\nsemantic structures to user intent detection in online medical queries,”\\nin 2017 IEEE International Conference on Big Data (Big Data) , Dec.\\n2017, pp. 1019–1026.\\n[105] I. Giachos, E. C. Papakitsos, and G. Chorozoglou, “Exploring natural\\nlanguage understanding in robotic interfaces,” International Journal of\\nAdvances in Intelligent Informatics, vol. 3, no. 1, pp. 10–19, Mar. 2017.\\n[106] J. Thomason, A. Padmakumar, J. Sinapov, N. Walker, Y . Jiang,\\nH. Yedidsion, J. Hart, P. Stone, and R. J. Mooney, “Improving grounded\\nnatural language understanding through human-robot dialog,” 2019\\nInternational Conference on Robotics and Automation (ICRA) , pp.\\n6934–6941, May 2019.\\n[107] A. Feder, K. A. Keith, E. Manzoor, R. Pryzant, D. Sridhar, Z. Wood-\\nDoughty, J. Eisenstein, J. Grimmer, R. Reichart, M. E. Roberts, B. M.\\nStewart, V . Veitch, and D. Yang, “Causal inference in natural lan-\\nguage processing: Estimation, prediction, interpretation and beyond,”\\narXiv:2109.00725 [cs], Sep. 2021.\\n[108] J. Zeng, M. F. Gensheimer, D. L. Rubin, S. Athey, and R. D. Shachter,\\n“Uncovering interpretable potential confounders in electronic medical\\nrecords,” medRxiv : the preprint server for health sciences , 2021.\\n[109] S. Doan, E. W. Yang, S. S. Tilak, P. W. Li, D. S. Zisook, and M. Torii,\\n“Extracting health-related causality from twitter messages using natural\\nlanguage processing,” BMC Medical Informatics and Decision Making,\\nvol. 19, no. 3, p. 79, Apr. 2019.\\n[110] G. Nordon, G. Koren, V . Shalev, B. Kimelfeld, U. Shalit, and K. Radin-\\nsky, “Building causal graphs from medical literature and electronic\\nmedical records,” Proceedings of the AAAI Conference on Artiﬁcial\\nIntelligence, vol. 33, no. 01, pp. 1102–1109, Jul. 2019.\\n[111] R. Stiefelhagen, C. Fugen, R. Gieselmann, H. Holzapfel, K. Nickel,\\nand A. Waibel, “Natural human-robot interaction using speech, head\\npose and gestures,” in 2004 IEEE/RSJ International Conference on\\nIntelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566) ,\\nvol. 3, Sep. 2004, pp. 2422–2427 vol.3.\\n[112] G. P. Finley, E. Edwards, W. Salloum, A. Robinson, N. Sadoughi,\\nN. Axtmann, M. Korenevsky, M. Brenndoerfer, M. Miller, and\\nD. Suendermann-Oeft, “Semi-supervised acoustic model retraining\\nfor medical ASR,” in Speech and Computer , ser. Lecture Notes in\\nComputer Science, A. Karpov, O. Jokisch, and R. Potapova, Eds.\\nCham: Springer International Publishing, 2018, pp. 177–187.\\n[113] J. Sas and T. Poreba, “Optimal acoustic model complexity selection in\\npolish medical speech recognition,” Journal of Medical Informatics &\\nTechnologies, vol. V ol. 17, 2011.\\n[114] J. M. Paulett and C. P. Langlotz, “Improving language models for\\nradiology speech recognition,” Journal of Biomedical Informatics ,\\nvol. 42, no. 1, pp. 53–58, Feb. 2009.\\n[115] C.-C. Chiu, A. Tripathi, K. Chou, C. Co, N. Jaitly, D. Jaunzeikare,\\nA. Kannan, P. Nguyen, H. Sak, A. Sankar, J. Tansuwan, N. Wan,\\nY . Wu, and X. Zhang, “Speech recognition for medical conversations,”\\narXiv:1711.07274 [cs, eess, stat] , Jun. 2018.\\n[116] E. Edwards, W. Salloum, G. P. Finley, J. Fone, G. Cardiff, M. Miller,\\nand D. Suendermann-Oeft, “Medical speech recognition: Reaching\\nparity with humans,” in Speech and Computer , ser. Lecture Notes\\nin Computer Science, A. Karpov, R. Potapova, and I. Mporas, Eds.\\nCham: Springer International Publishing, 2017, pp. 512–524.\\n[117] T. He, W. Zhao, and L. Xu, “DOP-tacotron: A fast chinese TTS system\\nwith local-based attention,” in 2020 Chinese Control And Decision\\nConference (CCDC), Aug. 2020, pp. 4345–4350.\\n[118] K. Sugiura, Y . Shiga, H. Kawai, T. Misu, and C. Hori, “Non-monologue\\nHMM-based speech synthesis for service robots: A cloud robotics\\napproach,” in 2014 IEEE International Conference on Robotics and\\nAutomation (ICRA), May 2014, pp. 2237–2242.\\n[119] H. Akbari, B. Khalighinejad, J. L. Herrero, A. D. Mehta, and N. Mes-\\ngarani, “Towards reconstructing intelligible speech from the human\\nauditory cortex,” Scientiﬁc Reports, vol. 9, no. 1, p. 874, Jan. 2019.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='14 IEEE REVIEWS IN BIOMEDICAL ENGINEERING\\n[120] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, “Speech synthesis\\nfrom neural decoding of spoken sentences,” Nature, vol. 568, no. 7753,\\npp. 493–498, Apr. 2019.\\n[121] C. Herff, L. Diener, M. Angrick, E. Mugler, M. C. Tate, M. A. Goldrick,\\nD. J. Krusienski, M. W. Slutzky, and T. Schultz, “Generating natural,\\nintelligible speech from brain activity in motor, premotor, and inferior\\nfrontal cortices,” Frontiers in Neuroscience, vol. 13, p. 1267, 2019.\\n[122] C. Jreige, R. Patel, and H. T. Bunnell, “V ocaliD: Personalizing text-\\nto-speech synthesis for individuals with severe speech impairment,” in\\nProceedings of the 11th International ACM SIGACCESS Conference\\non Computers and Accessibility, ser. Assets ’09. New York, NY , USA:\\nAssociation for Computing Machinery, Oct. 2009, pp. 259–260.\\n[123] M. Marge, C. Espy-Wilson, N. G. Ward, A. Alwan, Y . Artzi, M. Bansal,\\nG. Blankenship, J. Chai, H. Daum ´e, D. Dey, M. Harper, T. Howard,\\nC. Kennington, I. Kruijff-Korbayov ´a, D. Manocha, C. Matuszek,\\nR. Mead, R. Mooney, R. K. Moore, M. Ostendorf, H. Pon-Barry, A. I.\\nRudnicky, M. Scheutz, R. S. Amant, T. Sun, S. Tellex, D. Traum, and\\nZ. Yu, “Spoken language interaction with robots: Recommendations for\\nfuture research,” Computer Speech & Language , vol. 71, p. 101255,\\nJan. 2022.\\n[124] J. James, B. T. Balamurali, C. I. Watson, and B. MacDonald, “Empa-\\nthetic speech synthesis and testing for healthcare robots,” International\\nJournal of Social Robotics , Sep. 2020.\\n[125] X. Li, B. MacDonald, and C. I. Watson, “Expressive facial speech\\nsynthesis on a robotic platform,” in Proceedings of the 2009 IEEE/RSJ\\nInternational Conference on Intelligent Robots and Systems , ser.\\nIROS’09. St. Louis, MO, USA: IEEE Press, Oct. 2009, pp. 5009–\\n5014.\\n[126] S. Roehling, B. Macdonald, and C. Watson, “Towards expressive\\nspeech synthesis in english on a robotic platform,” in In Proceedings\\nof the Australasian International Conference on Speech Science and\\nTechnology, 2006, pp. 130–135.\\n[127] K. K ¨uhne, M. H. Fischer, and Y . Zhou, “The human takes it all: Hu-\\nmanlike synthesized voices are perceived as less eerie and more likable.\\nevidence from a subjective ratings study,” Frontiers in Neurorobotics,\\nvol. 14, p. 105, 2020.\\n[128] F. Jiang, Y . Jiang, H. Zhi, Y . Dong, H. Li, S. Ma, Y . Wang, Q. Dong,\\nH. Shen, and Y . Wang, “Artiﬁcial intelligence in healthcare: Past,\\npresent and future,” Stroke and Vascular Neurology, vol. 2, no. 4, Dec.\\n2017.\\n[129] K. N. Dew, A. M. Turner, Y . K. Choi, A. Bosold, and K. Kirchhoff,\\n“Development of machine translation technology for assisting health\\ncommunication: A systematic review,”Journal of Biomedical Informat-\\nics, vol. 85, pp. 56–67, Sep. 2018.\\n[130] G. Randhawa, M. Ferreyra, R. Ahmed, O. Ezzat, and K. Pottie, “Using\\nmachine translation in clinical practice,” Canadian Family Physician ,\\nvol. 59, no. 4, pp. 382–383, Apr. 2013.\\n[131] F. Goss, S. Blackley, C. Ortega, L. Kowalski, A. Landman, C. Lin,\\nM. Meteer, S. Bakes, S. Gradwohl, D. Bates, and Z. Li, “A clinician\\nsurvey of using speech recognition for clinical documentation in the\\nelectronic health record,” International Journal of Medical Informatics,\\nvol. 130, Jul. 2019.\\n[132] K. Saxena, R. Diamond, R. F. Conant, T. H. Mitchell, i. G. Gallopyn,\\nand K. E. Yakimow, “Provider adoption of speech recognition and its\\nimpact on satisfaction, documentation quality, efﬁciency, and cost in an\\ninpatient EHR,” AMIA Summits on Translational Science Proceedings ,\\nvol. 2018, pp. 186–195, May 2018.\\n[133] Y . Zhao, “Speech-recognition technology in health care and special-\\nneeds assistance [life sciences],” IEEE Signal Processing Magazine ,\\nvol. 26, no. 3, pp. 87–90, May 2009.\\n[134] T. R. Goodwin and S. M. Harabagiu, “Medical question answering for\\nclinical decision support,” Proceedings of the ... ACM International\\nConference on Information & Knowledge Management. ACM Interna-\\ntional Conference on Information and Knowledge Management , vol.\\n2016, pp. 297–306, Oct. 2016.\\n[135] G. Xu, W. Rong, Y . Wang, Y . Ouyang, and Z. Xiong, “External\\nfeatures enriched model for biomedical question answering,” BMC\\nBioinformatics, vol. 22, no. 1, p. 272, May 2021.\\n[136] X. Shi, D. Jiang, Y . Huang, X. Wang, Q. Chen, J. Yan, and B. Tang,\\n“Family history information extraction via deep joint learning,” BMC\\nMedical Informatics and Decision Making , vol. 19, no. Suppl 10, Dec.\\n2019.\\n[137] A. Gupta, I. Banerjee, and D. L. Rubin, “Automatic information\\nextraction from unstructured mammography reports using distributed\\nsemantics,” Journal of Biomedical Informatics, vol. 78, pp. 78–86, Feb.\\n2018.\\n[138] J. Yang, Y . Liu, M. Qian, C. Guan, and X. Yuan, “Information\\nextraction from electronic medical records using multitask recurrent\\nneural network with contextual word embedding,” Applied Sciences ,\\nvol. 9, no. 18, p. 3658, Jan. 2019.\\n[139] S. Zheng, S. K. Jabbour, S. E. O’Reilly, J. J. Lu, L. Dong, L. Ding,\\nY . Xiao, N. Yue, F. Wang, and W. Zou, “Automated information\\nextraction on treatment and prognosis for Non–Small cell lung can-\\ncer radiotherapy patients: Clinical study,” JMIR Medical Informatics ,\\nvol. 6, no. 1, p. e8, Feb. 2018.\\n[140] H. Liang, B. Y . Tsui, H. Ni, C. C. S. Valentim, S. L. Baxter, G. Liu,\\nW. Cai, D. S. Kermany, X. Sun, J. Chen, L. He, J. Zhu, P. Tian, H. Shao,\\nL. Zheng, R. Hou, S. Hewett, G. Li, P. Liang, X. Zang, Z. Zhang,\\nL. Pan, H. Cai, R. Ling, S. Li, Y . Cui, S. Tang, H. Ye, X. Huang,\\nW. He, W. Liang, Q. Zhang, J. Jiang, W. Yu, J. Gao, W. Ou, Y . Deng,\\nQ. Hou, B. Wang, C. Yao, Y . Liang, S. Zhang, Y . Duan, R. Zhang,\\nS. Gibson, C. L. Zhang, O. Li, E. D. Zhang, G. Karin, N. Nguyen,\\nX. Wu, C. Wen, J. Xu, W. Xu, B. Wang, W. Wang, J. Li, B. Pizzato,\\nC. Bao, D. Xiang, W. He, S. He, Y . Zhou, W. Haw, M. Goldbaum,\\nA. Tremoulet, C.-N. Hsu, H. Carter, L. Zhu, K. Zhang, and H. Xia,\\n“Evaluation and accurate diagnoses of pediatric diseases using artiﬁcial\\nintelligence,” Nature Medicine, vol. 25, no. 3, pp. 433–438, Mar. 2019.\\n[141] H. Harkema, W. W. Chapman, M. Saul, E. S. Dellon, R. E. Schoen, and\\nA. Mehrotra, “Developing a natural language processing application\\nfor measuring the quality of colonoscopy procedures,” Journal of the\\nAmerican Medical Informatics Association: JAMIA , vol. 18 Suppl 1,\\npp. i150–156, Dec. 2011.\\n[142] A. Mehrotra, E. S. Dellon, R. E. Schoen, M. Saul, F. Bishehsari,\\nC. Farmer, and H. Harkema, “Applying a natural language processing\\ntool to electronic health records to assess performance on colonoscopy\\nquality measures,” Gastrointestinal Endoscopy , vol. 75, no. 6, pp.\\n1233–1239.e14, Jun. 2012.\\n[143] S. Wunnava, X. Qin, T. Kakar, C. Sen, E. A. Rundensteiner, and\\nX. Kong, “Adverse drug event detection from electronic health records\\nusing hierarchical recurrent neural networks with dual-level embed-\\nding,” Drug Safety, vol. 42, no. 1, pp. 113–122, Jan. 2019.\\n[144] R. G. Jackson, R. Patel, N. Jayatilleke, A. Kolliakou, M. Ball,\\nG. Gorrell, A. Roberts, R. J. Dobson, and R. Stewart, “Natural\\nlanguage processing to extract symptoms of severe mental illness from\\nclinical text: The clinical record interactive search comprehensive data\\nextraction (CRIS-CODE) project,”BMJ Open, vol. 7, no. 1, p. e012012,\\nJan. 2017.\\n[145] J. Luo, L. Lan, D. Yang, S. Huang, M. Li, J. Yin, J. Xiao, and X. Zhou,\\n“Early prediction of organ failures in patients with acute pancreatitis\\nusing text mining,” Scientiﬁc Programming, vol. 2021, p. e6683942,\\nMay 2021.\\n[146] X. Wang, X. Xu, W. Tong, R. Roberts, and Z. Liu, “InferBERT: A\\ntransformer-based causal inference framework for enhancing pharma-\\ncovigilance,” Frontiers in Artiﬁcial Intelligence , vol. 4, p. 67, 2021.\\n[147] S. V . Wang, O. V . Patterson, J. J. Gagne, J. S. Brown, R. Ball,\\nP. Jonsson, A. Wright, L. Zhou, W. Goettsch, and A. Bate, “Transparent\\nreporting on research using unstructured electronic health record data to\\ngenerate ‘real world’ evidence of comparative effectiveness and safety,”\\nDrug Safety, vol. 42, no. 11, pp. 1297–1309, Nov. 2019.\\n[148] A. Lee, B. E. Alving, M. B. Horup, and L. Thrysoee, “Information\\nretrieval as a part of evidence-based practice: Retrieval skills, behavior\\nand needs among nurses at a large university hospital:,” Nordic Journal\\nof Nursing Research , Aug. 2019.\\n[149] T. B. Patrick, G. Demiris, L. C. Folk, D. E. Moxley, J. A. Mitchell, and\\nD. Tao, “Evidence-based retrieval in evidence-based medicine,”Journal\\nof the Medical Library Association , vol. 92, no. 2, pp. 196–199, Apr.\\n2004.\\n[150] N. Ford, D. Miller, A. Booth, A. O’rourke, J. Ralph, and E. Turnock,\\n“Information retrieval for evidence-based decision making,” JOURNAL\\nOF DOCUMENTATION, vol. 55, Oct. 1999.\\n[151] N. W. Sterling, R. E. Patzer, M. Di, and J. D. Schrager, “Prediction\\nof emergency department patient disposition based on natural language\\nprocessing of triage notes,” International Journal of Medical Informat-\\nics, vol. 129, pp. 184–188, Sep. 2019.\\n[152] B. Tahayori, N. Chini-Foroush, and H. Akhlaghi, “Advanced natural\\nlanguage processing technique to predict patient disposition based on\\nemergency triage notes,” Emergency Medicine Australasia , vol. 33,\\nno. 3, pp. 480–484, 2021.\\n[153] E. Sezgin, Y . Huang, U. Ramtekkar, and S. Lin, “Readiness for voice\\nassistants to support healthcare delivery during a health crisis and\\npandemic,” npj Digital Medicine , vol. 3, no. 1, pp. 1–4, Sep. 2020.\\n[154] S. Sp ¨anig, A. Emberger-Klein, J.-P. Sowa, A. Canbay, K. Menrad, and\\nD. Heider, “The virtual doctor: An interactive artiﬁcial intelligence'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ZHOU et al.: NATURAL LANGUAGE PROCESSING FOR SMART HEALTHCARE 15\\nbased on deep learning for non-invasive prediction of diabetes,” Arti-\\nﬁcial Intelligence in Medicine , vol. 100, p. 101706, Sep. 2019.\\n[155] M. Gandhi, V . K. Singh, and V . Kumar, “IntelliDoctor - AI based\\nmedical assistant,” in 2019 Fifth International Conference on Science\\nTechnology Engineering and Mathematics (ICONSTEM) , vol. 1, Mar.\\n2019, pp. 162–168.\\n[156] E. I. Agustin, R. T. Yunardi, and A. A. Firdaus, “V oice recognition\\nsystem for controlling electrical appliances in smart hospital room,”\\nTELKOMNIKA (Telecommunication Computing Electronics and Con-\\ntrol), vol. 17, no. 2, pp. 965–972, Apr. 2019.\\n[157] A. Ismail, S. Abdlerazek, and I. M. El-Henawy, “Development of smart\\nhealthcare system based on speech recognition using support vector\\nmachine and dynamic time warping,” Sustainability, vol. 12, no. 6, p.\\n2403, Jan. 2020.\\n[158] L. Grasse, S. J. Boutros, and M. S. Tata, “Speech interaction to control a\\nhands-free delivery robot for high-risk health care scenarios,” Frontiers\\nin Robotics and AI , vol. 8, p. 40, 2021.\\n[159] J. Holland, L. Kingston, C. McCarthy, E. Armstrong, P. O’Dwyer,\\nF. Merz, and M. McConnell, “Service robots in the healthcare sector,”\\nRobotics, vol. 10, no. 1, p. 47, Mar. 2021.\\n[160] C. M. Lineback, R. Garg, E. Oh, A. M. Naidech, J. L. Holl, and\\nS. Prabhakaran, “Prediction of 30-day readmission after stroke using\\nmachine learning and natural language processing,” Frontiers in Neu-\\nrology, vol. 12, p. 1069, 2021.\\n[161] A. Rajkomar, E. Oren, K. Chen, A. M. Dai, N. Hajaj, M. Hardt, P. J.\\nLiu, X. Liu, J. Marcus, M. Sun, P. Sundberg, H. Yee, K. Zhang,\\nY . Zhang, G. Flores, G. E. Duggan, J. Irvine, Q. Le, K. Litsch,\\nA. Mossin, J. Tansuwan, D. Wang, J. Wexler, J. Wilson, D. Ludwig,\\nS. L. V olchenboum, K. Chou, M. Pearson, S. Madabushi, N. H. Shah,\\nA. J. Butte, M. D. Howell, C. Cui, G. S. Corrado, and J. Dean,\\n“Scalable and accurate deep learning with electronic health records,”\\nnpj Digital Medicine , vol. 1, no. 1, p. 18, Dec. 2018.\\n[162] A. Rumshisky, M. Ghassemi, T. Naumann, P. Szolovits, V . M. Cas-\\ntro, T. H. McCoy, and R. H. Perlis, “Predicting early psychiatric\\nreadmission with natural language processing of narrative discharge\\nsummaries,” Translational Psychiatry , vol. 6, no. 10, p. e921, Oct.\\n2016.\\n[163] O. Alfarghaly, R. Khaled, A. Elkorany, M. Helal, and A. Fahmy, “Au-\\ntomated radiology report generation using conditioned transformers,”\\nInformatics in Medicine Unlocked , vol. 24, p. 100557, Jan. 2021.\\n[164] B. Chintagunta, N. Katariya, X. Amatriain, and A. Kannan, “Medically\\naware GPT-3 as a data generator for medical dialogue summarization,”\\nin Proceedings of the Second Workshop on Natural Language Process-\\ning for Medical Conversations. Online: Association for Computational\\nLinguistics, Jun. 2021, pp. 66–76.\\n[165] A. M. Ibrahim, “Ontology-driven information retrieval for healthcare\\ninformation system : A case study,” International Journal of Network\\nSecurity & Its Applications , vol. 5, no. 1, pp. 61–69, Jan. 2013.\\n[166] M. Khanbhai, P. Anyadi, J. Symons, K. Flott, A. Darzi, and E. Mayer,\\n“Applying natural language processing and machine learning tech-\\nniques to patient experience feedback: A systematic review,” BMJ\\nHealth & Care Informatics , vol. 28, no. 1, p. e100262, Mar. 2021.\\n[167] K. Nawab, G. Ramsey, and R. Schreiber, “Natural language processing\\nto extract meaningful information from patient experience feedback,”\\nApplied Clinical Informatics , vol. 11, no. 2, pp. 242–252, Mar. 2020.\\n[168] K. Doing-Harris, D. L. Mowery, C. Daniels, W. W. Chapman, and\\nM. Conway, “Understanding patient satisfaction with received health-\\ncare services: A natural language processing approach,” AMIA Annual\\nSymposium Proceedings, vol. 2016, pp. 524–533, Feb. 2017.\\n[169] D. Rodger, A. Skuse, M. Wilmore, S. Humphreys, J. Dalton,\\nM. Flabouris, V . L. Clifton, D. Rodger, A. Skuse, M. Wilmore,\\nS. Humphreys, J. Dalton, M. Flabouris, and V . L. Clifton, “Preg-\\nnant women’s use of information and communications technologies\\nto access pregnancy-related health information in South Australia,”\\nAustralian Journal of Primary Health , vol. 19, no. 4, pp. 308–312,\\nDec. 2013.\\n[170] B. Zhou, K. Wu, P. Lv, J. Wang, G. Chen, B. Ji, and S. Liu, “A\\nnew remote health-care system based on moving robot intended for\\nthe elderly at home,” Journal of Healthcare Engineering, vol. 2018, p.\\n4949863, Feb. 2018.\\n[171] P. J. Rani, J. Bakthakumar, B. P. Kumaar, U. P. Kumaar, and S. Kumar,\\n“V oice controlled home automation system using natural language\\nprocessing (NLP) and internet of things (IoT),” in 2017 Third Inter-\\nnational Conference on Science Technology Engineering Management\\n(ICONSTEM), Mar. 2017, pp. 368–373.\\n[172] A. Tapus, M. J. Mataric, and B. Scassellati, “Socially assistive\\nrobotics,” IEEE Robotics Automation Magazine , vol. 14, no. 1, pp.\\n35–42, Mar. 2007.\\n[173] N. Mavridis, “A review of verbal and non-verbal human–robot interac-\\ntive communication,” Robotics and Autonomous Systems , vol. 63, pp.\\n22–35, Jan. 2015.\\n[174] J. R. Green, R. L. MacDonald, P.-P. Jiang, J. Cattiau, R. Heywood,\\nR. Cave, K. Seaver, M. A. Ladewig, J. Tobin, M. P. Brenner, P. C.\\nNelson, and K. Tomanek, “Automatic speech recognition of disordered\\nspeech: Personalized models outperforming human listeners on short\\nphrases,” in Interspeech 2021. ISCA, Aug. 2021, pp. 4778–4782.\\n[175] K. Hux, K. Knollman-Porter, J. Brown, and S. E. Wallace, “Compre-\\nhension of synthetic speech and digitized natural speech by adults with\\naphasia,”Journal of Communication Disorders, vol. 69, pp. 15–26, Sep.\\n2017.\\n[176] K. Hux, J. A. Brown, S. Wallace, K. Knollman-Porter, A. Saylor, and\\nE. Lapp, “Effect of text-to-speech rate on reading comprehension by\\nadults with aphasia,” American Journal of Speech-Language Pathology,\\nvol. 29, no. 1, pp. 168–184, Jul. 2020.\\n[177] S. Cassidy, B. Stenger, L. Van Dongen, K. Yanagisawa, R. Anderson,\\nV . Wan, S. Baron-Cohen, and R. Cipolla, “Expressive visual text-to-\\nspeech as an assistive technology for individuals with autism spectrum\\nconditions,” Computer Vision and Image Understanding , vol. 148, pp.\\n193–200, Jul. 2016.\\n[178] B. Repova, M. Zabrodsky, J. Plzak, D. Kalfert, J. Matousek, and\\nJ. Betka, “Text-to-speech synthesis as an alternative communication\\nmeans after total laryngectomy,” Biomedical Papers of the Medical\\nFaculty of the University Palacky, Olomouc, Czechoslovakia, Apr. 2020.\\n[179] F. C. Lyall, P. J. Clamp, and D. Hajioff, “Smartphone speech-to-text\\napplications for communication with profoundly deaf patients,” The\\nJournal of Laryngology and Otology , vol. 130, no. 1, pp. 104–106,\\nJan. 2016.\\n[180] S. Nittrouer, L. M. Krieg, and J. H. Lowenstein, “Speech recognition\\nin noise by children with and without dyslexia: How is it related to\\nreading?” Research in developmental disabilities , vol. 77, pp. 98–113,\\nJun. 2018.\\n[181] D. Ria ˜no, M. Peleg, and A. ten Teije, “Ten years of knowledge repre-\\nsentation for health care (2009–2018): Topics, trends, and challenges,”\\nArtiﬁcial Intelligence in Medicine , vol. 100, p. 101713, Sep. 2019.\\n[182] Q. Bao, L. Ni, and J. Liu, “HHH: An online medical chatbot system\\nbased on knowledge graph and hierarchical bi-directional attention,”\\nProceedings of the Australasian Computer Science Week Multiconfer-\\nence, pp. 1–10, Feb. 2020.\\n[183] W. Xie, R. Ding, J. Yan, and Y . Qu, “A mobile-based question-\\nanswering and early warning system for assisting diabetes manage-\\nment,” Wireless Communications and Mobile Computing, vol. 2018, p.\\ne9163160, Jun. 2018.\\n[184] P. Peters, Y . Qian, and J. Ding, “Translating medical terminology and\\nbilingual terminography,” Lexicography: Journal of ASIALEX , vol. 3,\\nno. 2, pp. 99–113, 2016.\\n[185] A. Renato, J. Casta ˜no, M. d. P. A. Williams, H. Berinsky, M. L.\\nGambarte, H. Park, D. P ´erez-Rey, C. Otero, and D. Luna, “A machine\\ntranslation approach for medical terms,” in HEALTHINF, 2018.\\n[186] J. Leo, G. Kurdi, N. Matentzoglu, B. Parsia, U. Sattler, S. Forge,\\nG. Donato, and W. Dowling, “Ontology-based generation of medical,\\nmulti-term MCQs,” International Journal of Artiﬁcial Intelligence in\\nEducation, vol. 29, no. 2, pp. 145–188, May 2019.\\n[187] NHS, “Nhs population screening explained,” nhs population screening\\nexplained, Feb 2013. [Online]. Available: https://www.gov.uk/guidance/\\nnhs-population-screening-explained\\n[188] P. M, G. M, Newton-DameRemle, T. E, P. E, M. H, and G. N, “The\\nstate of population health surveillance using electronic health records:\\nA narrative review,” Population Health Management, Jun. 2015.\\n[189] D. Georgiou, A. MacFarlane, and T. Russell-Rose, “Extracting senti-\\nment from healthcare survey data: An evaluation of sentiment analysis\\ntools,” in 2015 Science and Information Conference (SAI) , Jul. 2015,\\npp. 352–361.\\n[190] H. ¨Ozt¨urk, A. ¨Ozg¨ur, P. Schwaller, T. Laino, and E. Ozkirimli, “Explor-\\ning chemical space using natural language processing methodologies\\nfor drug discovery,”Drug Discovery Today, vol. 25, no. 4, pp. 689–705,\\nApr. 2020.\\n[191] F. Lake, “Artiﬁcial intelligence in drug discovery: What is new, and\\nwhat is next?” Future Drug Discovery , vol. 1, no. 2, p. FDD19, Oct.\\n2019.\\n[192] T.-H. Pham, Y . Qiu, J. Zeng, L. Xie, and P. Zhang, “A deep learning\\nframework for high-throughput mechanism-driven phenotype com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='16 IEEE REVIEWS IN BIOMEDICAL ENGINEERING\\npound screening and its application to COVID-19 drug repurposing,”\\nNature Machine Intelligence , vol. 3, no. 3, pp. 247–257, Mar. 2021.\\n[193] J. Schwartz, M. Awale, and J.-L. Reymond, “SMIfp (SMILES ﬁnger-\\nprint) chemical space for virtual screening and visualization of large\\ndatabases of organic molecules,” Journal of Chemical Information and\\nModeling, vol. 53, no. 8, pp. 1979–1989, Aug. 2013.\\n[194] F. Zhang, B. Sun, X. Diao, W. Zhao, and T. Shu, “Prediction of adverse\\ndrug reactions based on knowledge graph embedding,” BMC Medical\\nInformatics and Decision Making , vol. 21, no. 1, p. 38, Feb. 2021.\\n[195] K. Bouhedjar, A. Boukelia, A. K. Nacereddine, A. Boucheham, A. Be-\\nlaidi, and A. Djerourou, “A natural language processing approach\\nbased on embedding deep learning from heterogeneous compounds\\nfor quantitative structure–activity relationship modeling,” Chemical\\nBiology & Drug Design , vol. 96, no. 3, pp. 961–972, 2020.\\n[196] W. Jeon and D. Kim, “FP2VEC: A new molecular featurizer for\\nlearning molecular properties,” Bioinformatics, vol. 35, no. 23, pp.\\n4979–4985, Dec. 2019.\\n[197] L. Chen, Y . Gu, X. Ji, C. Lou, Z. Sun, H. Li, Y . Gao, and Y . Huang,\\n“Clinical trial cohort selection based on multi-level rule-based natural\\nlanguage processing system,” Journal of the American Medical Infor-\\nmatics Association : JAMIA, vol. 26, no. 11, pp. 1218–1226, Jul. 2019.\\n[198] S. Harrer, P. Shah, B. Antony, and J. Hu, “Artiﬁcial intelligence for\\nclinical trial design,” Trends in Pharmacological Sciences , vol. 40,\\nno. 8, pp. 577–591, Aug. 2019.\\n[199] H. Tissot, F. Asselbergs, A. Shah, D. Brealey, S. Harris, R. Agbakoba,\\nA. Folarin, L. Romao, L. Roguski, and R. Dobson, “Natural language\\nprocessing for mimicking clinical trial recruitment in critical care:\\nA semi-automated simulation based on the LeoPARDS trial,” IEEE\\nJournal of Biomedical and Health Informatics , vol. PP, pp. 1–1, Mar.\\n2020.\\n[200] X. Chen, H. Xie, G. Cheng, L. K. M. Poon, M. Leng, and F. L. Wang,\\n“Trends and features of the applications of natural language processing\\ntechniques for clinical trials text analysis,” Applied Sciences , vol. 10,\\nno. 6, p. 2157, Jan. 2020.\\n[201] C. L. Ventola, “Big data and pharmacovigilance: Data mining for\\nadverse drug events and interactions,” Pharmacy and Therapeutics ,\\nvol. 43, no. 6, pp. 340–351, Jun. 2018.\\n[202] X. Wang, G. Hripcsak, M. Markatou, and C. Friedman, “Active\\ncomputerized pharmacovigilance using natural language processing,\\nstatistics, and electronic health records: A feasibility study,” Journal\\nof the American Medical Informatics Association : JAMIA , vol. 16,\\nno. 3, pp. 328–337, 2009.\\n[203] F. Liu, A. Jagannatha, and H. Yu, “Towards drug safety surveillance\\nand pharmacovigilance: Current progress in detecting medication and\\nadverse drug events from electronic health records,” Drug Safety ,\\nvol. 42, no. 1, pp. 95–97, Jan. 2019.\\n[204] B. Zhou, G. Yang, Z. Shi, and S. Ma, “Interpretable Temporal Attention\\nNetwork for COVID-19 forecasting,”Applied Soft Computing, vol. 120,\\np. 108691, May 2022.\\n[205] N. Zheng, S. Du, J. Wang, H. Zhang, W. Cui, Z. Kang, T. Yang, B. Lou,\\nY . Chi, H. Long, M. Ma, Q. Yuan, S. Zhang, D. Zhang, F. Ye, and\\nJ. Xin, “Predicting COVID-19 in china using hybrid AI model,” IEEE\\nTransactions on Cybernetics, vol. 50, no. 7, pp. 2891–2904, Jul. 2020.\\n[206] Q. Chen, R. Leaman, A. Allot, L. Luo, C.-H. Wei, S. Yan, and Z. Lu,\\n“Artiﬁcial intelligence in action: Addressing the COVID-19 pandemic\\nwith natural language processing,” Annual Review of Biomedical Data\\nScience, vol. 4, no. 1, pp. 313–339, 2021.\\n[207] A. Chapman, K. Peterson, A. Turano, T. Box, K. Wallace, and\\nM. Jones, “A natural language processing system for national COVID-\\n19 surveillance in the US department of veterans affairs,” in Proceed-\\nings of the 1st Workshop on NLP for COVID-19 at ACL 2020. Online:\\nAssociation for Computational Linguistics, 2020.\\n[208] R. C. Cury, I. Megyeri, T. Lindsey, R. Macedo, J. Batlle, S. Kim,\\nB. Baker, R. Harris, and R. H. Clark, “Natural language processing\\nand machine learning for detection of respiratory illness by chest CT\\nimaging and tracking of COVID-19 pandemic in the united states,”\\nRadiology: Cardiothoracic Imaging , vol. 3, no. 1, p. e200596, Feb.\\n2021.\\n[209] D. DeCaprio, J. Gartner, C. J. McCall, T. Burgess, K. Garcia,\\nS. Kothari, and S. Sayed, “Building a COVID-19 vulnerability index,”\\nJournal of Medical Artiﬁcial Intelligence , vol. 3, no. 0, Dec. 2020.\\n[210] S. M. Meystre, P. M. Heider, Y . Kim, M. Davis, J. Obeid, J. Madory,\\nand A. V . Alekseyenko, “Natural language processing enabling\\nCOVID-19 predictive analytics to support data-driven patient advising\\nand pooled testing,” Journal of the American Medical Informatics\\nAssociation: JAMIA, vol. 29, no. 1, pp. 12–21, Dec. 2021.\\n[211] L. Wang, L. Jiang, D. Pan, Q. Wang, Z. Yin, Z. Kang, H. Tian, X. Geng,\\nJ. Shao, W. Pan, J. Yin, L. Fang, Y . Wang, W. Zhang, Z. Li, J. Zheng,\\nW. Hu, Y . Pan, D. Yu, S. Guo, W. Lu, Q. Li, Y . Zhou, and H. Xu, “Novel\\napproach by natural language processing for COVID-19 knowledge\\ndiscovery,” Biomedical Journal, Apr. 2022.\\n[212] A. Keshavarzi Arshadi, J. Webb, M. Salem, E. Cruz, S. Calad-\\nThomson, N. Ghadirian, J. Collins, E. Diez-Cecilia, B. Kelly,\\nH. Goodarzi, and J. S. Yuan, “Artiﬁcial Intelligence for COVID-19\\nDrug Discovery and Vaccine Development,” Frontiers in Artiﬁcial\\nIntelligence, vol. 3, 2020.\\n[213] Z. Liu, R. A. Roberts, M. Lal-Nag, X. Chen, R. Huang, and W. Tong,\\n“AI-based language models powering drug discovery and develop-\\nment,” Drug Discovery Today , vol. 26, no. 11, pp. 2593–2607, Nov.\\n2021.\\n[214] WHO, “10 global health issues to track in 2021,”\\nhttps://www.who.int/news-room/spotlight/10-global-health-issues-\\nto-track-in-2021, 2020.\\n[215] H.-J. Dai, C.-H. Su, Y .-Q. Lee, Y .-C. Zhang, C.-K. Wang, C.-J. Kuo,\\nand C.-S. Wu, “Deep learning-based natural language processing for\\nscreening psychiatric patients,” Frontiers in Psychiatry, vol. 11, 2021.\\n[216] D. D. DeSouza, J. Robin, M. Gumus, and A. Yeung, “Natural language\\nprocessing as an emerging tool to detect late-life depression,” Frontiers\\nin Psychiatry, vol. 12, 2021.\\n[217] R. G. Jackson, R. Patel, N. Jayatilleke, A. Kolliakou, M. Ball,\\nG. Gorrell, A. Roberts, R. J. Dobson, and R. Stewart, “Natural\\nlanguage processing to extract symptoms of severe mental illness from\\nclinical text: The clinical record interactive search comprehensive data\\nextraction (CRIS-CODE) project,”BMJ Open, vol. 7, no. 1, p. e012012,\\nJan. 2017.\\n[218] J. Cohen, J. Wright-Berryman, L. Rohlfs, D. Trocinski, L. Daniel, and\\nT. W. Klatt, “Integration and validation of a natural language processing\\nmachine learning suicide risk prediction model based on open-ended\\ninterview language in the emergency department,” Frontiers in Digital\\nHealth, vol. 4, 2022.\\n[219] D. Harvey, F. Lobban, P. Rayson, A. Warner, and S. Jones, “Natural\\nlanguage processing methods and bipolar disorder: Scoping review,”\\nJMIR Mental Health , vol. 9, no. 4, p. e35928, Apr. 2022.\\n[220] T. Zhang, A. M. Schoene, S. Ji, and S. Ananiadou, “Natural language\\nprocessing applied to mental illness detection: A narrative review,” npj\\nDigital Medicine, vol. 5, no. 1, pp. 1–13, Apr. 2022.\\n[221] G. Bedi, F. Carrillo, G. A. Cecchi, D. F. Slezak, M. Sigman, N. B. Mota,\\nS. Ribeiro, D. C. Javitt, M. Copelli, and C. M. Corcoran, “Automated\\nanalysis of free speech predicts psychosis onset in high-risk youths,”\\nnpj Schizophrenia, vol. 1, no. 1, pp. 1–7, Aug. 2015.\\n[222] R. A. Calvo, D. N. Milne, M. S. Hussain, and H. Christensen, “Natural\\nlanguage processing in mental health applications using non-clinical\\ntexts†,” Natural Language Engineering , vol. 23, no. 5, pp. 649–685,\\nSep. 2017.\\n[223] T. Althoff, K. Clark, and J. Leskovec, “Large-scale analysis of coun-\\nseling conversations: An application of natural language processing\\nto mental health,” Transactions of the Association for Computational\\nLinguistics, vol. 4, pp. 463–476, 2016.\\n[224] V . Chattaraman, W.-S. Kwon, J. E. Gilbert, and K. Ross, “Should AI-\\nBased, conversational digital assistants employ social- or task-oriented\\ninteraction style? A task-competency and reciprocity perspective for\\nolder adults,” Computers in Human Behavior , vol. 90, pp. 315–330,\\nJan. 2019.\\n[225] A. Amin-Nejad, J. Ive, and S. Velupillai, “Exploring transformer text\\ngeneration for medical dataset augmentation,” in Proceedings of the\\n12th Language Resources and Evaluation Conference . Marseille,\\nFrance: European Language Resources Association, May 2020, pp.\\n4699–4708.\\n[226] J. Ive, N. Viani, J. Kam, L. Yin, S. Verma, S. Puntis, R. N. Cardinal,\\nA. Roberts, R. Stewart, and S. Velupillai, “Generation and evaluation\\nof artiﬁcial mental health records for natural language processing,” npj\\nDigital Medicine, vol. 3, no. 1, pp. 1–9, May 2020.\\n[227] X. Soto, O. Perez-de-Vi ˜naspre, G. Labaka, and M. Oronoz, “Neural\\nmachine translation of clinical texts between long distance languages,”\\nJournal of the American Medical Informatics Association , vol. 26,\\nno. 12, pp. 1478–1487, Dec. 2019.\\n[228] K. Wołk and K. Marasek, “Neural-based machine translation for\\nmedical text domain. based on european medicines agency leaﬂet\\ntexts,” Procedia Computer Science , vol. 64, pp. 2–9, Jan. 2015.\\n[229] K. Wolk and K. P. Marasek, “Translation of medical texts using neural\\nnetworks,” International Journal of Reliable and Quality E-Healthcare\\n(IJRQEH), vol. 5, no. 4, pp. 51–66, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 16, 'page_label': '17', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ZHOU et al.: NATURAL LANGUAGE PROCESSING FOR SMART HEALTHCARE 17\\nBinggui Zhou received the B.Eng. degree from\\nJinan University, Zhuhai, China, in 2018, and the\\nM.Sc. degree from the University of Macau, Macao,\\nChina, in 2021, respectively. He is currently working\\ntoward the Ph.D. degree in Electrical and Computer\\nEngineering with the University of Macau, Macao,\\nChina. He also serves as a Research Assistant\\nwith the School of Intelligent Systems Science and\\nEngineering, Jinan University, Zhuhai, China. His\\nresearch interests include Natural Language Process-\\ning, Artiﬁcial Intelligence, and AI assisted Wireless\\nCommunications.\\nGuanghua Yang received his Ph.D. degree in elec-\\ntrical and electronic engineering from the University\\nof Hong Kong, Hong Kong, in 2006. From 2006\\nto 2013, he served as post-doctoral fellow, research\\nassociate at the University of Hong Kong. Since\\nApril 2017, he has been with Jinan University, where\\nhe is currently a Full Professor in the School of\\nIntelligent Systems Science and Engineering. His\\nresearch interests are in the general areas of AI and\\nits applications.\\nZheng Shi received his B.S. degree in communi-\\ncation engineering from Anhui Normal University,\\nChina, in 2010 and his M.S. degree in communi-\\ncation and information system from Nanjing Uni-\\nversity of Posts and Telecommunications (NUPT),\\nChina, in 2013. He obtained his Ph.D. degree in\\nElectrical and Computer Engineering from Univer-\\nsity of Macau, Macao, in 2017. He is currently an\\nAssociate Professor with the School of Intelligent\\nSystems Science and Engineering, Jinan University,\\nZhuhai, China. His current research interests include\\nhybrid automatic repeat request, non-orthogonal multiple access, machine\\nlearning and Internet of Things.\\nShaodan Ma received the double Bachelor’s degrees\\nin science and economics and the M.Eng. degree\\nin electronic engineering from Nankai University,\\nTianjin, China, in 1999 and 2002, respectively, and\\nthe Ph.D. degree in electrical and electronic engi-\\nneering from The University of Hong Kong, Hong\\nKong, in 2006. From 2006 to 2011, she was a post-\\ndoctoral fellow at The University of Hong Kong.\\nSince August 2011, she has been with the University\\nof Macau, where she is currently a Professor. Her\\nresearch interests include array signal processing,\\nmachine learning, wireless sensing and mmwave communications.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='arXiv:2506.05176v3  [cs.CL]  11 Jun 2025\\nTechnical Report\\nQwen3 Embedding: Advancing Text Embedding and\\nReranking Through Foundation Models\\nYanzhao Zhang* Mingxin Li* Dingkun Long* Xin Zhang*\\nHuan Lin Baosong Yang Pengjun Xie An Yang\\nDayiheng Liu Junyang Lin Fei Huang Jingren Zhou\\nTongyi Lab Alibaba Group\\nhttps://huggingface.co/Qwen\\nhttps://modelscope.cn/organization/qwen\\nhttps://github.com/QwenLM/Qwen3-Embedding\\nAbstract\\nIn this work, we introduce the Qwen3 Embedding series, a significant advancement\\nover its predecessor, the GTE-Qwen series, in text embedding and reranking capabili-\\nties, built upon the Qwen3 foundation models. Leveraging the Qwen3 LLMs’ robust\\ncapabilities in multilingual text understanding and generation, our innovative multi-\\nstage training pipeline combines large-scale unsupervised pre-training with supervised\\nfine-tuning on high-quality datasets. Effective model merging strategies further ensure\\nthe robustness and adaptability of the Qwen3 Embedding series. During the training\\nprocess, the Qwen3 LLMs serve not only as backbone models but also play a crucial role\\nin synthesizing high-quality, rich, and diverse training data across multiple domains\\nand languages, thus enhancing the training pipeline. The Qwen3 Embedding series\\noffers a spectrum of model sizes (0.6B, 4B, 8B) for both embedding and reranking tasks,\\naddressing diverse deployment scenarios where users can optimize for either efficiency\\nor effectiveness. Empirical evaluations demonstrate that the Qwen3 Embedding series\\nachieves state-of-the-art results across diverse benchmarks. Notably, it excels on the\\nmultilingual evaluation benchmark MTEB for text embedding, as well as in various\\nretrieval tasks, including code retrieval, cross-lingual retrieval and multilingual retrieval.\\nTo facilitate reproducibility and promote community-driven research and development,\\nthe Qwen3 Embedding models are publicly available under the Apache 2.0 license.\\n1 Introduction\\nText embedding and reranking are fundamental components in numerous natural language pro-\\ncessing and information retrieval applications, including web search, question answering, recom-\\nmendation systems, and beyond (Karpukhin et al., 2020; Huang et al., 2020; Zhao et al., 2023; 2024).\\nHigh-quality embeddings enable models to capture semantic relationships between texts, while\\neffective reranking mechanisms ensure that the most relevant results are prioritized. Recently,\\nemerging application paradigms such as Retrieval-Augmented Generation (RAG) and agent sys-\\ntems, driven by the advancement of large language models (e.g., Qwen3 (Yang et al., 2025), GPT-4o\\n(Hurst et al., 2024)), have introduced new requirements and challenges for text embedding and\\nreranking, both in terms of model training paradigms and application scenarios. Despite significant\\nadvancements, training embedding and reranking models that perform well in scalability, contextual\\nunderstanding, and alignment with specific downstream tasks remains challenging.\\nThe emergence of large language models (LLMs) has significantly advanced the development of text\\nembedding and reranking models. Prior to the introduction of LLMs, the predominant approach\\n∗ Equal contribution\\n1'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\ninvolved using encoder-only pretrained language models like BERT as the foundational model\\nfor training (Reimers & Gurevych, 2019). The richer world knowledge, text understanding, and\\nreasoning abilities inherent in LLMs have led to further enhancements in models trained on these\\narchitectures. Additionally, there has been considerable research facilitating the integration of LLMs\\ninto processes such as training data synthesis and quality data filtering (Wang et al., 2024; Lee et al.,\\n2024; 2025b). The fundamental characteristics of LLMs have also inspired the introduction of new\\ntraining paradigms. For instance, during the embedding model training process, incorporating\\ndifferentiated tasks across aspects such as instruction type, domain, and language has yielded\\nimproved performance in downstream tasks (Su et al., 2023). Similarly, for reranking model training,\\nadvancements have been realized through both zero-shot methods based on user prompts and\\napproaches combining supervised fine-tuning (Ma et al., 2023; Pradeep et al., 2023; Zhang et al.,\\n2024a; Zhuang et al., 2024).\\nIn this work, we introduce the Qwen3 Embedding series models, which are constructed on top\\nof the Qwen3 foundation models. The Qwen3 foundation has simultaneously released base and\\ninstruct model versions, and we exploit the robust multilingual text understanding and generation\\ncapabilities of these models to fully realize their potential in training embedding and reranking\\nmodels. To train the embedding models, we implement a multi-stage training pipeline that involves\\nlarge-scale unsupervised pre-training followed by supervised fine tuning on high-quality datasets.\\nWe also employ model merging with various model checkpoints to enhance robustness and general-\\nization. The Qwen3 instruct model allows for efficient synthesis of a vast, high-quality, multilingual,\\nand multi-task text relevance dataset. This synthetic data is utilized in the initial unsupervised\\ntraining stage, while a subset of high-quality, small-scale data is selected for the second stage of\\nsupervised training. For the reranking models, we adopt a two-stage training scheme in a similar\\nmanner, consisting of high-quality supervised fine tuning and a model merging stage. Based on\\ndifferent sizes of the Qwen3 backbone models (including 0.6B, 4B, and 8B), we ultimately trained\\nthree text embedding models and three text reranking models. To facilitate their application in\\ndownstream tasks, the Qwen3 Embedding series supports several practical features, such as flexible\\ndimension representation for embedding models and customizable instructions for both embedding\\nand reranking models.\\nWe evaluate the Qwen3 Embedding series across a comprehensive set of benchmarks spanning\\nmultiple tasks and domains. Experimental results demonstrate that our embedding and reranking\\nmodels achieve state-of-the-art performance, performing competitively against leading proprietary\\nmodels in several retrieval tasks. For example, the flagship model Qwen3-8B-Embedding attains a\\nscore of 70.58 on the MTEB Multilingual benchmark (Enevoldsen et al., 2025) and 80.68 on the MTEB\\nCode benchmark (Enevoldsen et al., 2025), surpassing the previous state-of-the-art proprietary\\nembedding model, Gemini-Embedding (Lee et al., 2025b). Moreover, our reranking model delivers\\ncompetitive results across a range of retrieval tasks. The Qwen3-Reranker-0.6B model exceeds\\npreviously top-performing models in numerous retrieval tasks, while the larger Qwen3-Reranker-8B\\nmodel demonstrates even superior performance, improving ranking results by 3.0 points over the\\n0.6B model across multiple tasks. Furthermore, we include a constructive ablation study to elucidate\\nthe key factors contributing to the superior performance of the Qwen3 Embedding series, providing\\ninsights into its effectiveness.\\nIn the following sections, we describe the design of the model architecture, detail the training\\nprocedures, present the experimental results for both the embedding and reranking models of the\\nQwen3 Embedding Series, and conclude this technical report by summarizing the key findings and\\noutlining potential directions for future research.\\n2 Model Architecture\\nThe core idea behind embedding and reranking models is to evaluate relevance in a task-aware\\nmanner. Given a query q and a document d, embedding and reranking models assess their relevance\\nbased on a similarity criterion defined by instruction I. To enable the models for task-aware rele-\\nvance estimation, training data is often organized as {Ii, qi, d+\\ni , d−\\ni,1, · · ·, d−\\ni,n}, where d+\\ni represents a\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nFigure 1: Model architecture of Qwen3-Embedding (left) and Qwen3-Reranker (right).\\npositive (relevant) document for query qi, and d−\\ni,j are negative (irrelevant) documents. Training the\\nmodel on diverse text pairs broadens its applicability to a range of downstream tasks, including\\nretrieval, semantic textual similarity, classification, and clustering.\\nArchitecture The Qwen3 embedding and reranking models are built on the dense version of\\nQwen3 foundation models and are available in three sizes: 0.6B, 4B, and 8B parameters. We initialize\\nthese models using the Qwen3 foundation models to leverage their capabilities in text modeling\\nand instruction following. The model layers, hidden size, and context length for each model\\nconfiguration are detailed in Table 1.\\nEmbedding Models For text embeddings, we utilize LLMs with causal attention, appending an\\n[EOS] token at the end of the input sequence. The final embedding is derived from the hidden state\\nof the last layer corresponding to this [EOS] token.\\nTo ensure embeddings follow instructions during downstream tasks, we concatenate the instruction\\nand the query into a single input context, while leaving the document unchanged before processing\\nwith LLMs. The input format for queries is as follows:\\n{Instruction} {Query}<|endoftext|>\\nReranking Models To more accurately evaluate text similarity, we employ LLMs for point-wise\\nreranking within a single context. Similar to the embedding model, to enable instruction-following\\ncapability, we include the instruction in the input context. We use the LLM chat template and frame\\nthe similarity assessment task as a binary classification problem. The input to LLMs adheres to the\\ntemplate shown below:\\n<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the\\nInstruct provided. Note that the answer can only be \"yes\" or\\n\"no\".<|im_end|>\\n,→\\n,→\\n<|im_start|>user\\n<Instruct>: {Instruction}\\n<Query>: {Query}\\n<Document>: {Document}<|im_end|>\\n<|im_start|>assistant\\n<think>\\\\n\\\\n</think>\\\\n\\\\n\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nModel Type Models Size Layers Sequence\\nLength\\nEmbedding\\nDimension\\nMRL\\nSupport\\nInstruction\\nAware\\nText Embedding\\nQwen3-Embedding-0.6B0.6B 28 32K 1024 Yes Yes\\nQwen3-Embedding-4B 4B 36 32K 2560 Yes Yes\\nQwen3-Embedding-8B 8B 36 32K 4096 Yes Yes\\nText Reranking\\nQwen3-Reranker-0.6B 0.6B 28 32K - - Yes\\nQwen3-Reranker-4B 4B 36 32K - - Yes\\nQwen3-Reranker-8B 8B 36 32K - - Yes\\nTable 1: Model architecture of Qwen3 Embedding models. “MRL Support” indicates whether the\\nembedding model supports custom dimensions for the final embedding. “Instruction Aware” notes\\nwhether the embedding or reranker model supports customizing the input instruction according to\\ndifferent tasks.\\nFigure 2: Training pipeline of Qwen3 Embedding and Reranking models.\\nTo calculate the relevance score based on the given input, we assess the likelihood of the next token\\nbeing ”yes” or ”no.” This is expressed mathematically as follows:\\nscore(q, d) = eP(yes|I,q,d)\\neP(yes|I,q,d) + eP(no|I,q,d)\\n3 Models Training\\nIn this section, we describe the multi-stage training pipeline adopted and present the key elements of\\nthis training recipe, including training objective, training data synthesis, and filtering of high-quality\\ntraining data.\\n3.1 Training Objective\\nBefore introducing our training pipeline, we first outline the optimized loss functions used for the\\nembedding and reranking models during the training process. For the embedding model, we utilize\\nan improved contrastive loss based on the InfoNCE framework (Oord et al., 2018). Given a batch of\\nN training instances, the loss is defined as:\\nLembedding = − 1\\nN\\nN\\n∑\\ni\\nlog e(s(qi,d+\\ni )/τ)\\nZi\\n, (1)\\nwhere s(·, ·) is a similarity function (we use cosine similarity), τ is a temperature parameter, and Zi\\nis the normalization factor that aggregates the similarity scores of the positive pair against various\\nnegative pairs:\\nZi = e(s(qi,d+\\ni )/τ) +\\nK\\n∑\\nk\\nmik e(s(qi,d−\\ni,k)/τ) + ∑\\nj̸=i\\nmij e(s(qi,qj)/τ) + ∑\\nj̸=i\\nmij e(s(d+\\ni ,dj)/τ) + ∑\\nj̸=i\\nmij e(s(qi,dj)/τ)\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nwhere these terms represent similarities with: (1) the positive document d+\\ni , (2) K hard negatives\\nd−\\ni,k, (3) other in-batch queries qj, (4) other in-batch documents dj compared against the positive\\ndocument d+\\ni . (5) other in-batch documents dj compared against the query qi. The mask factor mij is\\ndesigned to mitigate the impact of false negatives and is defined as:\\nmij =\\n(\\n0 if sij > s(qi, d+\\ni ) +0.1 or dj == d+\\ni ,\\n1 otherwise,\\namong which sij is the corresponding score of qi, dj or qi, qj.\\nFor the reranking model, we optimize the Supervised Fine-Tuning (SFT) loss defined as:\\nLreranking = −log p(l|P(q, d)), (2)\\nwhere p(·|∗) denotes the probability assigned by LLM. The label l is “yes” for positive documents\\nand “no” for negatives. This loss function encourages the model to assign higher probabilities to\\ncorrect labels, thereby improving the ranking performance.\\n3.2 Multi-stage Training\\nThe multi-stage training approach is a common practice for training text embedding models (Li et al.,\\n2023; Wang et al., 2022; Chen et al., 2024). This strategy typically begins with initial training on large-\\nscale, semi-supervised data that includes noise, followed by fine-tuning using smaller, high-quality\\nsupervised datasets. This two-step process enhances the performance and generalization capabilities\\nof embedding models. Large-scale weakly supervised training data contribute significantly to\\nthe model’s generalization, while fine-tuning with high-quality data in subsequent stages further\\nimproves model performance. Both stages of training for embedding models utilize the optimization\\nobjective defined in Equation 1, whereas the reranking model training employs the loss function\\ndefined in Equation 2 as the optimization target.\\nBuilding upon the existing multi-stage training framework, the Qwen3 Embedding series introduces\\nthe following key innovations:\\n• Large-Scale Synthetic Data-Driven Weak Supervision Training: Unlike previous works (e.g.,\\nGTE, E5, BGE models), where weakly supervised training data are primarily collected from\\nopen-source communities such as Q&A forums or academic papers, we propose leveraging\\nthe text understanding and generation capabilities of foundation models to synthesize pair\\ndata directly. This approach allows for arbitrary definition of various dimensions of the\\ndesired pair data, such as task, language, length, and difficulty within the synthesis prompts.\\nCompared to data collection from open-domain sources, foundation model-driven data\\nsynthesis offers greater controllability, enabling precise management of the quality and\\ndiversity of the generated data, particularly in low-resource scenarios and languages.\\n• High-Quality Synthetic Data Utilization in Supervised Fine Tuning: Due to the exceptional\\nperformance of the Qwen3 Foundation model, the synthesized data is of notably high quality.\\nTherefore, in the second stage of supervised training, selective incorporation of this high-\\nquality synthetic data further enhances the overall model performance and generalization\\ncapabilities.\\n• Model Merging: Inspired by previous work (Li et al., 2024), after completing the supervised\\nfine-tuning, we applied a model merging technique based on spherical linear interpolation\\n(slerp). This technique involves merging multiple model checkpoints saved during the\\nfine-tuning process. This step aims to boost the model’s robustness and generalization\\nperformance across various data distributions.\\nIt is important to note that the reranking model’s training process does not include a first-stage\\nweakly supervised training phase.\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\n3.3 Synthetic Dataset\\nTo create a robust synthetic dataset for training models on various similarity tasks, we generate\\ndiverse text pairs spanning categories such as retrieval, bitext mining, classification, and semantic\\ntextual similarity (STS). The quality of these synthetic data pairs is ensured by utilizing the Qwen3-\\n32B model as the foundational model for data synthesis. We have designed a diverse prompting\\nstrategy to improve the variety and authenticity of the generated data. For instance, in the text\\nretrieval task, we synthesize data using the multilingual pre-training corpus from Qwen3. During\\nthe data synthesis process, specific roles are assigned to each document to simulate potential\\nusers querying that document. This injection of user perspectives enhances the diversity and\\nrealism of the synthetic queries. Specifically, we utilize a retrieval model to identify the top five\\nrole candidates for each document from a role library and present these documents along with\\ntheir role candidates to the prompt. This guides the model in outputting the most suitable role\\nconfiguration for query generation. Moreover, the prompt incorporates various dimensions such as\\nquery type (e.g., keyword, factual, summary, judgment), query length, difficulty, and language. This\\nmultidimensional approach ensures the quality and diversity of the synthetic data.\\nFinally, we create a total of approximately 150 million pairs of multi-task weak supervision training\\ndata. Our experiments reveal that the embedding model trained with these synthetic data performs\\nexceptionally well in downstream evaluations, particularly surpassing many previously supervised\\nmodels in the MTEB Multilingual benchmarks. This motivates us to filter the synthetic data to\\nidentify high-quality pairs for inclusion in a second stage of supervised training. We employ a\\nsimple cosine similarity calculation to select data pairs, retaining those with a cosine similarity\\ngreater than 0.7 from randomly sampled data. Ultimately, approximately 12 million high-quality\\nsupervised training data pairs are selected for further training.\\nModel Size Mean(Task)Mean(Type)BitextMiningClass-ificationClus-tering Inst.RetrievalMultilabelClass. PairClass.Rerank Retrieval STS\\nSelected Open-Source Models\\nNV-Embed-v2 7B 56.29 49.58 57.84 57.29 40.80 1.04 18.63 78.94 63.82 56.72 71.10GritLM-7B 7B 60.92 53.74 70.53 61.83 49.75 3.45 22.77 79.94 63.78 58.31 73.33BGE-M3 0.6B 59.56 52.18 79.11 60.35 40.88 -3.11 20.1 80.76 62.79 54.60 74.12multilingual-e5-large-instruct 0.6B63.22 55.08 80.13 64.94 50.75 -0.40 22.91 80.86 62.61 57.12 76.81gte-Qwen2-1.5B-instruct 1.5B59.45 52.69 62.51 58.32 52.05 0.74 24.02 81.58 62.58 60.78 71.61gte-Qwen2-7b-Instruct 7B 62.51 55.93 73.92 61.55 52.77 4.94 25.48 85.13 65.55 60.08 73.98\\nCommercial APIs\\ntext-embedding-3-large - 58.93 51.41 62.17 60.27 46.89 -2.68 22.03 79.17 63.89 59.27 71.68Cohere-embed-multilingual-v3.0 -61.12 53.23 70.50 62.95 46.89 -1.89 22.74 79.88 64.07 59.16 74.80Gemini Embedding - 68.37 59.59 79.28 71.82 54.59 5.18 29.16 83.63 65.58 67.71 79.40\\nQwen3 Embedding Models\\nQwen3-Embedding-0.6B0.6B64.33 56.00 72.22 66.83 52.33 5.09 24.59 80.83 61.41 64.64 76.17Qwen3-Embedding-4B 4B 69.45 60.86 79.36 72.33 57.15 11.56 26.77 85.05 65.08 69.60 80.86Qwen3-Embedding-8B 8B 70.58 61.69 80.89 74.00 57.65 10.06 28.66 86.40 65.63 70.88 81.08\\nTable 2: Performance on MTEB Multilingual (Enevoldsen et al., 2025). For compared models, the\\nscores are retrieved from MTEB online leaderboard on June 4th, 2025.\\n4 Evaluation\\nWe conduct comprehensive and fair evaluations across multiple benchmarks to assess the capabilities\\nof Qwen3 Embedding models.\\n4.1 Settings\\nFor the text embedding models, we utilize the Massive Multilingual Text Embedding Benchmark\\n(MMTEB) (Enevoldsen et al., 2025) for evaluation. MMTEB is a large-scale, community-driven\\nexpansion of MTEB (Muennighoff et al., 2023), covering over 500 quality-controlled evaluation tasks\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nModel Size Dim MTEB (Eng, v2) CMTEB MTEB (Code)\\nMean (Task) Mean (Type)Mean (Task) Mean (Type)\\nSelected Open-Source Models\\nNV-Embed-v2 7B 4096 69.81 65.00 63.0 62.0 -\\nGritLM-7B 7B 4096 67.07 63.22 - - 73.6α\\nmultilingual-e5-large-instruct 0.6B1024 65.53 61.21 - - 65.0α\\ngte-Qwen2-1.5b-instruct 1.5B 1536 67.20 63.26 67.12 67.79 -\\ngte-Qwen2-7b-instruct 7B 3584 70.72 65.77 71.62 72.19 56.41γ\\nCommercial APIs\\ntext-embedding-3-large - 3072 66.43 62.15 - - 58.95γ\\ncohere-embed-multilingual-v3.0 -1024 66.01 61.43 - - 51.94γ\\nGemini Embedding - 3072 73.30 67.67 - - 74.66γ\\nQwen3 Embedding Models\\nQwen3-Embedding-0.6B 0.6B 1024 70.70 64.88 66.33 67.44 75.41\\nQwen3-Embedding-4B 4B 2560 74.60 68.09 72.26 73.50 80.06\\nQwen3-Embedding-8B 8B 4096 75.22 68.70 73.83 75.00 80.68\\nTable 3: Performance on MTEB Engilish, MTEB Chinese, MTEB Code. αTaken from (Enevoldsen\\net al., 2025). γTaken from (Lee et al., 2025b). For other compared models, the scores are retrieved\\nfrom MTEB online leaderboard on June 4th, 2025.\\nacross more than 250 languages. In addition to classic text tasks such as as a variety of retrieval,\\nclassification, and semantic textual similarity, MMTEB includes a diverse set of challenging and\\nnovel tasks, such as instruction following, long-document retrieval, and code retrieval, representing\\nthe largest multilingual collection of evaluation tasks for embedding models to date. Our MMTEB\\nevaluations encompass 216 individual evaluation tasks, consisting of 131 tasks for MTEB (Multilin-\\ngual) (Enevoldsen et al., 2025), 41 tasks for MTEB (English, v2) (Muennighoff et al., 2023), 32 tasks\\nfor CMTEB (Xiao et al., 2024), and 12 code retrieval tasks for MTEB (Code) (Enevoldsen et al., 2025).\\nMoreover, we select a series of text retrieval tasks to assess the text reranking capabilities of our\\nmodels. We explore three types of retrieval tasks: (1) Basic Relevance Retrieval, categorized into\\nEnglish, Chinese, and Multilingual, evaluated on MTEB (Muennighoff et al., 2023), CMTEB (Xiao\\net al., 2024), MMTEB (Enevoldsen et al., 2025), and MLDR (Chen et al., 2024), respectively; (2) Code\\nRetrieval, evaluated on MTEB-Code (Enevoldsen et al., 2025), which comprises only code-related\\nretrieval data.; and (3) Complex Instruction Retrieval, evaluated on FollowIR (Weller et al., 2024).\\nCompared Methods We compare our models with the most prominent open-source text embed-\\nding models and commercial API services. The open-source models include the GTE (Li et al.,\\n2023; Zhang et al., 2024b), E5 (Wang et al., 2022), and BGE (Xiao et al., 2024) series, as well as NV-\\nEmbed-v2 (Lee et al., 2025a), GritLM-7B Muennighoff et al. (2025). The commercial APIs evaluated\\nare text-embedding-3-large from OpenAI, Gemini-embedding from Google, and Cohere-embed-\\nmultilingual-v3.0. For reranking, we compare with the rerankers of jina1, mGTE (Zhang et al., 2024b)\\nand BGE-m3 (Chen et al., 2024).\\n4.2 Main Results\\nEmbedding In Table 2, we present the evaluation results on MMTEB (Enevoldsen et al., 2025),\\nwhich comprehensively covers a wide range of embedding tasks across multiple languages. Our\\nQwen3-Embedding-4B/8B models achieve the best performance, and our smallest model, Qwen3-\\nEmbedding-0.6B, only lags behind the best-performing baseline method (Gemini-Embedding),\\ndespite having only 0.6B parameters. In Table 3, we present the evaluation results on MTEB (English,\\nv2) (Muennighoff et al., 2023), CMTEB (Xiao et al., 2024), and MTEB (Code) (Enevoldsen et al.,\\n2025). The scores reflect similar trends as MMTEB, with our Qwen3-Embedding-4B/8B models\\n1https://hf.co/jinaai/jina-reranker-v2-base-multilingual\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nBasic Relevance Retrieval\\nModel Param MTEB-R CMTEB-R MMTEB-R MLDR MTEB-Code FollowIR\\nQwen3-Embedding-0.6B 0.6B 61.82 71.02 64.64 50.26 75.41 5.09\\nJina-multilingual-reranker-v2-base 0.3B 58.22 63.37 63.73 39.66 58.98 -0.68\\ngte-multilingual-reranker-base 0.3B 59.51 74.08 59.44 66.33 54.18 -1.64\\nBGE-reranker-v2-m3 0.6B 57.03 72.16 58.36 59.51 41.38 -0.01\\nQwen3-Reranker-0.6B 0.6B 65.80 71.31 66.36 67.28 73.42 5.41\\nQwen3-Reranker-4B 4B 69.76 75.94 72.74 69.97 81.20 14.84\\nQwen3-Reranker-8B 8B 69.02 77.45 72.94 70.19 81.22 8.05\\nTable 4: Evaluation results for reranking models. We use the retrieval subsets of MTEB(eng, v2),\\nMTEB(cmn, v1) and MMTEB, which are MTEB-R, CMTEB-R and MMTEM-R. The rest are all retrieval\\ntasks. All scores are our runs based on the retrieval top-100 results from the first row.\\nModel MMTEB MTEB (Eng, v2)CMTEB MTEB (Code, v1)\\nQwen3-Embedding-0.6B w/ only synthetic data58.49 60.63 59.78 66.79\\nQwen3-Embedding-0.6B w/o synthetic data 61.21 65.59 63.37 74.58\\nQwen3-Embedding-0.6B w/o model merge 62.56 68.18 64.76 74.89\\nQwen3-Embedding-0.6B 64.33 70.70 66.33 75.41\\nTable 5: Performance (mean task) on MMTEB, MTEB(eng, v2), CMTEB and MTEB(code, v1) for\\nQwen3-Embedding-0.6B model with different training setting.\\nconsistently outperforming others. Notably, the Qwen3-Embedding-0.6B model ranks just behind\\nthe Gemini-Embedding, while being competitive with the gte-Qwen2-7B-instruct.\\nReranking In Table 4, we present the evaluation results on various reranking tasks ( §4.1). We\\nutilize the Qwen3-Embedding-0.6B model to retrieve the top-100 candidates and then apply different\\nreranking models for further refinement. This approach ensures a fair evaluation of the reranking\\nmodels. Our results indicate that all three Qwen3-Reranker models enhance performance compared\\nto the embedding model and surpass all baseline reranking methods, with Qwen3-Reranker-8B\\nachieving the highest performance across most tasks.\\n4.3 Analysis\\nTo further analyze and explore the key elements of the Qwen3 Embedding model training framework,\\nwe conduct an analysis from the following dimensions:\\nEffectiveness of Large-Scale Weakly Supervised Pre-TrainingWe first analyze the effectiveness\\nof the large-scale weak supervised training stage for the embedding models. As shown in Table 5,\\nthe Qwen3-Embedding-0.6B model trained solely on synthetic data (without subsequent training\\nstages, as indicated in the first row) achieves reasonable and strong performance compared to the\\nfinal Qwen3-Embedding-0.6B model (as shown in the last row). If we further remove the weak\\nsupervised training stage (i.e., without synthetic data training, as seen in the second row), the final\\nperformance shows a clear decline. This indicates that the large-scale weak supervised training\\nstage is crucial for achieving superior performance.\\nEffectiveness of Model MergingNext, we compare the performance differences arising from the\\nmodel merging stage. As shown in Table 5, the model trained without model merging techniques\\n(the third row, which uses data sampling to balance various tasks) performs considerably worse\\nthan the final Qwen3-Embedding-0.6B model (which employs model merging, as shown in the last\\nrow). This indicates that the model merging stage is also critical for developing strong models.\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\n5 Conclusion\\nIn this technical report, we present the Qwen3-Embedding series, a comprehensive suite of text\\nembedding and reranking models based on the Qwen3 foundation models. These models are\\ndesigned to excel in a wide range of text embedding and reranking tasks, including multilingual\\nretrieval, code retrieval, and complex instruction following. The Qwen3-Embedding models are\\nbuilt upon a robust multi-stage training pipeline that combines large-scale weakly supervised\\npre-training on synthetic data with supervised fine-tuning and model merging on high-quality\\ndatasets. The Qwen3 LLMs play a crucial role in synthesizing diverse training data across multiple\\nlanguages and tasks, thereby enhancing the models’ capabilities. Our comprehensive evaluations\\ndemonstrate that the Qwen3-Embedding models achieve state-of-the-art performance across various\\nbenchmarks, including MTEB, CMTEB, MMTEB, and several retrieval benchmarks. We are pleased\\nto open-source the Qwen3-Embedding and Qwen3-Reranker models (0.6B, 4B, and 8B), making\\nthem available for the community to use and build upon.\\nReferences\\nJianlyu Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. M3-embedding:\\nMulti-linguality, multi-functionality, multi-granularity text embeddings through self-knowledge\\ndistillation. In Findings of the Association for Computational Linguistics: ACL 2024, pp. 2318–2335,\\nBangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://\\naclanthology.org/2024.findings-acl.137/.\\nKenneth Enevoldsen, Isaac Chung, Imene Kerboua, M´arton Kardos, Ashwin Mathur, David Stap,\\nJay Gala, Wissam Siblini, Dominik Krzemi ´nski, Genta Indra Winata, et al. MMTEB: Massive\\nmultilingual text embedding benchmark. In The Thirteenth International Conference on Learning\\nRepresentations, 2025. URL https://openreview.net/forum?id=zl3pfz4VCV.\\nTao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. Scaling synthetic data creation\\nwith 1,000,000,000 personas. arXiv preprint arXiv:2406.20094, 2024.\\nJui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padman-\\nabhan, Giuseppe Ottaviano, and Linjun Yang. Embedding-based retrieval in facebook search. In\\nProceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,\\npp. 2553–2561, 2020.\\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark,\\nAJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint\\narXiv:2410.21276, 2024.\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick SH Lewis, Ledell Wu, Sergey Edunov, Danqi\\nChen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In EMNLP\\n(1), pp. 6769–6781, 2020.\\nChankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro,\\nand Wei Ping. Nv-embed: Improved techniques for training llms as generalist embedding models.\\narXiv preprint arXiv:2405.17428, 2024.\\nChankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro,\\nand Wei Ping. NV-embed: Improved techniques for training LLMs as generalist embedding\\nmodels. In The Thirteenth International Conference on Learning Representations, 2025a. URL https:\\n//openreview.net/forum?id=lgsyLSsDRe.\\nJinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Shanbhogue, Iftekhar Naim, Gus-\\ntavo Hern´andez ´Abrego, Zhe Li, Kaifeng Chen, Henrique Schechter Vera, et al. Gemini embedding:\\nGeneralizable embeddings from gemini. arXiv preprint arXiv:2503.07891, 2025b.\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nMingxin Li, Zhijie Nie, Yanzhao Zhang, Dingkun Long, Richong Zhang, and Pengjun Xie. Improving\\ngeneral text embedding model: Tackling task conflict and data imbalance through model merging.\\narXiv preprint arXiv:2410.15035, 2024.\\nZehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards\\ngeneral text embeddings with multi-stage contrastive learning, 2023. URL https://arxiv.org/\\nabs/2308.03281.\\nXueguang Ma, Xinyu Zhang, Ronak Pradeep, and Jimmy Lin. Zero-shot listwise document reranking\\nwith a large language model. arXiv preprint arXiv:2305.02156, 2023.\\nNiklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. MTEB: Massive text embed-\\nding benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for\\nComputational Linguistics, pp. 2014–2037, Dubrovnik, Croatia, May 2023. Association for Computa-\\ntional Linguistics. URL https://aclanthology.org/2023.eacl-main.148/.\\nNiklas Muennighoff, Hongjin SU, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and\\nDouwe Kiela. Generative representational instruction tuning. In The Thirteenth International Con-\\nference on Learning Representations, 2025. URL https://openreview.net/forum?id=BC4lIvfSzv.\\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\\ntive coding. arXiv preprint arXiv:1807.03748, 2018.\\nRonak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. Rankvicuna: Zero-shot listwise docu-\\nment reranking with open-source large language models. arXiv preprint arXiv:2309.15088, 2023.\\nNils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-\\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\\nand the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp.\\n3982–3992, Hong Kong, China, November 2019. Association for Computational Linguistics. URL\\nhttps://aclanthology.org/D19-1410/.\\nHongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih,\\nNoah A Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-finetuned text\\nembeddings. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 1102–1121,\\n2023.\\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\\nand Furu Wei. Text embeddings by weakly-supervised contrastive pre-training, 2022. URL\\nhttps://arxiv.org/abs/2212.03533.\\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Im-\\nproving text embeddings with large language models. In Proceedings of the 62nd Annual Meet-\\ning of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 11897–11916,\\nBangkok, Thailand, August 2024. Association for Computational Linguistics. URL https:\\n//aclanthology.org/2024.acl-long.642/.\\nOrion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme,\\nDawn Lawrie, and Luca Soldaini. Followir: Evaluating and teaching information retrieval models\\nto follow instructions. arXiv preprint arXiv:2403.15246, 2024.\\nShitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. C-pack:\\nPacked resources for general chinese embeddings. In Proceedings of the 47th International ACM\\nSIGIR Conference on Research and Development in Information Retrieval, SIGIR ’24, pp. 641–649, New\\nYork, NY, USA, 2024. Association for Computing Machinery. URLhttps://doi.org/10.1145/\\n3626772.3657878.\\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\\nChengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025.\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nLonghui Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, and Min Zhang. A\\ntwo-stage adaptation of large language models for text ranking. In Findings of the Association for\\nComputational Linguistics ACL 2024, pp. 11880–11891, 2024a.\\nXin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong Tang, Huan Lin, Baosong\\nYang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min Zhang. mGTE: Generalized\\nlong-context text representation and reranking models for multilingual text retrieval. In Franck\\nDernoncourt, Daniel Preo t ¸iuc-Pietro, and Anastasia Shimorina (eds.), Proceedings of the 2024\\nConference on Empirical Methods in Natural Language Processing: Industry Track, pp. 1393–1412,\\nMiami, Florida, US, November 2024b. Association for Computational Linguistics. doi: 10.18653/\\nv1/2024.emnlp-industry.103. URL https://aclanthology.org/2024.emnlp-industry.103/.\\nWayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji-Rong Wen. Dense text retrieval based on pretrained\\nlanguage models: A survey. ACM Transactions on Information Systems, 42(4):1–60, 2024.\\nXiangyu Zhao, Maolin Wang, Xinjian Zhao, Jiansheng Li, Shucheng Zhou, Dawei Yin, Qing Li,\\nJiliang Tang, and Ruocheng Guo. Embedding in recommender systems: A survey. arXiv preprint\\narXiv:2310.18608, 2023.\\nShengyao Zhuang, Honglei Zhuang, Bevan Koopman, and Guido Zuccon. A setwise approach for\\neffective and highly efficient zero-shot ranking with large language models. In Proceedings of the\\n47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp.\\n38–47, 2024.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nA Appendix\\nA.1 Synthetic Data\\nWe construct four types of synthetic data—retrieval, bitext mining, semantic textual similarity, and\\nclassification to enable the model to adapt to various similarity tasks during pre-training. To ensure\\nboth multilingual and cross-lingual diversity, the data is generated using Qwen3 32B. Below is an\\nexample of a synthetic retrieval text pair. The retrieval data is synthesized using a document-to-\\nquery approach. We collect a multilingual corpus from the pre-training corpus of the Qwen3 base\\nmodel to serve as the document source. A two-stage generation pipeline is then applied, consisting\\nof: (1) configuration and (2) query generation. In the configuration stage, we use large language\\nmodels (LLMs) to determine the “Question Type”, “Difficulty”, and “Character” for the synthetic\\nquery. The candidate characters are retrieved from Persona Hub (Ge et al., 2024), selecting the top\\nfive most relevant to the given document. This step aims to enhance the diversity of the generated\\nqueries. The template used is as follows:\\nGiven a **Passage** and **Character**, select the appropriate option from\\nthree fields: Character, Question_Type, Difficulty, and return the output\\nin JSON format.\\n,→\\n,→\\nFirst, select the Character who are likely to be interested in the Passage\\nfrom the candidates. Then select the Question_Type that the Character\\nmight ask about the Passage; Finally, choose the Difficulty of the\\npossible question based on the Passage, the Character, and the\\nQuestion_Type.\\n,→\\n,→\\n,→\\n,→\\nCharacter: Given by input **Character**\\nQuestion_Type:\\n- keywords: ...\\n- acquire_knowledge: ...\\n- summary: ...\\n- yes_or_no: ...\\n- background: ...\\nDifficulty:\\n- high_school: ...\\n- university: ...\\n- phd: ...\\nHere are some examples\\n<Example1> <Example2> <Example3>\\nNow, generate the **output** based on the **Passage** and **Character** from\\nuser, the **Passage** will be in {language} language and the **Character**\\nwill be in English.\\n,→\\n,→\\nEnsure to generate only the JSON output with content in English.\\n**Passage**:\\n{passage}\\n**Character**:\\n{character}\\nIn the query generation stage, we use the configuration selected in the first stage to guide the\\ngeneration of queries. Additionally, we explicitly specify the desired length and language of the\\ngenerated query. The template used is as follows:\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content=\"Technical Report\\nGiven a **Character**, **Passage**, and **Requirement**, generate a query from\\nthe **Character**'s perspective that satisfies the **Requirement** and can\\nbe used to retrieve the **Passage**. Please return the result in JSON\\nformat.\\n,→\\n,→\\n,→\\nHere is an example:\\n<example>\\nNow, generate the **output** based on the **Character**, **Passage** and\\n**Requirement** from user, the **Passage** will be in {corpus_language}\\nlanguage, the **Character** and **Requirement** will be in English.\\n,→\\n,→\\nEnsure to generate only the JSON output, with the key in English and the value\\nin {queries_language} language.,→\\n**Character**\\n{character}\\n**Passage**\\n{passage}\\n**Requirment**\\n- Type: {type};\\n- Difficulty: {difficulty};\\n- Length: the length of the generated sentences should be {length} words;\\n- Languange: the language in which the results are generated should be\\n{language} language;,→\\nStage Dataset Size\\nWeakly Supervised Pre-Training Synthetic Data ∼ 150M\\nSupervised Fine Tuning\\nMS MARCO, NQ, HotpotQA, NLI,\\nDureader, T2-Ranking, SimCLUE,\\nMIRACL, MLDR, Mr.TyDi,\\nMulti-CPR, CodeSearchNet .etc\\n+ High-quality Synthetic Data\\nLabeled Data: ∼ 7M\\nSynthetic Data: ∼ 12M\\nTable 6: Statistics of training data utilized at each stage.\\nA.2 Detail Results\\nMTEB(eng, v2) Param Mean\\n(Task)\\nMean\\n(Type)\\nClass-\\nification\\nClus-\\ntering\\nPair\\nClass. Rerank Retrieval STS Summ.\\nmultilingual-e5-large-instruct 0.6B 65.53 61.21 75.54 49.89 86.24 48.74 53.47 84.72 29.89\\nNV-Embed-v2 7.8B 69.81 65.00 87.19 47.66 88.69 49.61 62.84 83.82 35.21\\nGritLM-7B 7.2B 67.07 63.22 81.25 50.82 87.29 49.59 54.95 83.03 35.65\\ngte-Qwen2-1.5B-instruct 1.5B 67.20 63.26 85.84 53.54 87.52 49.25 50.25 82.51 33.94\\nstellaen1.5Bv5 1.5B 69.43 65.32 89.38 57.06 88.02 50.19 52.42 83.27 36.91\\ngte-Qwen2-7B-instruct 7.6B 70.72 65.77 88.52 58.97 85.9 50.47 58.09 82.69 35.74\\ngemini-embedding-exp-03-07 - 73.3 67.67 90.05 59.39 87.7 48.59 64.35 85.29 38.28\\nQwen3-Embedding-0.6B 0.6B 70.70 64.88 85.76 54.05 84.37 48.18 61.83 86.57 33.43\\nQwen3-Embedding-4B 4B 74.60 68.09 89.84 57.51 87.01 50.76 68.46 88.72 34.39\\nQwen3-Embedding-8B 8B 75.22 68.70 90.43 58.57 87.52 51.56 69.44 88.58 34.83\\nTable 7: Results on MTEB(eng, v2) (Muennighoff et al., 2023). We compare models from the online\\nleaderboard.\\n13\"),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nMTEB(cmn, v1) Param Mean\\n(Task)\\nMean\\n(Type)\\nClass-\\nification\\nClus-\\ntering\\nPair\\nClass. Rerank Retrieval STS\\nmultilingual-e5-large-instruct 0.6B 58.08 58.24 69.80 48.23 64.52 57.45 63.65 45.81\\ngte-Qwen2-7B-instruct 7.6B 71.62 72.19 75.77 66.06 81.16 69.24 75.70 65.20\\ngte-Qwen2-1.5B-instruct 1.5B 67.12 67.79 72.53 54.61 79.5 68.21 71.86 60.05\\nQwen3-Embedding-0.6B 0.6B 66.33 67.44 71.40 68.74 76.42 62.58 71.03 54.52\\nQwen3-Embedding-4B 4B 72.26 73.50 75.46 77.89 83.34 66.05 77.03 61.26\\nQwen3-Embedding-8B 8B 73.84 75.00 76.97 80.08 84.23 66.99 78.21 63.53\\nTable 8: Results on C-MTEB (Xiao et al., 2024) (MTEB(cmn, v1).\\nMTEB(Code, v1) Avg.Apps COIR-CodeSearch-Net\\nCode-Edit-Search\\nCode-Feedback-MT\\nCode-Feedback-ST\\nCode-SearchNet-CCR\\nCode-SearchNet\\nCode-Trans-Ocean-Contest\\nCode-Trans-Ocean-DLCosQAStack-Overflow-QA\\nSynthetic-Text2SQL\\nBGEmultilingual 62.0422.93 68.14 60.48 60.52 76.70 73.23 83.43 86.84 32.64 27.93 92.93 58.67NV-Embed-v2 63.7429.72 61.85 73.96 60.27 81.72 68.82 86.61 89.14 33.40 34.82 92.36 60.90gte-Qwen2-7B-instruct 62.1728.39 71.79 67.06 57.66 85.15 66.24 86.96 81.83 32.17 31.26 84.34 53.22gte-Qwen2-1.5B-instruct 61.9828.91 71.56 59.60 49.92 81.92 72.08 91.08 79.02 32.73 32.23 90.27 54.49\\nBGE-M3 (Dense) 58.2214.77 58.07 59.83 47.86 69.27 53.55 61.98 86.22 29.37 27.36 80.71 49.65Jina-v3 58.85 28.99 67.83 57.24 59.66 78.13 54.17 85.50 77.37 30.91 35.15 90.79 41.49\\nQwen3-Embedding-0.6B 75.4175.34 84.69 64.42 90.82 86.39 91.72 91.01 86.05 31.36 36.48 89.99 76.74Qwen3-Embedding-4B 80.0689.18 87.93 76.49 93.21 89.51 95.59 92.34 90.99 35.04 37.98 94.32 78.21Qwen3-Embedding-8B 80.6891.07 89.51 76.97 93.70 89.93 96.35 92.66 93.73 32.81 38.04 94.75 78.75\\nQwen3-Reranker-0.6B 73.4269.43 85.09 72.37 83.83 78.05 94.76 88.8 84.69 33.94 36.83 93.24 62.48Qwen3-Reranker-4B 81.2094.25 90.91 82.53 95.25 88.54 97.58 92.48 93.66 36.78 35.14 97.11 75.06Qwen3-Reranker-8B 81.2294.55 91.88 84.58 95.64 88.43 95.67 92.78 90.83 34.89 37.43 97.3 73.4\\nTable 9: Performance on MTEB(Code, v1) (Enevoldsen et al., 2025). We report nDCG@10 scores.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='1\\nRetrieval-Augmented Generation for Large\\nLanguage Models: A Survey\\nYunfan Gaoa, Yun Xiongb, Xinyu Gao b, Kangxiang Jia b, Jinliu Pan b, Yuxi Bic, Yi Dai a, Jiawei Sun a, Meng\\nWangc, and Haofen Wang a,c\\naShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\\nbShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\ncCollege of Design and Innovation, Tongji University\\nAbstract—Large Language Models (LLMs) showcase impres-\\nsive capabilities but encounter challenges like hallucination,\\noutdated knowledge, and non-transparent, untraceable reasoning\\nprocesses. Retrieval-Augmented Generation (RAG) has emerged\\nas a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the\\ngeneration, particularly for knowledge-intensive tasks, and allows\\nfor continuous knowledge updates and integration of domain-\\nspecific information. RAG synergistically merges LLMs’ intrin-\\nsic knowledge with the vast, dynamic repositories of external\\ndatabases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing\\nthe Naive RAG, the Advanced RAG, and the Modular RAG.\\nIt meticulously scrutinizes the tripartite foundation of RAG\\nframeworks, which includes the retrieval, the generation and the\\naugmentation techniques. The paper highlights the state-of-the-\\nart technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG\\nsystems. Furthermore, this paper introduces up-to-date evalua-\\ntion framework and benchmark. At the end, this article delineates\\nthe challenges currently faced and points out prospective avenues\\nfor research and development 1.\\nIndex Terms—Large language model, retrieval-augmented gen-\\neration, natural language processing, information retrieval\\nI. I NTRODUCTION\\nL\\nARGE language models (LLMs) have achieved remark-\\nable success, though they still face significant limitations,\\nespecially in domain-specific or knowledge-intensive tasks [1],\\nnotably producing “hallucinations” [2] when handling queries\\nbeyond their training data or requiring current information. To\\novercome challenges, Retrieval-Augmented Generation (RAG)\\nenhances LLMs by retrieving relevant document chunks from\\nexternal knowledge base through semantic similarity calcu-\\nlation. By referencing external knowledge, RAG effectively\\nreduces the problem of generating factually incorrect content.\\nIts integration into LLMs has resulted in widespread adoption,\\nestablishing RAG as a key technology in advancing chatbots\\nand enhancing the suitability of LLMs for real-world applica-\\ntions.\\nRAG technology has rapidly developed in recent years, and\\nthe technology tree summarizing related research is shown\\nCorresponding Author.Email:haofen.wang@tongji.edu.cn\\n1Resources are available at https://github.com/Tongji-KGLLM/\\nRAG-Survey\\nin Figure 1. The development trajectory of RAG in the era\\nof large models exhibits several distinct stage characteristics.\\nInitially, RAG’s inception coincided with the rise of the\\nTransformer architecture, focusing on enhancing language\\nmodels by incorporating additional knowledge through Pre-\\nTraining Models (PTM). This early stage was characterized\\nby foundational work aimed at refining pre-training techniques\\n[3]–[5].The subsequent arrival of ChatGPT [6] marked a\\npivotal moment, with LLM demonstrating powerful in context\\nlearning (ICL) capabilities. RAG research shifted towards\\nproviding better information for LLMs to answer more com-\\nplex and knowledge-intensive tasks during the inference stage,\\nleading to rapid development in RAG studies. As research\\nprogressed, the enhancement of RAG was no longer limited\\nto the inference stage but began to incorporate more with LLM\\nfine-tuning techniques.\\nThe burgeoning field of RAG has experienced swift growth,\\nyet it has not been accompanied by a systematic synthesis that\\ncould clarify its broader trajectory. This survey endeavors to\\nfill this gap by mapping out the RAG process and charting\\nits evolution and anticipated future paths, with a focus on the\\nintegration of RAG within LLMs. This paper considers both\\ntechnical paradigms and research methods, summarizing three\\nmain research paradigms from over 100 RAG studies, and\\nanalyzing key technologies in the core stages of “Retrieval,”\\n“Generation,” and “Augmentation.” On the other hand, current\\nresearch tends to focus more on methods, lacking analysis and\\nsummarization of how to evaluate RAG. This paper compre-\\nhensively reviews the downstream tasks, datasets, benchmarks,\\nand evaluation methods applicable to RAG. Overall, this\\npaper sets out to meticulously compile and categorize the\\nfoundational technical concepts, historical progression, and\\nthe spectrum of RAG methodologies and applications that\\nhave emerged post-LLMs. It is designed to equip readers and\\nprofessionals with a detailed and structured understanding of\\nboth large models and RAG. It aims to illuminate the evolution\\nof retrieval augmentation techniques, assess the strengths and\\nweaknesses of various approaches in their respective contexts,\\nand speculate on upcoming trends and innovations.\\nOur contributions are as follows:\\n• In this survey, we present a thorough and systematic\\nreview of the state-of-the-art RAG methods, delineating\\nits evolution through paradigms including naive RAG,\\narXiv:2312.10997v5  [cs.CL]  27 Mar 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='2\\nFig. 1. Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs,\\nresearch on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent\\nresearch has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models\\nin the pre-training stage through retrieval-augmented techniques.\\nadvanced RAG, and modular RAG. This review contex-\\ntualizes the broader scope of RAG research within the\\nlandscape of LLMs.\\n• We identify and discuss the central technologies integral\\nto the RAG process, specifically focusing on the aspects\\nof “Retrieval”, “Generation” and “Augmentation”, and\\ndelve into their synergies, elucidating how these com-\\nponents intricately collaborate to form a cohesive and\\neffective RAG framework.\\n• We have summarized the current assessment methods of\\nRAG, covering 26 tasks, nearly 50 datasets, outlining\\nthe evaluation objectives and metrics, as well as the\\ncurrent evaluation benchmarks and tools. Additionally,\\nwe anticipate future directions for RAG, emphasizing\\npotential enhancements to tackle current challenges.\\nThe paper unfolds as follows: Section II introduces the\\nmain concept and current paradigms of RAG. The following\\nthree sections explore core components—“Retrieval”, “Gen-\\neration” and “Augmentation”, respectively. Section III focuses\\non optimization methods in retrieval,including indexing, query\\nand embedding optimization. Section IV concentrates on post-\\nretrieval process and LLM fine-tuning in generation. Section V\\nanalyzes the three augmentation processes. Section VI focuses\\non RAG’s downstream tasks and evaluation system. Sec-\\ntion VII mainly discusses the challenges that RAG currently\\nfaces and its future development directions. At last, the paper\\nconcludes in Section VIII.\\nII. O VERVIEW OF RAG\\nA typical application of RAG is illustrated in Figure 2.\\nHere, a user poses a question to ChatGPT about a recent,\\nwidely discussed news. Given ChatGPT’s reliance on pre-\\ntraining data, it initially lacks the capacity to provide up-\\ndates on recent developments. RAG bridges this information\\ngap by sourcing and incorporating knowledge from external\\ndatabases. In this case, it gathers relevant news articles related\\nto the user’s query. These articles, combined with the original\\nquestion, form a comprehensive prompt that empowers LLMs\\nto generate a well-informed answer.\\nThe RAG research paradigm is continuously evolving, and\\nwe categorize it into three stages: Naive RAG, Advanced\\nRAG, and Modular RAG, as showed in Figure 3. Despite\\nRAG method are cost-effective and surpass the performance\\nof the native LLM, they also exhibit several limitations.\\nThe development of Advanced RAG and Modular RAG is\\na response to these specific shortcomings in Naive RAG.\\nA. Naive RAG\\nThe Naive RAG research paradigm represents the earli-\\nest methodology, which gained prominence shortly after the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='3\\nFig. 2. A representative instance of the RAG process applied to question answering. It mainly consists of 3 steps. 1) Indexing. Documents are split into chunks,\\nencoded into vectors, and stored in a vector database. 2) Retrieval. Retrieve the Top k chunks most relevant to the question based on semantic similarity. 3)\\nGeneration. Input the original question and the retrieved chunks together into LLM to generate the final answer.\\nwidespread adoption of ChatGPT. The Naive RAG follows\\na traditional process that includes indexing, retrieval, and\\ngeneration, which is also characterized as a “Retrieve-Read”\\nframework [7].\\nIndexing starts with the cleaning and extraction of raw data\\nin diverse formats like PDF, HTML, Word, and Markdown,\\nwhich is then converted into a uniform plain text format. To\\naccommodate the context limitations of language models, text\\nis segmented into smaller, digestible chunks. Chunks are then\\nencoded into vector representations using an embedding model\\nand stored in vector database. This step is crucial for enabling\\nefficient similarity searches in the subsequent retrieval phase.\\nRetrieval. Upon receipt of a user query, the RAG system\\nemploys the same encoding model utilized during the indexing\\nphase to transform the query into a vector representation.\\nIt then computes the similarity scores between the query\\nvector and the vector of chunks within the indexed corpus.\\nThe system prioritizes and retrieves the top K chunks that\\ndemonstrate the greatest similarity to the query. These chunks\\nare subsequently used as the expanded context in prompt.\\nGeneration. The posed query and selected documents are\\nsynthesized into a coherent prompt to which a large language\\nmodel is tasked with formulating a response. The model’s\\napproach to answering may vary depending on task-specific\\ncriteria, allowing it to either draw upon its inherent parametric\\nknowledge or restrict its responses to the information con-\\ntained within the provided documents. In cases of ongoing\\ndialogues, any existing conversational history can be integrated\\ninto the prompt, enabling the model to engage in multi-turn\\ndialogue interactions effectively.\\nHowever, Naive RAG encounters notable drawbacks:\\nRetrieval Challenges . The retrieval phase often struggles\\nwith precision and recall, leading to the selection of misaligned\\nor irrelevant chunks, and the missing of crucial information.\\nGeneration Difficulties. In generating responses, the model\\nmay face the issue of hallucination, where it produces con-\\ntent not supported by the retrieved context. This phase can\\nalso suffer from irrelevance, toxicity, or bias in the outputs,\\ndetracting from the quality and reliability of the responses.\\nAugmentation Hurdles . Integrating retrieved information\\nwith the different task can be challenging, sometimes resulting\\nin disjointed or incoherent outputs. The process may also\\nencounter redundancy when similar information is retrieved\\nfrom multiple sources, leading to repetitive responses. Deter-\\nmining the significance and relevance of various passages and\\nensuring stylistic and tonal consistency add further complexity.\\nFacing complex issues, a single retrieval based on the original\\nquery may not suffice to acquire adequate context information.\\nMoreover, there’s a concern that generation models might\\noverly rely on augmented information, leading to outputs that\\nsimply echo retrieved content without adding insightful or\\nsynthesized information.\\nB. Advanced RAG\\nAdvanced RAG introduces specific improvements to over-\\ncome the limitations of Naive RAG. Focusing on enhancing re-\\ntrieval quality, it employs pre-retrieval and post-retrieval strate-\\ngies. To tackle the indexing issues, Advanced RAG refines\\nits indexing techniques through the use of a sliding window\\napproach, fine-grained segmentation, and the incorporation of\\nmetadata. Additionally, it incorporates several optimization\\nmethods to streamline the retrieval process [8].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='4\\nFig. 3. Comparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)\\nAdvanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a\\nchain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the\\nintroduction of multiple specific functional modules and the replacement of existing modules. The overall process is not limited to sequential retrieval and\\ngeneration; it includes methods such as iterative and adaptive retrieval.\\nPre-retrieval process. In this stage, the primary focus is\\non optimizing the indexing structure and the original query.\\nThe goal of optimizing indexing is to enhance the quality of\\nthe content being indexed. This involves strategies: enhancing\\ndata granularity, optimizing index structures, adding metadata,\\nalignment optimization, and mixed retrieval. While the goal\\nof query optimization is to make the user’s original question\\nclearer and more suitable for the retrieval task. Common\\nmethods include query rewriting query transformation, query\\nexpansion and other techniques [7], [9]–[11].\\nPost-Retrieval Process. Once relevant context is retrieved,\\nit’s crucial to integrate it effectively with the query. The main\\nmethods in post-retrieval process include rerank chunks and\\ncontext compressing. Re-ranking the retrieved information to\\nrelocate the most relevant content to the edges of the prompt is\\na key strategy. This concept has been implemented in frame-\\nworks such as LlamaIndex 2, LangChain3, and HayStack [12].\\nFeeding all relevant documents directly into LLMs can lead\\nto information overload, diluting the focus on key details with\\nirrelevant content.To mitigate this, post-retrieval efforts con-\\ncentrate on selecting the essential information, emphasizing\\ncritical sections, and shortening the context to be processed.\\n2https://www.llamaindex.ai\\n3https://www.langchain.com/\\nC. Modular RAG\\nThe modular RAG architecture advances beyond the for-\\nmer two RAG paradigms, offering enhanced adaptability and\\nversatility. It incorporates diverse strategies for improving its\\ncomponents, such as adding a search module for similarity\\nsearches and refining the retriever through fine-tuning. Inno-\\nvations like restructured RAG modules [13] and rearranged\\nRAG pipelines [14] have been introduced to tackle specific\\nchallenges. The shift towards a modular RAG approach is\\nbecoming prevalent, supporting both sequential processing and\\nintegrated end-to-end training across its components. Despite\\nits distinctiveness, Modular RAG builds upon the foundational\\nprinciples of Advanced and Naive RAG, illustrating a progres-\\nsion and refinement within the RAG family.\\n1) New Modules: The Modular RAG framework introduces\\nadditional specialized components to enhance retrieval and\\nprocessing capabilities. The Search module adapts to spe-\\ncific scenarios, enabling direct searches across various data\\nsources like search engines, databases, and knowledge graphs,\\nusing LLM-generated code and query languages [15]. RAG-\\nFusion addresses traditional search limitations by employing\\na multi-query strategy that expands user queries into diverse\\nperspectives, utilizing parallel vector searches and intelligent\\nre-ranking to uncover both explicit and transformative knowl-\\nedge [16]. The Memory module leverages the LLM’s memory\\nto guide retrieval, creating an unbounded memory pool that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='5\\naligns the text more closely with data distribution through iter-\\native self-enhancement [17], [18]. Routing in the RAG system\\nnavigates through diverse data sources, selecting the optimal\\npathway for a query, whether it involves summarization,\\nspecific database searches, or merging different information\\nstreams [19]. The Predict module aims to reduce redundancy\\nand noise by generating context directly through the LLM,\\nensuring relevance and accuracy [13]. Lastly, the Task Adapter\\nmodule tailors RAG to various downstream tasks, automating\\nprompt retrieval for zero-shot inputs and creating task-specific\\nretrievers through few-shot query generation [20], [21] .This\\ncomprehensive approach not only streamlines the retrieval pro-\\ncess but also significantly improves the quality and relevance\\nof the information retrieved, catering to a wide array of tasks\\nand queries with enhanced precision and flexibility.\\n2) New Patterns: Modular RAG offers remarkable adapt-\\nability by allowing module substitution or reconfiguration\\nto address specific challenges. This goes beyond the fixed\\nstructures of Naive and Advanced RAG, characterized by a\\nsimple “Retrieve” and “Read” mechanism. Moreover, Modular\\nRAG expands this flexibility by integrating new modules or\\nadjusting interaction flow among existing ones, enhancing its\\napplicability across different tasks.\\nInnovations such as the Rewrite-Retrieve-Read [7]model\\nleverage the LLM’s capabilities to refine retrieval queries\\nthrough a rewriting module and a LM-feedback mechanism\\nto update rewriting model., improving task performance.\\nSimilarly, approaches like Generate-Read [13] replace tradi-\\ntional retrieval with LLM-generated content, while Recite-\\nRead [22] emphasizes retrieval from model weights, enhanc-\\ning the model’s ability to handle knowledge-intensive tasks.\\nHybrid retrieval strategies integrate keyword, semantic, and\\nvector searches to cater to diverse queries. Additionally, em-\\nploying sub-queries and hypothetical document embeddings\\n(HyDE) [11] seeks to improve retrieval relevance by focusing\\non embedding similarities between generated answers and real\\ndocuments.\\nAdjustments in module arrangement and interaction, such\\nas the Demonstrate-Search-Predict (DSP) [23] framework\\nand the iterative Retrieve-Read-Retrieve-Read flow of ITER-\\nRETGEN [14], showcase the dynamic use of module out-\\nputs to bolster another module’s functionality, illustrating a\\nsophisticated understanding of enhancing module synergy.\\nThe flexible orchestration of Modular RAG Flow showcases\\nthe benefits of adaptive retrieval through techniques such as\\nFLARE [24] and Self-RAG [25]. This approach transcends\\nthe fixed RAG retrieval process by evaluating the necessity\\nof retrieval based on different scenarios. Another benefit of\\na flexible architecture is that the RAG system can more\\neasily integrate with other technologies (such as fine-tuning\\nor reinforcement learning) [26]. For example, this can involve\\nfine-tuning the retriever for better retrieval results, fine-tuning\\nthe generator for more personalized outputs, or engaging in\\ncollaborative fine-tuning [27].\\nD. RAG vs Fine-tuning\\nThe augmentation of LLMs has attracted considerable atten-\\ntion due to their growing prevalence. Among the optimization\\nmethods for LLMs, RAG is often compared with Fine-tuning\\n(FT) and prompt engineering. Each method has distinct charac-\\nteristics as illustrated in Figure 4. We used a quadrant chart to\\nillustrate the differences among three methods in two dimen-\\nsions: external knowledge requirements and model adaption\\nrequirements. Prompt engineering leverages a model’s inherent\\ncapabilities with minimum necessity for external knowledge\\nand model adaption. RAG can be likened to providing a model\\nwith a tailored textbook for information retrieval, ideal for pre-\\ncise information retrieval tasks. In contrast, FT is comparable\\nto a student internalizing knowledge over time, suitable for\\nscenarios requiring replication of specific structures, styles, or\\nformats.\\nRAG excels in dynamic environments by offering real-\\ntime knowledge updates and effective utilization of external\\nknowledge sources with high interpretability. However, it\\ncomes with higher latency and ethical considerations regarding\\ndata retrieval. On the other hand, FT is more static, requiring\\nretraining for updates but enabling deep customization of the\\nmodel’s behavior and style. It demands significant compu-\\ntational resources for dataset preparation and training, and\\nwhile it can reduce hallucinations, it may face challenges with\\nunfamiliar data.\\nIn multiple evaluations of their performance on various\\nknowledge-intensive tasks across different topics, [28] re-\\nvealed that while unsupervised fine-tuning shows some im-\\nprovement, RAG consistently outperforms it, for both exist-\\ning knowledge encountered during training and entirely new\\nknowledge. Additionally, it was found that LLMs struggle\\nto learn new factual information through unsupervised fine-\\ntuning. The choice between RAG and FT depends on the\\nspecific needs for data dynamics, customization, and com-\\nputational capabilities in the application context. RAG and\\nFT are not mutually exclusive and can complement each\\nother, enhancing a model’s capabilities at different levels.\\nIn some instances, their combined use may lead to optimal\\nperformance. The optimization process involving RAG and FT\\nmay require multiple iterations to achieve satisfactory results.\\nIII. R ETRIEVAL\\nIn the context of RAG, it is crucial to efficiently retrieve\\nrelevant documents from the data source. There are several\\nkey issues involved, such as the retrieval source, retrieval\\ngranularity, pre-processing of the retrieval, and selection of\\nthe corresponding embedding model.\\nA. Retrieval Source\\nRAG relies on external knowledge to enhance LLMs, while\\nthe type of retrieval source and the granularity of retrieval\\nunits both affect the final generation results.\\n1) Data Structure: Initially, text is s the mainstream source\\nof retrieval. Subsequently, the retrieval source expanded to in-\\nclude semi-structured data (PDF) and structured data (Knowl-\\nedge Graph, KG) for enhancement. In addition to retrieving\\nfrom original external sources, there is also a growing trend in\\nrecent researches towards utilizing content generated by LLMs\\nthemselves for retrieval and enhancement purposes.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='6\\nTABLE I\\nSUMMARY OF RAG METHODS\\nMethod Retrieval Source Retrieval\\nData Type\\nRetrieval\\nGranularity\\nAugmentation\\nStage\\nRetrieval\\nprocess\\nCoG [29] Wikipedia Text Phrase Pre-training Iterative\\nDenseX [30] FactoidWiki Text Proposition Inference Once\\nEAR [31] Dataset-base Text Sentence Tuning Once\\nUPRISE [20] Dataset-base Text Sentence Tuning Once\\nRAST [32] Dataset-base Text Sentence Tuning Once\\nSelf-Mem [17] Dataset-base Text Sentence Tuning Iterative\\nFLARE [24] Search Engine,Wikipedia Text Sentence Tuning Adaptive\\nPGRA [33] Wikipedia Text Sentence Inference Once\\nFILCO [34] Wikipedia Text Sentence Inference Once\\nRADA [35] Dataset-base Text Sentence Inference Once\\nFilter-rerank [36] Synthesized dataset Text Sentence Inference Once\\nR-GQA [37] Dataset-base Text Sentence Pair Tuning Once\\nLLM-R [38] Dataset-base Text Sentence Pair Inference Iterative\\nTIGER [39] Dataset-base Text Item-base Pre-training Once\\nLM-Indexer [40] Dataset-base Text Item-base Tuning Once\\nBEQUE [9] Dataset-base Text Item-base Tuning Once\\nCT-RAG [41] Synthesized dataset Text Item-base Tuning Once\\nAtlas [42] Wikipedia, Common Crawl Text Chunk Pre-training Iterative\\nRA VEN [43] Wikipedia Text Chunk Pre-training Once\\nRETRO++ [44] Pre-training Corpus Text Chunk Pre-training Iterative\\nINSTRUCTRETRO [45] Pre-training corpus Text Chunk Pre-training Iterative\\nRRR [7] Search Engine Text Chunk Tuning Once\\nRA-e2e [46] Dataset-base Text Chunk Tuning Once\\nPROMPTAGATOR [21] BEIR Text Chunk Tuning Once\\nAAR [47] MSMARCO,Wikipedia Text Chunk Tuning Once\\nRA-DIT [27] Common Crawl,Wikipedia Text Chunk Tuning Once\\nRAG-Robust [48] Wikipedia Text Chunk Tuning Once\\nRA-Long-Form [49] Dataset-base Text Chunk Tuning Once\\nCoN [50] Wikipedia Text Chunk Tuning Once\\nSelf-RAG [25] Wikipedia Text Chunk Tuning Adaptive\\nBGM [26] Wikipedia Text Chunk Inference Once\\nCoQ [51] Wikipedia Text Chunk Inference Iterative\\nToken-Elimination [52] Wikipedia Text Chunk Inference Once\\nPaperQA [53] Arxiv,Online Database,PubMed Text Chunk Inference Iterative\\nNoiseRAG [54] FactoidWiki Text Chunk Inference Once\\nIAG [55] Search Engine,Wikipedia Text Chunk Inference Once\\nNoMIRACL [56] Wikipedia Text Chunk Inference Once\\nToC [57] Search Engine,Wikipedia Text Chunk Inference Recursive\\nSKR [58] Dataset-base,Wikipedia Text Chunk Inference Adaptive\\nITRG [59] Wikipedia Text Chunk Inference Iterative\\nRAG-LongContext [60] Dataset-base Text Chunk Inference Once\\nITER-RETGEN [14] Wikipedia Text Chunk Inference Iterative\\nIRCoT [61] Wikipedia Text Chunk Inference Recursive\\nLLM-Knowledge-Boundary [62] Wikipedia Text Chunk Inference Once\\nRAPTOR [63] Dataset-base Text Chunk Inference Recursive\\nRECITE [22] LLMs Text Chunk Inference Once\\nICRALM [64] Pile,Wikipedia Text Chunk Inference Iterative\\nRetrieve-and-Sample [65] Dataset-base Text Doc Tuning Once\\nZemi [66] C4 Text Doc Tuning Once\\nCRAG [67] Arxiv Text Doc Inference Once\\n1-PAGER [68] Wikipedia Text Doc Inference Iterative\\nPRCA [69] Dataset-base Text Doc Inference Once\\nQLM-Doc-ranking [70] Dataset-base Text Doc Inference Once\\nRecomp [71] Wikipedia Text Doc Inference Once\\nDSP [23] Wikipedia Text Doc Inference Iterative\\nRePLUG [72] Pile Text Doc Inference Once\\nARM-RAG [73] Dataset-base Text Doc Inference Iterative\\nGenRead [13] LLMs Text Doc Inference Iterative\\nUniMS-RAG [74] Dataset-base Text Multi Tuning Once\\nCREA-ICL [19] Dataset-base Crosslingual,Text Sentence Inference Once\\nPKG [75] LLM Tabular,Text Chunk Inference Once\\nSANTA [76] Dataset-base Code,Text Item Pre-training Once\\nSURGE [77] Freebase KG Sub-Graph Tuning Once\\nMK-ToD [78] Dataset-base KG Entity Tuning Once\\nDual-Feedback-ToD [79] Dataset-base KG Entity Sequence Tuning Once\\nKnowledGPT [15] Dataset-base KG Triplet Inference Muti-time\\nFABULA [80] Dataset-base,Graph KG Entity Inference Once\\nHyKGE [81] CMeKG KG Entity Inference Once\\nKALMV [82] Wikipedia KG Triplet Inference Iterative\\nRoG [83] Freebase KG Triplet Inference Iterative\\nG-Retriever [84] Dataset-base TextGraph Sub-Graph Inference Once'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='7\\nFig. 4. RAG compared with other model optimization methods in the aspects of “External Knowledge Required” and “Model Adaption Required”. Prompt\\nEngineering requires low modifications to the model and external knowledge, focusing on harnessing the capabilities of LLMs themselves. Fine-tuning, on\\nthe other hand, involves further training the model. In the early stages of RAG (Naive RAG), there is a low demand for model modifications. As research\\nprogresses, Modular RAG has become more integrated with fine-tuning techniques.\\nUnstructured Data , such as text, is the most widely used\\nretrieval source, which are mainly gathered from corpus. For\\nopen-domain question-answering (ODQA) tasks, the primary\\nretrieval sources are Wikipedia Dump with the current major\\nversions including HotpotQA 4 (1st October , 2017), DPR5 (20\\nDecember, 2018). In addition to encyclopedic data, common\\nunstructured data includes cross-lingual text [19] and domain-\\nspecific data (such as medical [67]and legal domains [29]).\\nSemi-structured data. typically refers to data that contains a\\ncombination of text and table information, such as PDF. Han-\\ndling semi-structured data poses challenges for conventional\\nRAG systems due to two main reasons. Firstly, text splitting\\nprocesses may inadvertently separate tables, leading to data\\ncorruption during retrieval. Secondly, incorporating tables into\\nthe data can complicate semantic similarity searches. When\\ndealing with semi-structured data, one approach involves lever-\\naging the code capabilities of LLMs to execute Text-2-SQL\\nqueries on tables within databases, such as TableGPT [85].\\nAlternatively, tables can be transformed into text format for\\nfurther analysis using text-based methods [75]. However, both\\nof these methods are not optimal solutions, indicating substan-\\ntial research opportunities in this area.\\nStructured data , such as knowledge graphs (KGs) [86] ,\\nwhich are typically verified and can provide more precise in-\\nformation. KnowledGPT [15] generates KB search queries and\\nstores knowledge in a personalized base, enhancing the RAG\\nmodel’s knowledge richness. In response to the limitations of\\nLLMs in understanding and answering questions about textual\\ngraphs, G-Retriever [84] integrates Graph Neural Networks\\n4https://hotpotqa.github.io/wiki-readme.html\\n5https://github.com/facebookresearch/DPR\\n(GNNs), LLMs and RAG, enhancing graph comprehension\\nand question-answering capabilities through soft prompting\\nof the LLM, and employs the Prize-Collecting Steiner Tree\\n(PCST) optimization problem for targeted graph retrieval. On\\nthe contrary, it requires additional effort to build, validate,\\nand maintain structured databases. On the contrary, it requires\\nadditional effort to build, validate, and maintain structured\\ndatabases.\\nLLMs-Generated Content. Addressing the limitations of\\nexternal auxiliary information in RAG, some research has\\nfocused on exploiting LLMs’ internal knowledge. SKR [58]\\nclassifies questions as known or unknown, applying retrieval\\nenhancement selectively. GenRead [13] replaces the retriever\\nwith an LLM generator, finding that LLM-generated contexts\\noften contain more accurate answers due to better alignment\\nwith the pre-training objectives of causal language modeling.\\nSelfmem [17] iteratively creates an unbounded memory pool\\nwith a retrieval-enhanced generator, using a memory selec-\\ntor to choose outputs that serve as dual problems to the\\noriginal question, thus self-enhancing the generative model.\\nThese methodologies underscore the breadth of innovative\\ndata source utilization in RAG, striving to improve model\\nperformance and task effectiveness.\\n2) Retrieval Granularity: Another important factor besides\\nthe data format of the retrieval source is the granularity of\\nthe retrieved data. Coarse-grained retrieval units theoretically\\ncan provide more relevant information for the problem, but\\nthey may also contain redundant content, which could distract\\nthe retriever and language models in downstream tasks [50],\\n[87]. On the other hand, fine-grained retrieval unit granularity\\nincreases the burden of retrieval and does not guarantee seman-\\ntic integrity and meeting the required knowledge. Choosing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='8\\nthe appropriate retrieval granularity during inference can be\\na simple and effective strategy to improve the retrieval and\\ndownstream task performance of dense retrievers.\\nIn text, retrieval granularity ranges from fine to coarse,\\nincluding Token, Phrase, Sentence, Proposition, Chunks, Doc-\\nument. Among them, DenseX [30]proposed the concept of\\nusing propositions as retrieval units. Propositions are defined\\nas atomic expressions in the text, each encapsulating a unique\\nfactual segment and presented in a concise, self-contained nat-\\nural language format. This approach aims to enhance retrieval\\nprecision and relevance. On the Knowledge Graph (KG),\\nretrieval granularity includes Entity, Triplet, and sub-Graph.\\nThe granularity of retrieval can also be adapted to downstream\\ntasks, such as retrieving Item IDs [40]in recommendation tasks\\nand Sentence pairs [38]. Detailed information is illustrated in\\nTable I.\\nB. Indexing Optimization\\nIn the Indexing phase, documents will be processed, seg-\\nmented, and transformed into Embeddings to be stored in a\\nvector database. The quality of index construction determines\\nwhether the correct context can be obtained in the retrieval\\nphase.\\n1) Chunking Strategy: The most common method is to split\\nthe document into chunks on a fixed number of tokens (e.g.,\\n100, 256, 512) [88]. Larger chunks can capture more context,\\nbut they also generate more noise, requiring longer processing\\ntime and higher costs. While smaller chunks may not fully\\nconvey the necessary context, they do have less noise. How-\\never, chunks leads to truncation within sentences, prompting\\nthe optimization of a recursive splits and sliding window meth-\\nods, enabling layered retrieval by merging globally related\\ninformation across multiple retrieval processes [89]. Never-\\ntheless, these approaches still cannot strike a balance between\\nsemantic completeness and context length. Therefore, methods\\nlike Small2Big have been proposed, where sentences (small)\\nare used as the retrieval unit, and the preceding and following\\nsentences are provided as (big) context to LLMs [90].\\n2) Metadata Attachments: Chunks can be enriched with\\nmetadata information such as page number, file name, au-\\nthor,category timestamp. Subsequently, retrieval can be filtered\\nbased on this metadata, limiting the scope of the retrieval.\\nAssigning different weights to document timestamps during\\nretrieval can achieve time-aware RAG, ensuring the freshness\\nof knowledge and avoiding outdated information.\\nIn addition to extracting metadata from the original doc-\\numents, metadata can also be artificially constructed. For\\nexample, adding summaries of paragraph, as well as intro-\\nducing hypothetical questions. This method is also known as\\nReverse HyDE. Specifically, using LLM to generate questions\\nthat can be answered by the document, then calculating the\\nsimilarity between the original question and the hypothetical\\nquestion during retrieval to reduce the semantic gap between\\nthe question and the answer.\\n3) Structural Index: One effective method for enhancing\\ninformation retrieval is to establish a hierarchical structure for\\nthe documents. By constructing In structure, RAG system can\\nexpedite the retrieval and processing of pertinent data.\\nHierarchical index structure . File are arranged in parent-\\nchild relationships, with chunks linked to them. Data sum-\\nmaries are stored at each node, aiding in the swift traversal\\nof data and assisting the RAG system in determining which\\nchunks to extract. This approach can also mitigate the illusion\\ncaused by block extraction issues.\\nKnowledge Graph index . Utilize KG in constructing the\\nhierarchical structure of documents contributes to maintaining\\nconsistency. It delineates the connections between different\\nconcepts and entities, markedly reducing the potential for\\nillusions. Another advantage is the transformation of the\\ninformation retrieval process into instructions that LLM can\\ncomprehend, thereby enhancing the accuracy of knowledge\\nretrieval and enabling LLM to generate contextually coherent\\nresponses, thus improving the overall efficiency of the RAG\\nsystem. To capture the logical relationship between document\\ncontent and structure, KGP [91] proposed a method of building\\nan index between multiple documents using KG. This KG\\nconsists of nodes (representing paragraphs or structures in the\\ndocuments, such as pages and tables) and edges (indicating\\nsemantic/lexical similarity between paragraphs or relationships\\nwithin the document structure), effectively addressing knowl-\\nedge retrieval and reasoning problems in a multi-document\\nenvironment.\\nC. Query Optimization\\nOne of the primary challenges with Naive RAG is its\\ndirect reliance on the user’s original query as the basis for\\nretrieval. Formulating a precise and clear question is difficult,\\nand imprudent queries result in subpar retrieval effectiveness.\\nSometimes, the question itself is complex, and the language\\nis not well-organized. Another difficulty lies in language\\ncomplexity ambiguity. Language models often struggle when\\ndealing with specialized vocabulary or ambiguous abbrevi-\\nations with multiple meanings. For instance, they may not\\ndiscern whether “LLM” refers to large language model or a\\nMaster of Laws in a legal context.\\n1) Query Expansion: Expanding a single query into mul-\\ntiple queries enriches the content of the query, providing\\nfurther context to address any lack of specific nuances, thereby\\nensuring the optimal relevance of the generated answers.\\nMulti-Query. By employing prompt engineering to expand\\nqueries via LLMs, these queries can then be executed in\\nparallel. The expansion of queries is not random, but rather\\nmeticulously designed.\\nSub-Query. The process of sub-question planning represents\\nthe generation of the necessary sub-questions to contextualize\\nand fully answer the original question when combined. This\\nprocess of adding relevant context is, in principle, similar\\nto query expansion. Specifically, a complex question can be\\ndecomposed into a series of simpler sub-questions using the\\nleast-to-most prompting method [92].\\nChain-of-Verification(CoVe). The expanded queries undergo\\nvalidation by LLM to achieve the effect of reducing halluci-\\nnations. Validated expanded queries typically exhibit higher\\nreliability [93].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='9\\n2) Query Transformation: The core concept is to retrieve\\nchunks based on a transformed query instead of the user’s\\noriginal query.\\nQuery Rewrite.The original queries are not always optimal\\nfor LLM retrieval, especially in real-world scenarios. There-\\nfore, we can prompt LLM to rewrite the queries. In addition to\\nusing LLM for query rewriting, specialized smaller language\\nmodels, such as RRR (Rewrite-retrieve-read) [7]. The imple-\\nmentation of the query rewrite method in the Taobao, known\\nas BEQUE [9] has notably enhanced recall effectiveness for\\nlong-tail queries, resulting in a rise in GMV .\\nAnother query transformation method is to use prompt\\nengineering to let LLM generate a query based on the original\\nquery for subsequent retrieval. HyDE [11] construct hypothet-\\nical documents (assumed answers to the original query). It\\nfocuses on embedding similarity from answer to answer rather\\nthan seeking embedding similarity for the problem or query.\\nUsing the Step-back Prompting method [10], the original\\nquery is abstracted to generate a high-level concept question\\n(step-back question). In the RAG system, both the step-back\\nquestion and the original query are used for retrieval, and both\\nthe results are utilized as the basis for language model answer\\ngeneration.\\n3) Query Routing: Based on varying queries, routing to\\ndistinct RAG pipeline,which is suitable for a versatile RAG\\nsystem designed to accommodate diverse scenarios.\\nMetadata Router/ Filter . The first step involves extracting\\nkeywords (entity) from the query, followed by filtering based\\non the keywords and metadata within the chunks to narrow\\ndown the search scope.\\nSemantic Router is another method of routing involves\\nleveraging the semantic information of the query. Specific\\napprach see Semantic Router 6. Certainly, a hybrid routing\\napproach can also be employed, combining both semantic and\\nmetadata-based methods for enhanced query routing.\\nD. Embedding\\nIn RAG, retrieval is achieved by calculating the similarity\\n(e.g. cosine similarity) between the embeddings of the ques-\\ntion and document chunks, where the semantic representation\\ncapability of embedding models plays a key role. This mainly\\nincludes a sparse encoder (BM25) and a dense retriever (BERT\\narchitecture Pre-training language models). Recent research\\nhas introduced prominent embedding models such as AngIE,\\nV oyage, BGE,etc [94]–[96], which are benefit from multi-task\\ninstruct tuning. Hugging Face’s MTEB leaderboard 7 evaluates\\nembedding models across 8 tasks, covering 58 datasests. Ad-\\nditionally, C-MTEB focuses on Chinese capability, covering\\n6 tasks and 35 datasets. There is no one-size-fits-all answer\\nto “which embedding model to use.” However, some specific\\nmodels are better suited for particular use cases.\\n1) Mix/hybrid Retrieval : Sparse and dense embedding\\napproaches capture different relevance features and can ben-\\nefit from each other by leveraging complementary relevance\\ninformation. For instance, sparse retrieval models can be used\\n6https://github.com/aurelio-labs/semantic-router\\n7https://huggingface.co/spaces/mteb/leaderboard\\nto provide initial search results for training dense retrieval\\nmodels. Additionally, pre-training language models (PLMs)\\ncan be utilized to learn term weights to enhance sparse\\nretrieval. Specifically, it also demonstrates that sparse retrieval\\nmodels can enhance the zero-shot retrieval capability of dense\\nretrieval models and assist dense retrievers in handling queries\\ncontaining rare entities, thereby improving robustness.\\n2) Fine-tuning Embedding Model: In instances where the\\ncontext significantly deviates from pre-training corpus, partic-\\nularly within highly specialized disciplines such as healthcare,\\nlegal practice, and other sectors replete with proprietary jargon,\\nfine-tuning the embedding model on your own domain dataset\\nbecomes essential to mitigate such discrepancies.\\nIn addition to supplementing domain knowledge, another\\npurpose of fine-tuning is to align the retriever and generator,\\nfor example, using the results of LLM as the supervision signal\\nfor fine-tuning, known as LSR (LM-supervised Retriever).\\nPROMPTAGATOR [21] utilizes the LLM as a few-shot query\\ngenerator to create task-specific retrievers, addressing chal-\\nlenges in supervised fine-tuning, particularly in data-scarce\\ndomains. Another approach, LLM-Embedder [97], exploits\\nLLMs to generate reward signals across multiple downstream\\ntasks. The retriever is fine-tuned with two types of supervised\\nsignals: hard labels for the dataset and soft rewards from\\nthe LLMs. This dual-signal approach fosters a more effective\\nfine-tuning process, tailoring the embedding model to diverse\\ndownstream applications. REPLUG [72] utilizes a retriever\\nand an LLM to calculate the probability distributions of the\\nretrieved documents and then performs supervised training\\nby computing the KL divergence. This straightforward and\\neffective training method enhances the performance of the\\nretrieval model by using an LM as the supervisory signal,\\neliminating the need for specific cross-attention mechanisms.\\nMoreover, inspired by RLHF (Reinforcement Learning from\\nHuman Feedback), utilizing LM-based feedback to reinforce\\nthe retriever through reinforcement learning.\\nE. Adapter\\nFine-tuning models may present challenges, such as in-\\ntegrating functionality through an API or addressing con-\\nstraints arising from limited local computational resources.\\nConsequently, some approaches opt to incorporate an external\\nadapter to aid in alignment.\\nTo optimize the multi-task capabilities of LLM, UP-\\nRISE [20] trained a lightweight prompt retriever that can\\nautomatically retrieve prompts from a pre-built prompt pool\\nthat are suitable for a given zero-shot task input. AAR\\n(Augmentation-Adapted Retriver) [47] introduces a universal\\nadapter designed to accommodate multiple downstream tasks.\\nWhile PRCA [69] add a pluggable reward-driven contextual\\nadapter to enhance performance on specific tasks. BGM [26]\\nkeeps the retriever and LLM fixed,and trains a bridge Seq2Seq\\nmodel in between. The bridge model aims to transform the\\nretrieved information into a format that LLMs can work with\\neffectively, allowing it to not only rerank but also dynami-\\ncally select passages for each query, and potentially employ\\nmore advanced strategies like repetition. Furthermore, PKG'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='10\\nintroduces an innovative method for integrating knowledge\\ninto white-box models via directive fine-tuning [75]. In this\\napproach, the retriever module is directly substituted to gen-\\nerate relevant documents according to a query. This method\\nassists in addressing the difficulties encountered during the\\nfine-tuning process and enhances model performance.\\nIV. G ENERATION\\nAfter retrieval, it is not a good practice to directly input all\\nthe retrieved information to the LLM for answering questions.\\nFollowing will introduce adjustments from two perspectives:\\nadjusting the retrieved content and adjusting the LLM.\\nA. Context Curation\\nRedundant information can interfere with the final gener-\\nation of LLM, and overly long contexts can also lead LLM\\nto the “Lost in the middle” problem [98]. Like humans, LLM\\ntends to only focus on the beginning and end of long texts,\\nwhile forgetting the middle portion. Therefore, in the RAG\\nsystem, we typically need to further process the retrieved\\ncontent.\\n1) Reranking: Reranking fundamentally reorders document\\nchunks to highlight the most pertinent results first, effectively\\nreducing the overall document pool, severing a dual purpose\\nin information retrieval, acting as both an enhancer and a\\nfilter, delivering refined inputs for more precise language\\nmodel processing [70]. Reranking can be performed using\\nrule-based methods that depend on predefined metrics like\\nDiversity, Relevance, and MRR, or model-based approaches\\nlike Encoder-Decoder models from the BERT series (e.g.,\\nSpanBERT), specialized reranking models such as Cohere\\nrerank or bge-raranker-large, and general large language mod-\\nels like GPT [12], [99].\\n2) Context Selection/Compression: A common misconcep-\\ntion in the RAG process is the belief that retrieving as many\\nrelevant documents as possible and concatenating them to form\\na lengthy retrieval prompt is beneficial. However, excessive\\ncontext can introduce more noise, diminishing the LLM’s\\nperception of key information .\\n(Long) LLMLingua [100], [101] utilize small language\\nmodels (SLMs) such as GPT-2 Small or LLaMA-7B, to\\ndetect and remove unimportant tokens, transforming it into\\na form that is challenging for humans to comprehend but\\nwell understood by LLMs. This approach presents a direct\\nand practical method for prompt compression, eliminating the\\nneed for additional training of LLMs while balancing language\\nintegrity and compression ratio. PRCA tackled this issue by\\ntraining an information extractor [69]. Similarly, RECOMP\\nadopts a comparable approach by training an information\\ncondenser using contrastive learning [71]. Each training data\\npoint consists of one positive sample and five negative sam-\\nples, and the encoder undergoes training using contrastive loss\\nthroughout this process [102] .\\nIn addition to compressing the context, reducing the num-\\nber of documents aslo helps improve the accuracy of the\\nmodel’s answers. Ma et al. [103] propose the “Filter-Reranker”\\nparadigm, which combines the strengths of LLMs and SLMs.\\nIn this paradigm, SLMs serve as filters, while LLMs function\\nas reordering agents. The research shows that instructing\\nLLMs to rearrange challenging samples identified by SLMs\\nleads to significant improvements in various Information\\nExtraction (IE) tasks. Another straightforward and effective\\napproach involves having the LLM evaluate the retrieved\\ncontent before generating the final answer. This allows the\\nLLM to filter out documents with poor relevance through LLM\\ncritique. For instance, in Chatlaw [104], the LLM is prompted\\nto self-suggestion on the referenced legal provisions to assess\\ntheir relevance.\\nB. LLM Fine-tuning\\nTargeted fine-tuning based on the scenario and data char-\\nacteristics on LLMs can yield better results. This is also one\\nof the greatest advantages of using on-premise LLMs. When\\nLLMs lack data in a specific domain, additional knowledge can\\nbe provided to the LLM through fine-tuning. Huggingface’s\\nfine-tuning data can also be used as an initial step.\\nAnother benefit of fine-tuning is the ability to adjust the\\nmodel’s input and output. For example, it can enable LLM to\\nadapt to specific data formats and generate responses in a par-\\nticular style as instructed [37]. For retrieval tasks that engage\\nwith structured data, the SANTA framework [76] implements\\na tripartite training regimen to effectively encapsulate both\\nstructural and semantic nuances. The initial phase focuses on\\nthe retriever, where contrastive learning is harnessed to refine\\nthe query and document embeddings.\\nAligning LLM outputs with human or retriever preferences\\nthrough reinforcement learning is a potential approach. For\\ninstance, manually annotating the final generated answers\\nand then providing feedback through reinforcement learning.\\nIn addition to aligning with human preferences, it is also\\npossible to align with the preferences of fine-tuned models\\nand retrievers [79]. When circumstances prevent access to\\npowerful proprietary models or larger parameter open-source\\nmodels, a simple and effective method is to distill the more\\npowerful models(e.g. GPT-4). Fine-tuning of LLM can also\\nbe coordinated with fine-tuning of the retriever to align pref-\\nerences. A typical approach, such as RA-DIT [27], aligns the\\nscoring functions between Retriever and Generator using KL\\ndivergence.\\nV. A UGMENTATION PROCESS IN RAG\\nIn the domain of RAG, the standard practice often involves\\na singular (once) retrieval step followed by generation, which\\ncan lead to inefficiencies and sometimes is typically insuffi-\\ncient for complex problems demanding multi-step reasoning,\\nas it provides a limited scope of information [105]. Many\\nstudies have optimized the retrieval process in response to this\\nissue, and we have summarised them in Figure 5.\\nA. Iterative Retrieval\\nIterative retrieval is a process where the knowledge base\\nis repeatedly searched based on the initial query and the text\\ngenerated so far, providing a more comprehensive knowledge'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='11\\nFig. 5. In addition to the most common once retrieval, RAG also includes three types of retrieval augmentation processes. (left) Iterative retrieval involves\\nalternating between retrieval and generation, allowing for richer and more targeted context from the knowledge base at each step. (Middle) Recursive retrieval\\ninvolves gradually refining the user query and breaking down the problem into sub-problems, then continuously solving complex problems through retrieval\\nand generation. (Right) Adaptive retrieval focuses on enabling the RAG system to autonomously determine whether external knowledge retrieval is necessary\\nand when to stop retrieval and generation, often utilizing LLM-generated special tokens for control.\\nbase for LLMs. This approach has been shown to enhance\\nthe robustness of subsequent answer generation by offering\\nadditional contextual references through multiple retrieval\\niterations. However, it may be affected by semantic discon-\\ntinuity and the accumulation of irrelevant information. ITER-\\nRETGEN [14] employs a synergistic approach that lever-\\nages “retrieval-enhanced generation” alongside “generation-\\nenhanced retrieval” for tasks that necessitate the reproduction\\nof specific information. The model harnesses the content\\nrequired to address the input task as a contextual basis for\\nretrieving pertinent knowledge, which in turn facilitates the\\ngeneration of improved responses in subsequent iterations.\\nB. Recursive Retrieval\\nRecursive retrieval is often used in information retrieval and\\nNLP to improve the depth and relevance of search results.\\nThe process involves iteratively refining search queries based\\non the results obtained from previous searches. Recursive\\nRetrieval aims to enhance the search experience by gradu-\\nally converging on the most pertinent information through a\\nfeedback loop. IRCoT [61] uses chain-of-thought to guide\\nthe retrieval process and refines the CoT with the obtained\\nretrieval results. ToC [57] creates a clarification tree that\\nsystematically optimizes the ambiguous parts in the Query. It\\ncan be particularly useful in complex search scenarios where\\nthe user’s needs are not entirely clear from the outset or where\\nthe information sought is highly specialized or nuanced. The\\nrecursive nature of the process allows for continuous learning\\nand adaptation to the user’s requirements, often resulting in\\nimproved satisfaction with the search outcomes.\\nTo address specific data scenarios, recursive retrieval and\\nmulti-hop retrieval techniques are utilized together. Recursive\\nretrieval involves a structured index to process and retrieve\\ndata in a hierarchical manner, which may include summarizing\\nsections of a document or lengthy PDF before performing a\\nretrieval based on this summary. Subsequently, a secondary\\nretrieval within the document refines the search, embodying\\nthe recursive nature of the process. In contrast, multi-hop\\nretrieval is designed to delve deeper into graph-structured data\\nsources, extracting interconnected information [106].\\nC. Adaptive Retrieval\\nAdaptive retrieval methods, exemplified by Flare [24] and\\nSelf-RAG [25], refine the RAG framework by enabling LLMs\\nto actively determine the optimal moments and content for\\nretrieval, thus enhancing the efficiency and relevance of the\\ninformation sourced.\\nThese methods are part of a broader trend wherein\\nLLMs employ active judgment in their operations, as seen\\nin model agents like AutoGPT, Toolformer, and Graph-\\nToolformer [107]–[109]. Graph-Toolformer, for instance, di-\\nvides its retrieval process into distinct steps where LLMs\\nproactively use retrievers, apply Self-Ask techniques, and em-\\nploy few-shot prompts to initiate search queries. This proactive\\nstance allows LLMs to decide when to search for necessary\\ninformation, akin to how an agent utilizes tools.\\nWebGPT [110] integrates a reinforcement learning frame-\\nwork to train the GPT-3 model in autonomously using a\\nsearch engine during text generation. It navigates this process\\nusing special tokens that facilitate actions such as search\\nengine queries, browsing results, and citing references, thereby\\nexpanding GPT-3’s capabilities through the use of external\\nsearch engines. Flare automates timing retrieval by monitoring\\nthe confidence of the generation process, as indicated by the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='12\\nprobability of generated terms [24]. When the probability falls\\nbelow a certain threshold would activates the retrieval system\\nto collect relevant information, thus optimizing the retrieval\\ncycle. Self-RAG [25] introduces “reflection tokens” that allow\\nthe model to introspect its outputs. These tokens come in\\ntwo varieties: “retrieve” and “critic”. The model autonomously\\ndecides when to activate retrieval, or alternatively, a predefined\\nthreshold may trigger the process. During retrieval, the gen-\\nerator conducts a fragment-level beam search across multiple\\nparagraphs to derive the most coherent sequence. Critic scores\\nare used to update the subdivision scores, with the flexibility\\nto adjust these weights during inference, tailoring the model’s\\nbehavior. Self-RAG’s design obviates the need for additional\\nclassifiers or reliance on Natural Language Inference (NLI)\\nmodels, thus streamlining the decision-making process for\\nwhen to engage retrieval mechanisms and improving the\\nmodel’s autonomous judgment capabilities in generating ac-\\ncurate responses.\\nVI. T ASK AND EVALUATION\\nThe rapid advancement and growing adoption of RAG\\nin the field of NLP have propelled the evaluation of RAG\\nmodels to the forefront of research in the LLMs community.\\nThe primary objective of this evaluation is to comprehend\\nand optimize the performance of RAG models across diverse\\napplication scenarios.This chapter will mainly introduce the\\nmain downstream tasks of RAG, datasets, and how to evaluate\\nRAG systems.\\nA. Downstream Task\\nThe core task of RAG remains Question Answering (QA),\\nincluding traditional single-hop/multi-hop QA, multiple-\\nchoice, domain-specific QA as well as long-form scenarios\\nsuitable for RAG. In addition to QA, RAG is continuously\\nbeing expanded into multiple downstream tasks, such as Infor-\\nmation Extraction (IE), dialogue generation, code search, etc.\\nThe main downstream tasks of RAG and their corresponding\\ndatasets are summarized in Table II.\\nB. Evaluation Target\\nHistorically, RAG models assessments have centered on\\ntheir execution in specific downstream tasks. These evaluations\\nemploy established metrics suitable to the tasks at hand. For\\ninstance, question answering evaluations might rely on EM\\nand F1 scores [7], [45], [59], [72], whereas fact-checking\\ntasks often hinge on Accuracy as the primary metric [4],\\n[14], [42]. BLEU and ROUGE metrics are also commonly\\nused to evaluate answer quality [26], [32], [52], [78]. Tools\\nlike RALLE, designed for the automatic evaluation of RAG\\napplications, similarly base their assessments on these task-\\nspecific metrics [160]. Despite this, there is a notable paucity\\nof research dedicated to evaluating the distinct characteristics\\nof RAG models.The main evaluation objectives include:\\nRetrieval Quality. Evaluating the retrieval quality is crucial\\nfor determining the effectiveness of the context sourced by\\nthe retriever component. Standard metrics from the domains\\nof search engines, recommendation systems, and information\\nretrieval systems are employed to measure the performance of\\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\\nNDCG are commonly utilized for this purpose [161], [162].\\nGeneration Quality . The assessment of generation quality\\ncenters on the generator’s capacity to synthesize coherent and\\nrelevant answers from the retrieved context. This evaluation\\ncan be categorized based on the content’s objectives: unlabeled\\nand labeled content. For unlabeled content, the evaluation\\nencompasses the faithfulness, relevance, and non-harmfulness\\nof the generated answers. In contrast, for labeled content,\\nthe focus is on the accuracy of the information produced by\\nthe model [161]. Additionally, both retrieval and generation\\nquality assessments can be conducted through manual or\\nautomatic evaluation methods [29], [161], [163].\\nC. Evaluation Aspects\\nContemporary evaluation practices of RAG models empha-\\nsize three primary quality scores and four essential abilities,\\nwhich collectively inform the evaluation of the two principal\\ntargets of the RAG model: retrieval and generation.\\n1) Quality Scores: Quality scores include context rele-\\nvance, answer faithfulness, and answer relevance. These qual-\\nity scores evaluate the efficiency of the RAG model from\\ndifferent perspectives in the process of information retrieval\\nand generation [164]–[166].\\nContext Relevance evaluates the precision and specificity\\nof the retrieved context, ensuring relevance and minimizing\\nprocessing costs associated with extraneous content.\\nAnswer Faithfulness ensures that the generated answers\\nremain true to the retrieved context, maintaining consistency\\nand avoiding contradictions.\\nAnswer Relevance requires that the generated answers are\\ndirectly pertinent to the posed questions, effectively addressing\\nthe core inquiry.\\n2) Required Abilities: RAG evaluation also encompasses\\nfour abilities indicative of its adaptability and efficiency:\\nnoise robustness, negative rejection, information integration,\\nand counterfactual robustness [167], [168]. These abilities are\\ncritical for the model’s performance under various challenges\\nand complex scenarios, impacting the quality scores.\\nNoise Robustness appraises the model’s capability to man-\\nage noise documents that are question-related but lack sub-\\nstantive information.\\nNegative Rejection assesses the model’s discernment in\\nrefraining from responding when the retrieved documents do\\nnot contain the necessary knowledge to answer a question.\\nInformation Integration evaluates the model’s proficiency in\\nsynthesizing information from multiple documents to address\\ncomplex questions.\\nCounterfactual Robustness tests the model’s ability to rec-\\nognize and disregard known inaccuracies within documents,\\neven when instructed about potential misinformation.\\nContext relevance and noise robustness are important for\\nevaluating the quality of retrieval, while answer faithfulness,\\nanswer relevance, negative rejection, information integration,\\nand counterfactual robustness are important for evaluating the\\nquality of generation.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='13\\nTABLE II\\nDOWNSTREAM TASKS AND DATASETS OF RAG\\nTask Sub Task Dataset Method\\nQA Single-hop Natural Qustion(NQ) [111]\\n[26], [30], [34], [42], [45], [50], [52], [59], [64], [82]\\n[3], [4], [22], [27], [40], [43], [54], [62], [71], [112]\\n[20], [44], [72]\\nTriviaQA(TQA) [113]\\n[13], [30], [34], [45], [50], [64]\\n[4], [27], [59], [62], [112]\\n[22], [25], [43], [44], [71], [72]\\nSQuAD [114] [20], [23], [30], [32], [45], [69], [112]\\nWeb Questions(WebQ) [115] [3], [4], [13], [30], [50], [68]\\nPopQA [116] [7], [25], [67]\\nMS MARCO [117] [4], [40], [52]\\nMulti-hop HotpotQA [118] [23], [26], [31], [34], [47], [51], [61], [82]\\n[7], [14], [22], [27], [59], [62], [69], [71], [91]\\n2WikiMultiHopQA [119] [14], [24], [48], [59], [61], [91]\\nMuSiQue [120] [14], [51], [61], [91]\\nLong-form QA ELI5 [121] [27], [34], [43], [49], [51]\\nNarrativeQA(NQA) [122] [45], [60], [63], [123]\\nASQA [124] [24], [57]\\nQMSum(QM) [125] [60], [123]\\nDomain QA Qasper [126] [60], [63]\\nCOVID-QA [127] [35], [46]\\nCMB [128],MMCU Medical [129] [81]\\nMulti-Choice QA QuALITY [130] [60], [63]\\nARC [131] [25], [67]\\nCommonsenseQA [132] [58], [66]\\nGraph QA GraphQA [84] [84]\\nDialog Dialog Generation Wizard of Wikipedia (WoW) [133] [13], [27], [34], [42]\\nPersonal Dialog KBP [134] [74], [135]\\nDuleMon [136] [74]\\nTask-oriented Dialog CamRest [137] [78], [79]\\nRecommendation Amazon(Toys,Sport,Beauty) [138] [39], [40]\\nIE Event Argument Extraction WikiEvent [139] [13], [27], [37], [42]\\nRAMS [140] [36], [37]\\nRelation Extraction T-REx [141],ZsRE [142] [27], [51]\\nReasoning Commonsense Reasoning HellaSwag [143] [20], [66]\\nCoT Reasoning CoT Reasoning [144] [27]\\nComplex Reasoning CSQA [145] [55]\\nOthers Language Understanding MMLU [146] [7], [27], [28], [42], [43], [47], [72]\\nLanguage Modeling WikiText-103 [147] [5], [29], [64], [71]\\nStrategyQA [148] [14], [24], [48], [51], [55], [58]\\nFact Checking/Verification FEVER [149] [4], [13], [27], [34], [42], [50]\\nPubHealth [150] [25], [67]\\nText Generation Biography [151] [67]\\nText Summarization WikiASP [152] [24]\\nXSum [153] [17]\\nText Classification VioLens [154] [19]\\nTREC [155] [33]\\nSentiment SST-2 [156] [20], [33], [38]\\nCode Search CodeSearchNet [157] [76]\\nRobustness Evaluation NoMIRACL [56] [56]\\nMath GSM8K [158] [73]\\nMachine Translation JRC-Acquis [159] [17]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='14\\nTABLE III\\nSUMMARY OF METRICS APPLICABLE FOR EVALUATION ASPECTS OF RAG\\nContext\\nRelevance Faithfulness Answer\\nRelevance\\nNoise\\nRobustness\\nNegative\\nRejection\\nInformation\\nIntegration\\nCounterfactual\\nRobustness\\nAccuracy ✓ ✓ ✓ ✓ ✓ ✓ ✓\\nEM ✓\\nRecall ✓\\nPrecision ✓ ✓\\nR-Rate ✓\\nCosine Similarity ✓\\nHit Rate ✓\\nMRR ✓\\nNDCG ✓\\nBLEU ✓ ✓ ✓\\nROUGE/ROUGE-L ✓ ✓ ✓\\nThe specific metrics for each evaluation aspect are sum-\\nmarized in Table III. It is essential to recognize that these\\nmetrics, derived from related work, are traditional measures\\nand do not yet represent a mature or standardized approach for\\nquantifying RAG evaluation aspects. Custom metrics tailored\\nto the nuances of RAG models, though not included here, have\\nalso been developed in some evaluation studies.\\nD. Evaluation Benchmarks and Tools\\nA series of benchmark tests and tools have been proposed\\nto facilitate the evaluation of RAG.These instruments furnish\\nquantitative metrics that not only gauge RAG model perfor-\\nmance but also enhance comprehension of the model’s capabil-\\nities across various evaluation aspects. Prominent benchmarks\\nsuch as RGB, RECALL and CRUD [167]–[169] focus on\\nappraising the essential abilities of RAG models. Concur-\\nrently, state-of-the-art automated tools like RAGAS [164],\\nARES [165], and TruLens 8 employ LLMs to adjudicate the\\nquality scores. These tools and benchmarks collectively form\\na robust framework for the systematic evaluation of RAG\\nmodels, as summarized in Table IV.\\nVII. D ISCUSSION AND FUTURE PROSPECTS\\nDespite the considerable progress in RAG technology, sev-\\neral challenges persist that warrant in-depth research.This\\nchapter will mainly introduce the current challenges and future\\nresearch directions faced by RAG.\\nA. RAG vs Long Context\\nWith the deepening of related research, the context of LLMs\\nis continuously expanding [170]–[172]. Presently, LLMs can\\neffortlessly manage contexts exceeding 200,000 tokens 9. This\\ncapability signifies that long-document question answering,\\npreviously reliant on RAG, can now incorporate the entire\\ndocument directly into the prompt. This has also sparked\\ndiscussions on whether RAG is still necessary when LLMs\\n8https://www.trulens.org/trulens eval/core concepts rag triad/\\n9https://kimi.moonshot.cn\\nare not constrained by context. In fact, RAG still plays an\\nirreplaceable role. On one hand, providing LLMs with a\\nlarge amount of context at once will significantly impact its\\ninference speed, while chunked retrieval and on-demand input\\ncan significantly improve operational efficiency. On the other\\nhand, RAG-based generation can quickly locate the original\\nreferences for LLMs to help users verify the generated an-\\nswers. The entire retrieval and reasoning process is observable,\\nwhile generation solely relying on long context remains a\\nblack box. Conversely, the expansion of context provides new\\nopportunities for the development of RAG, enabling it to\\naddress more complex problems and integrative or summary\\nquestions that require reading a large amount of material to\\nanswer [49]. Developing new RAG methods in the context of\\nsuper-long contexts is one of the future research trends.\\nB. RAG Robustness\\nThe presence of noise or contradictory information during\\nretrieval can detrimentally affect RAG’s output quality. This\\nsituation is figuratively referred to as “Misinformation can\\nbe worse than no information at all”. Improving RAG’s\\nresistance to such adversarial or counterfactual inputs is gain-\\ning research momentum and has become a key performance\\nmetric [48], [50], [82]. Cuconasu et al. [54] analyze which\\ntype of documents should be retrieved, evaluate the relevance\\nof the documents to the prompt, their position, and the\\nnumber included in the context. The research findings reveal\\nthat including irrelevant documents can unexpectedly increase\\naccuracy by over 30%, contradicting the initial assumption\\nof reduced quality. These results underscore the importance\\nof developing specialized strategies to integrate retrieval with\\nlanguage generation models, highlighting the need for further\\nresearch and exploration into the robustness of RAG.\\nC. Hybrid Approaches\\nCombining RAG with fine-tuning is emerging as a leading\\nstrategy. Determining the optimal integration of RAG and\\nfine-tuning whether sequential, alternating, or through end-to-\\nend joint training—and how to harness both parameterized'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='15\\nTABLE IV\\nSUMMARY OF EVALUATION FRAMEWORKS\\nEvaluation Framework Evaluation Targets Evaluation Aspects Quantitative Metrics\\nRGB† Retrieval Quality\\nGeneration Quality\\nNoise Robustness\\nNegative Rejection\\nInformation Integration\\nCounterfactual Robustness\\nAccuracy\\nEM\\nAccuracy\\nAccuracy\\nRECALL† Generation Quality Counterfactual Robustness R-Rate (Reappearance Rate)\\nRAGAS‡ Retrieval Quality\\nGeneration Quality\\nContext Relevance\\nFaithfulness\\nAnswer Relevance\\n*\\n*\\nCosine Similarity\\nARES‡ Retrieval Quality\\nGeneration Quality\\nContext Relevance\\nFaithfulness\\nAnswer Relevance\\nAccuracy\\nAccuracy\\nAccuracy\\nTruLens‡ Retrieval Quality\\nGeneration Quality\\nContext Relevance\\nFaithfulness\\nAnswer Relevance\\n*\\n*\\n*\\nCRUD† Retrieval Quality\\nGeneration Quality\\nCreative Generation\\nKnowledge-intensive QA\\nError Correction\\nSummarization\\nBLEU\\nROUGE-L\\nBertScore\\nRAGQuestEval\\n† represents a benchmark, and ‡ represents a tool. * denotes customized quantitative metrics, which deviate from traditional\\nmetrics. Readers are encouraged to consult pertinent literature for the specific quantification formulas associated with these\\nmetrics, as required.\\nand non-parameterized advantages are areas ripe for explo-\\nration [27]. Another trend is to introduce SLMs with specific\\nfunctionalities into RAG and fine-tuned by the results of RAG\\nsystem. For example, CRAG [67] trains a lightweight retrieval\\nevaluator to assess the overall quality of the retrieved docu-\\nments for a query and triggers different knowledge retrieval\\nactions based on confidence levels.\\nD. Scaling laws of RAG\\nEnd-to-end RAG models and pre-trained models based\\non RAG are still one of the focuses of current re-\\nsearchers [173].The parameters of these models are one of\\nthe key factors.While scaling laws [174] are established for\\nLLMs, their applicability to RAG remains uncertain. Initial\\nstudies like RETRO++ [44] have begun to address this, yet the\\nparameter count in RAG models still lags behind that of LLMs.\\nThe possibility of an Inverse Scaling Law 10, where smaller\\nmodels outperform larger ones, is particularly intriguing and\\nmerits further investigation.\\nE. Production-Ready RAG\\nRAG’s practicality and alignment with engineering require-\\nments have facilitated its adoption. However, enhancing re-\\ntrieval efficiency, improving document recall in large knowl-\\nedge bases, and ensuring data security—such as preventing\\n10https://github.com/inverse-scaling/prize\\ninadvertent disclosure of document sources or metadata by\\nLLMs—are critical engineering challenges that remain to be\\naddressed [175].\\nThe development of the RAG ecosystem is greatly impacted\\nby the progression of its technical stack. Key tools like\\nLangChain and LLamaIndex have quickly gained popularity\\nwith the emergence of ChatGPT, providing extensive RAG-\\nrelated APIs and becoming essential in the realm of LLMs.The\\nemerging technology stack, while not as rich in features as\\nLangChain and LLamaIndex, stands out through its specialized\\nproducts. For example, Flowise AI prioritizes a low-code\\napproach, allowing users to deploy AI applications, including\\nRAG, through a user-friendly drag-and-drop interface. Other\\ntechnologies like HayStack, Meltano, and Cohere Coral are\\nalso gaining attention for their unique contributions to the field.\\nIn addition to AI-focused vendors, traditional software and\\ncloud service providers are expanding their offerings to include\\nRAG-centric services. Weaviate’s Verba 11 is designed for\\npersonal assistant applications, while Amazon’s Kendra 12\\noffers intelligent enterprise search services, enabling users to\\nbrowse various content repositories using built-in connectors.\\nIn the development of RAG technology, there is a clear\\ntrend towards different specialization directions, such as: 1)\\nCustomization - tailoring RAG to meet specific requirements.\\n2) Simplification - making RAG easier to use to reduce the\\n11https://github.com/weaviate/Verba\\n12https://aws.amazon.com/cn/kendra/'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='16\\nFig. 6. Summary of RAG ecosystem\\ninitial learning curve. 3) Specialization - optimizing RAG to\\nbetter serve production environments.\\nThe mutual growth of RAG models and their technology\\nstacks is evident; technological advancements continuously\\nestablish new standards for existing infrastructure. In turn,\\nenhancements to the technology stack drive the development\\nof RAG capabilities. RAG toolkits are converging into a\\nfoundational technology stack, laying the groundwork for\\nadvanced enterprise applications. However, a fully integrated,\\ncomprehensive platform concept is still in the future, requiring\\nfurther innovation and development.\\nF . Multi-modal RAG\\nRAG has transcended its initial text-based question-\\nanswering confines, embracing a diverse array of modal data.\\nThis expansion has spawned innovative multimodal models\\nthat integrate RAG concepts across various domains:\\nImage. RA-CM3 [176] stands as a pioneering multimodal\\nmodel of both retrieving and generating text and images.\\nBLIP-2 [177] leverages frozen image encoders alongside\\nLLMs for efficient visual language pre-training, enabling zero-\\nshot image-to-text conversions. The “Visualize Before You\\nWrite” method [178] employs image generation to steer the\\nLM’s text generation, showing promise in open-ended text\\ngeneration tasks.\\nAudio and Video . The GSS method retrieves and stitches\\ntogether audio clips to convert machine-translated data into\\nspeech-translated data [179]. UEOP marks a significant ad-\\nvancement in end-to-end automatic speech recognition by\\nincorporating external, offline strategies for voice-to-text con-\\nversion [180]. Additionally, KNN-based attention fusion lever-\\nages audio embeddings and semantically related text embed-\\ndings to refine ASR, thereby accelerating domain adaptation.\\nVid2Seq augments language models with specialized temporal\\nmarkers, facilitating the prediction of event boundaries and\\ntextual descriptions within a unified output sequence [181].\\nCode. RBPS [182] excels in small-scale learning tasks by\\nretrieving code examples that align with developers’ objectives\\nthrough encoding and frequency analysis. This approach has\\ndemonstrated efficacy in tasks such as test assertion genera-\\ntion and program repair. For structured knowledge, the CoK\\nmethod [106] first extracts facts pertinent to the input query\\nfrom a knowledge graph, then integrates these facts as hints\\nwithin the input, enhancing performance in knowledge graph\\nquestion-answering tasks.\\nVIII. C ONCLUSION\\nThe summary of this paper, as depicted in Figure 6, empha-\\nsizes RAG’s significant advancement in enhancing the capa-\\nbilities of LLMs by integrating parameterized knowledge from\\nlanguage models with extensive non-parameterized data from\\nexternal knowledge bases. The survey showcases the evolution\\nof RAG technologies and their application on many different\\ntasks. The analysis outlines three developmental paradigms\\nwithin the RAG framework: Naive, Advanced, and Modu-\\nlar RAG, each representing a progressive enhancement over\\nits predecessors. RAG’s technical integration with other AI\\nmethodologies, such as fine-tuning and reinforcement learning,\\nhas further expanded its capabilities. Despite the progress in\\nRAG technology, there are research opportunities to improve\\nits robustness and its ability to handle extended contexts.\\nRAG’s application scope is expanding into multimodal do-\\nmains, adapting its principles to interpret and process diverse\\ndata forms like images, videos, and code. This expansion high-\\nlights RAG’s significant practical implications for AI deploy-\\nment, attracting interest from academic and industrial sectors.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='17\\nThe growing ecosystem of RAG is evidenced by the rise in\\nRAG-centric AI applications and the continuous development\\nof supportive tools. As RAG’s application landscape broadens,\\nthere is a need to refine evaluation methodologies to keep\\npace with its evolution. Ensuring accurate and representative\\nperformance assessments is crucial for fully capturing RAG’s\\ncontributions to the AI research and development community.\\nREFERENCES\\n[1] N. Kandpal, H. Deng, A. Roberts, E. Wallace, and C. Raffel, “Large\\nlanguage models struggle to learn long-tail knowledge,” in Interna-\\ntional Conference on Machine Learning . PMLR, 2023, pp. 15 696–\\n15 707.\\n[2] Y . Zhang, Y . Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\\nY . Zhang, Y . Chenet al., “Siren’s song in the ai ocean: A survey on hal-\\nlucination in large language models,” arXiv preprint arXiv:2309.01219,\\n2023.\\n[3] D. Arora, A. Kini, S. R. Chowdhury, N. Natarajan, G. Sinha, and\\nA. Sharma, “Gar-meets-rag paradigm for zero-shot information re-\\ntrieval,” arXiv preprint arXiv:2310.20158 , 2023.\\n[4] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,\\nH. K ¨uttler, M. Lewis, W.-t. Yih, T. Rockt ¨aschel et al. , “Retrieval-\\naugmented generation for knowledge-intensive nlp tasks,” Advances in\\nNeural Information Processing Systems, vol. 33, pp. 9459–9474, 2020.\\n[5] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli-\\ncan, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clarket al.,\\n“Improving language models by retrieving from trillions of tokens,”\\nin International conference on machine learning . PMLR, 2022, pp.\\n2206–2240.\\n[6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al. , “Training language\\nmodels to follow instructions with human feedback,” Advances in\\nneural information processing systems , vol. 35, pp. 27 730–27 744,\\n2022.\\n[7] X. Ma, Y . Gong, P. He, H. Zhao, and N. Duan, “Query rewrit-\\ning for retrieval-augmented large language models,” arXiv preprint\\narXiv:2305.14283, 2023.\\n[8] I. ILIN, “Advanced rag techniques: an il-\\nlustrated overview,” https://pub.towardsai.net/\\nadvanced-rag-techniques-an-illustrated-overview-04d193d8fec6,\\n2023.\\n[9] W. Peng, G. Li, Y . Jiang, Z. Wang, D. Ou, X. Zeng, E. Chen et al. ,\\n“Large language model based long-tail query rewriting in taobao\\nsearch,” arXiv preprint arXiv:2311.03758 , 2023.\\n[10] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V . Le,\\nand D. Zhou, “Take a step back: Evoking reasoning via abstraction in\\nlarge language models,” arXiv preprint arXiv:2310.06117 , 2023.\\n[11] L. Gao, X. Ma, J. Lin, and J. Callan, “Precise zero-shot dense retrieval\\nwithout relevance labels,” arXiv preprint arXiv:2212.10496 , 2022.\\n[12] V . Blagojevi, “Enhancing rag pipelines in haystack: Introducing diver-\\nsityranker and lostinthemiddleranker,” https://towardsdatascience.com/\\nenhancing-rag-pipelines-in-haystack-45f14e2bc9f5, 2023.\\n[13] W. Yu, D. Iter, S. Wang, Y . Xu, M. Ju, S. Sanyal, C. Zhu, M. Zeng,\\nand M. Jiang, “Generate rather than retrieve: Large language models\\nare strong context generators,” arXiv preprint arXiv:2209.10063, 2022.\\n[14] Z. Shao, Y . Gong, Y . Shen, M. Huang, N. Duan, and W. Chen,\\n“Enhancing retrieval-augmented large language models with iterative\\nretrieval-generation synergy,” arXiv preprint arXiv:2305.15294 , 2023.\\n[15] X. Wang, Q. Yang, Y . Qiu, J. Liang, Q. He, Z. Gu, Y . Xiao,\\nand W. Wang, “Knowledgpt: Enhancing large language models with\\nretrieval and storage access on knowledge bases,” arXiv preprint\\narXiv:2308.11761, 2023.\\n[16] A. H. Raudaschl, “Forget rag, the future\\nis rag-fusion,” https://towardsdatascience.com/\\nforget-rag-the-future-is-rag-fusion-1147298d8ad1, 2023.\\n[17] X. Cheng, D. Luo, X. Chen, L. Liu, D. Zhao, and R. Yan, “Lift\\nyourself up: Retrieval-augmented text generation with self memory,”\\narXiv preprint arXiv:2305.02437 , 2023.\\n[18] S. Wang, Y . Xu, Y . Fang, Y . Liu, S. Sun, R. Xu, C. Zhu, and\\nM. Zeng, “Training data is more valuable than you think: A simple\\nand effective method by retrieving from training data,” arXiv preprint\\narXiv:2203.08773, 2022.\\n[19] X. Li, E. Nie, and S. Liang, “From classification to generation:\\nInsights into crosslingual retrieval augmented icl,” arXiv preprint\\narXiv:2311.06595, 2023.\\n[20] D. Cheng, S. Huang, J. Bi, Y . Zhan, J. Liu, Y . Wang, H. Sun,\\nF. Wei, D. Deng, and Q. Zhang, “Uprise: Universal prompt retrieval\\nfor improving zero-shot evaluation,” arXiv preprint arXiv:2303.08518,\\n2023.\\n[21] Z. Dai, V . Y . Zhao, J. Ma, Y . Luan, J. Ni, J. Lu, A. Bakalov, K. Guu,\\nK. B. Hall, and M.-W. Chang, “Promptagator: Few-shot dense retrieval\\nfrom 8 examples,” arXiv preprint arXiv:2209.11755 , 2022.\\n[22] Z. Sun, X. Wang, Y . Tay, Y . Yang, and D. Zhou, “Recitation-augmented\\nlanguage models,” arXiv preprint arXiv:2210.01296 , 2022.\\n[23] O. Khattab, K. Santhanam, X. L. Li, D. Hall, P. Liang, C. Potts,\\nand M. Zaharia, “Demonstrate-search-predict: Composing retrieval\\nand language models for knowledge-intensive nlp,” arXiv preprint\\narXiv:2212.14024, 2022.\\n[24] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y . Yang,\\nJ. Callan, and G. Neubig, “Active retrieval augmented generation,”\\narXiv preprint arXiv:2305.06983 , 2023.\\n[25] A. Asai, Z. Wu, Y . Wang, A. Sil, and H. Hajishirzi, “Self-rag:\\nLearning to retrieve, generate, and critique through self-reflection,”\\narXiv preprint arXiv:2310.11511 , 2023.\\n[26] Z. Ke, W. Kong, C. Li, M. Zhang, Q. Mei, and M. Bendersky,\\n“Bridging the preference gap between retrievers and llms,” arXiv\\npreprint arXiv:2401.06954, 2024.\\n[27] X. V . Lin, X. Chen, M. Chen, W. Shi, M. Lomeli, R. James, P. Ro-\\ndriguez, J. Kahn, G. Szilvasy, M. Lewis et al. , “Ra-dit: Retrieval-\\naugmented dual instruction tuning,” arXiv preprint arXiv:2310.01352 ,\\n2023.\\n[28] O. Ovadia, M. Brief, M. Mishaeli, and O. Elisha, “Fine-tuning or\\nretrieval? comparing knowledge injection in llms,” arXiv preprint\\narXiv:2312.05934, 2023.\\n[29] T. Lan, D. Cai, Y . Wang, H. Huang, and X.-L. Mao, “Copy is all\\nyou need,” in The Eleventh International Conference on Learning\\nRepresentations, 2022.\\n[30] T. Chen, H. Wang, S. Chen, W. Yu, K. Ma, X. Zhao, D. Yu, and\\nH. Zhang, “Dense x retrieval: What retrieval granularity should we\\nuse?” arXiv preprint arXiv:2312.06648 , 2023.\\n[31] F. Luo and M. Surdeanu, “Divide & conquer for entailment-aware\\nmulti-hop evidence retrieval,” arXiv preprint arXiv:2311.02616 , 2023.\\n[32] Q. Gou, Z. Xia, B. Yu, H. Yu, F. Huang, Y . Li, and N. Cam-Tu,\\n“Diversify question generation with retrieval-augmented style transfer,”\\narXiv preprint arXiv:2310.14503 , 2023.\\n[33] Z. Guo, S. Cheng, Y . Wang, P. Li, and Y . Liu, “Prompt-guided re-\\ntrieval augmentation for non-knowledge-intensive tasks,”arXiv preprint\\narXiv:2305.17653, 2023.\\n[34] Z. Wang, J. Araki, Z. Jiang, M. R. Parvez, and G. Neubig, “Learning\\nto filter context for retrieval-augmented generation,” arXiv preprint\\narXiv:2311.08377, 2023.\\n[35] M. Seo, J. Baek, J. Thorne, and S. J. Hwang, “Retrieval-augmented\\ndata augmentation for low-resource domain tasks,” arXiv preprint\\narXiv:2402.13482, 2024.\\n[36] Y . Ma, Y . Cao, Y . Hong, and A. Sun, “Large language model is not\\na good few-shot information extractor, but a good reranker for hard\\nsamples!” arXiv preprint arXiv:2303.08559 , 2023.\\n[37] X. Du and H. Ji, “Retrieval-augmented generative question answering\\nfor event argument extraction,” arXiv preprint arXiv:2211.07067, 2022.\\n[38] L. Wang, N. Yang, and F. Wei, “Learning to retrieve in-context\\nexamples for large language models,”arXiv preprint arXiv:2307.07164,\\n2023.\\n[39] S. Rajput, N. Mehta, A. Singh, R. H. Keshavan, T. Vu, L. Heldt,\\nL. Hong, Y . Tay, V . Q. Tran, J. Samostet al., “Recommender systems\\nwith generative retrieval,” arXiv preprint arXiv:2305.05065 , 2023.\\n[40] B. Jin, H. Zeng, G. Wang, X. Chen, T. Wei, R. Li, Z. Wang, Z. Li,\\nY . Li, H. Lu et al. , “Language models as semantic indexers,” arXiv\\npreprint arXiv:2310.07815, 2023.\\n[41] R. Anantha, T. Bethi, D. V odianik, and S. Chappidi, “Context tuning\\nfor retrieval augmented generation,” arXiv preprint arXiv:2312.05708 ,\\n2023.\\n[42] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\\nJ. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, “Few-shot\\nlearning with retrieval augmented language models,” arXiv preprint\\narXiv:2208.03299, 2022.\\n[43] J. Huang, W. Ping, P. Xu, M. Shoeybi, K. C.-C. Chang, and B. Catan-\\nzaro, “Raven: In-context learning with retrieval augmented encoder-\\ndecoder language models,” arXiv preprint arXiv:2308.07922 , 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='18\\n[44] B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi, Y . Dong,\\nO. Kuchaiev, B. Li, C. Xiao et al. , “Shall we pretrain autoregressive\\nlanguage models with retrieval? a comprehensive study,”arXiv preprint\\narXiv:2304.06762, 2023.\\n[45] B. Wang, W. Ping, L. McAfee, P. Xu, B. Li, M. Shoeybi, and B. Catan-\\nzaro, “Instructretro: Instruction tuning post retrieval-augmented pre-\\ntraining,” arXiv preprint arXiv:2310.07713 , 2023.\\n[46] S. Siriwardhana, R. Weerasekera, E. Wen, T. Kaluarachchi, R. Rana,\\nand S. Nanayakkara, “Improving the domain adaptation of retrieval\\naugmented generation (rag) models for open domain question answer-\\ning,” Transactions of the Association for Computational Linguistics ,\\nvol. 11, pp. 1–17, 2023.\\n[47] Z. Yu, C. Xiong, S. Yu, and Z. Liu, “Augmentation-adapted retriever\\nimproves generalization of language models as generic plug-in,” arXiv\\npreprint arXiv:2305.17331, 2023.\\n[48] O. Yoran, T. Wolfson, O. Ram, and J. Berant, “Making retrieval-\\naugmented language models robust to irrelevant context,” arXiv\\npreprint arXiv:2310.01558, 2023.\\n[49] H.-T. Chen, F. Xu, S. A. Arora, and E. Choi, “Understanding re-\\ntrieval augmentation for long-form question answering,” arXiv preprint\\narXiv:2310.12150, 2023.\\n[50] W. Yu, H. Zhang, X. Pan, K. Ma, H. Wang, and D. Yu, “Chain-of-note:\\nEnhancing robustness in retrieval-augmented language models,” arXiv\\npreprint arXiv:2311.09210, 2023.\\n[51] S. Xu, L. Pang, H. Shen, X. Cheng, and T.-S. Chua, “Search-in-the-\\nchain: Towards accurate, credible and traceable large language models\\nfor knowledgeintensive tasks,” CoRR, vol. abs/2304.14732 , 2023.\\n[52] M. Berchansky, P. Izsak, A. Caciularu, I. Dagan, and M. Wasserblat,\\n“Optimizing retrieval-augmented reader models via token elimination,”\\narXiv preprint arXiv:2310.13682 , 2023.\\n[53] J. L ´ala, O. O’Donoghue, A. Shtedritski, S. Cox, S. G. Rodriques,\\nand A. D. White, “Paperqa: Retrieval-augmented generative agent for\\nscientific research,” arXiv preprint arXiv:2312.07559 , 2023.\\n[54] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano,\\nY . Maarek, N. Tonellotto, and F. Silvestri, “The power of noise:\\nRedefining retrieval for rag systems,” arXiv preprint arXiv:2401.14887,\\n2024.\\n[55] Z. Zhang, X. Zhang, Y . Ren, S. Shi, M. Han, Y . Wu, R. Lai, and\\nZ. Cao, “Iag: Induction-augmented generation framework for answer-\\ning reasoning questions,” in Proceedings of the 2023 Conference on\\nEmpirical Methods in Natural Language Processing , 2023, pp. 1–14.\\n[56] N. Thakur, L. Bonifacio, X. Zhang, O. Ogundepo, E. Kamalloo,\\nD. Alfonso-Hermelo, X. Li, Q. Liu, B. Chen, M. Rezagholizadeh et al.,\\n“Nomiracl: Knowing when you don’t know for robust multilingual\\nretrieval-augmented generation,” arXiv preprint arXiv:2312.11361 ,\\n2023.\\n[57] G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, “Tree of clarifica-\\ntions: Answering ambiguous questions with retrieval-augmented large\\nlanguage models,” arXiv preprint arXiv:2310.14696 , 2023.\\n[58] Y . Wang, P. Li, M. Sun, and Y . Liu, “Self-knowledge guided\\nretrieval augmentation for large language models,” arXiv preprint\\narXiv:2310.05002, 2023.\\n[59] Z. Feng, X. Feng, D. Zhao, M. Yang, and B. Qin, “Retrieval-\\ngeneration synergy augmented large language models,” arXiv preprint\\narXiv:2310.05149, 2023.\\n[60] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian,\\nE. Bakhturina, M. Shoeybi, and B. Catanzaro, “Retrieval meets long\\ncontext large language models,” arXiv preprint arXiv:2310.03025 ,\\n2023.\\n[61] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Interleav-\\ning retrieval with chain-of-thought reasoning for knowledge-intensive\\nmulti-step questions,” arXiv preprint arXiv:2212.10509 , 2022.\\n[62] R. Ren, Y . Wang, Y . Qu, W. X. Zhao, J. Liu, H. Tian, H. Wu, J.-\\nR. Wen, and H. Wang, “Investigating the factual knowledge boundary\\nof large language models with retrieval augmentation,” arXiv preprint\\narXiv:2307.11019, 2023.\\n[63] P. Sarthi, S. Abdullah, A. Tuli, S. Khanna, A. Goldie, and C. D.\\nManning, “Raptor: Recursive abstractive processing for tree-organized\\nretrieval,” arXiv preprint arXiv:2401.18059 , 2024.\\n[64] O. Ram, Y . Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-\\nBrown, and Y . Shoham, “In-context retrieval-augmented language\\nmodels,” arXiv preprint arXiv:2302.00083 , 2023.\\n[65] Y . Ren, Y . Cao, P. Guo, F. Fang, W. Ma, and Z. Lin, “Retrieve-and-\\nsample: Document-level event argument extraction via hybrid retrieval\\naugmentation,” in Proceedings of the 61st Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers) ,\\n2023, pp. 293–306.\\n[66] Z. Wang, X. Pan, D. Yu, D. Yu, J. Chen, and H. Ji, “Zemi: Learning\\nzero-shot semi-parametric language models from multiple tasks,” arXiv\\npreprint arXiv:2210.00185, 2022.\\n[67] S.-Q. Yan, J.-C. Gu, Y . Zhu, and Z.-H. Ling, “Corrective retrieval\\naugmented generation,” arXiv preprint arXiv:2401.15884 , 2024.\\n[68] P. Jain, L. B. Soares, and T. Kwiatkowski, “1-pager: One pass answer\\ngeneration and evidence retrieval,” arXiv preprint arXiv:2310.16568 ,\\n2023.\\n[69] H. Yang, Z. Li, Y . Zhang, J. Wang, N. Cheng, M. Li, and J. Xiao, “Prca:\\nFitting black-box large language models for retrieval question answer-\\ning via pluggable reward-driven contextual adapter,” arXiv preprint\\narXiv:2310.18347, 2023.\\n[70] S. Zhuang, B. Liu, B. Koopman, and G. Zuccon, “Open-source large\\nlanguage models are strong zero-shot query likelihood models for\\ndocument ranking,” arXiv preprint arXiv:2310.13243 , 2023.\\n[71] F. Xu, W. Shi, and E. Choi, “Recomp: Improving retrieval-augmented\\nlms with compression and selective augmentation,” arXiv preprint\\narXiv:2310.04408, 2023.\\n[72] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-\\nmoyer, and W.-t. Yih, “Replug: Retrieval-augmented black-box lan-\\nguage models,” arXiv preprint arXiv:2301.12652 , 2023.\\n[73] E. Melz, “Enhancing llm intelligence with arm-rag: Auxiliary ra-\\ntionale memory for retrieval augmented generation,” arXiv preprint\\narXiv:2311.04177, 2023.\\n[74] H. Wang, W. Huang, Y . Deng, R. Wang, Z. Wang, Y . Wang, F. Mi,\\nJ. Z. Pan, and K.-F. Wong, “Unims-rag: A unified multi-source\\nretrieval-augmented generation for personalized dialogue systems,”\\narXiv preprint arXiv:2401.13256 , 2024.\\n[75] Z. Luo, C. Xu, P. Zhao, X. Geng, C. Tao, J. Ma, Q. Lin, and D. Jiang,\\n“Augmented large language models with parametric knowledge guid-\\ning,” arXiv preprint arXiv:2305.04757 , 2023.\\n[76] X. Li, Z. Liu, C. Xiong, S. Yu, Y . Gu, Z. Liu, and G. Yu, “Structure-\\naware language model pretraining improves dense retrieval on struc-\\ntured data,” arXiv preprint arXiv:2305.19912 , 2023.\\n[77] M. Kang, J. M. Kwak, J. Baek, and S. J. Hwang, “Knowledge\\ngraph-augmented language models for knowledge-grounded dialogue\\ngeneration,” arXiv preprint arXiv:2305.18846 , 2023.\\n[78] W. Shen, Y . Gao, C. Huang, F. Wan, X. Quan, and W. Bi, “Retrieval-\\ngeneration alignment for end-to-end task-oriented dialogue system,”\\narXiv preprint arXiv:2310.08877 , 2023.\\n[79] T. Shi, L. Li, Z. Lin, T. Yang, X. Quan, and Q. Wang, “Dual-feedback\\nknowledge retrieval for task-oriented dialogue systems,” arXiv preprint\\narXiv:2310.14528, 2023.\\n[80] P. Ranade and A. Joshi, “Fabula: Intelligence report generation\\nusing retrieval-augmented narrative construction,” arXiv preprint\\narXiv:2310.13848, 2023.\\n[81] X. Jiang, R. Zhang, Y . Xu, R. Qiu, Y . Fang, Z. Wang, J. Tang,\\nH. Ding, X. Chu, J. Zhao et al. , “Think and retrieval: A hypothesis\\nknowledge graph enhanced medical large language models,” arXiv\\npreprint arXiv:2312.15883, 2023.\\n[82] J. Baek, S. Jeong, M. Kang, J. C. Park, and S. J. Hwang,\\n“Knowledge-augmented language model verification,” arXiv preprint\\narXiv:2310.12836, 2023.\\n[83] L. Luo, Y .-F. Li, G. Haffari, and S. Pan, “Reasoning on graphs: Faithful\\nand interpretable large language model reasoning,” arXiv preprint\\narXiv:2310.01061, 2023.\\n[84] X. He, Y . Tian, Y . Sun, N. V . Chawla, T. Laurent, Y . LeCun,\\nX. Bresson, and B. Hooi, “G-retriever: Retrieval-augmented generation\\nfor textual graph understanding and question answering,”arXiv preprint\\narXiv:2402.07630, 2024.\\n[85] L. Zha, J. Zhou, L. Li, R. Wang, Q. Huang, S. Yang, J. Yuan, C. Su,\\nX. Li, A. Su et al., “Tablegpt: Towards unifying tables, nature language\\nand commands into one gpt,” arXiv preprint arXiv:2307.08674 , 2023.\\n[86] M. Gaur, K. Gunaratna, V . Srinivasan, and H. Jin, “Iseeq: Information\\nseeking question generation using dynamic meta-information retrieval\\nand knowledge graphs,” in Proceedings of the AAAI Conference on\\nArtificial Intelligence, vol. 36, no. 10, 2022, pp. 10 672–10 680.\\n[87] F. Shi, X. Chen, K. Misra, N. Scales, D. Dohan, E. H. Chi, N. Sch ¨arli,\\nand D. Zhou, “Large language models can be easily distracted by\\nirrelevant context,” in International Conference on Machine Learning .\\nPMLR, 2023, pp. 31 210–31 227.\\n[88] R. Teja, “Evaluating the ideal chunk size for a rag\\nsystem using llamaindex,” https://www.llamaindex.ai/blog/\\nevaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5,\\n2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='19\\n[89] Langchain, “Recursively split by character,” https://python.langchain.\\ncom/docs/modules/data connection/document transformers/recursive\\ntext splitter, 2023.\\n[90] S. Yang, “Advanced rag 01: Small-to-\\nbig retrieval,” https://towardsdatascience.com/\\nadvanced-rag-01-small-to-big-retrieval-172181b396d4, 2023.\\n[91] Y . Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and T. Derr,\\n“Knowledge graph prompting for multi-document question answering,”\\narXiv preprint arXiv:2308.11730 , 2023.\\n[92] D. Zhou, N. Sch ¨arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schu-\\nurmans, C. Cui, O. Bousquet, Q. Le et al., “Least-to-most prompting\\nenables complex reasoning in large language models,” arXiv preprint\\narXiv:2205.10625, 2022.\\n[93] S. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz,\\nand J. Weston, “Chain-of-verification reduces hallucination in large\\nlanguage models,” arXiv preprint arXiv:2309.11495 , 2023.\\n[94] X. Li and J. Li, “Angle-optimized text embeddings,” arXiv preprint\\narXiv:2309.12871, 2023.\\n[95] V oyageAI, “V oyage’s embedding models,” https://docs.voyageai.com/\\nembeddings/, 2023.\\n[96] BAAI, “Flagembedding,” https://github.com/FlagOpen/\\nFlagEmbedding, 2023.\\n[97] P. Zhang, S. Xiao, Z. Liu, Z. Dou, and J.-Y . Nie, “Retrieve anything\\nto augment large language models,” arXiv preprint arXiv:2310.07554 ,\\n2023.\\n[98] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni,\\nand P. Liang, “Lost in the middle: How language models use long\\ncontexts,” arXiv preprint arXiv:2307.03172 , 2023.\\n[99] Y . Gao, T. Sheng, Y . Xiang, Y . Xiong, H. Wang, and J. Zhang, “Chat-\\nrec: Towards interactive and explainable llms-augmented recommender\\nsystem,” arXiv preprint arXiv:2303.14524 , 2023.\\n[100] N. Anderson, C. Wilson, and S. D. Richardson, “Lingua: Addressing\\nscenarios for live interpretation and automatic dubbing,” inProceedings\\nof the 15th Biennial Conference of the Association for Machine\\nTranslation in the Americas (Volume 2: Users and Providers Track\\nand Government Track) , J. Campbell, S. Larocca, J. Marciano,\\nK. Savenkov, and A. Yanishevsky, Eds. Orlando, USA: Association\\nfor Machine Translation in the Americas, Sep. 2022, pp. 202–209.\\n[Online]. Available: https://aclanthology.org/2022.amta-upg.14\\n[101] H. Jiang, Q. Wu, X. Luo, D. Li, C.-Y . Lin, Y . Yang, and L. Qiu,\\n“Longllmlingua: Accelerating and enhancing llms in long context\\nscenarios via prompt compression,” arXiv preprint arXiv:2310.06839 ,\\n2023.\\n[102] V . Karpukhin, B. O ˘guz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen,\\nand W.-t. Yih, “Dense passage retrieval for open-domain question\\nanswering,” arXiv preprint arXiv:2004.04906 , 2020.\\n[103] Y . Ma, Y . Cao, Y . Hong, and A. Sun, “Large language model is\\nnot a good few-shot information extractor, but a good reranker for\\nhard samples!” ArXiv, vol. abs/2303.08559, 2023. [Online]. Available:\\nhttps://api.semanticscholar.org/CorpusID:257532405\\n[104] J. Cui, Z. Li, Y . Yan, B. Chen, and L. Yuan, “Chatlaw: Open-source\\nlegal large language model with integrated external knowledge bases,”\\narXiv preprint arXiv:2306.16092 , 2023.\\n[105] O. Yoran, T. Wolfson, O. Ram, and J. Berant, “Making retrieval-\\naugmented language models robust to irrelevant context,” arXiv\\npreprint arXiv:2310.01558, 2023.\\n[106] X. Li, R. Zhao, Y . K. Chia, B. Ding, L. Bing, S. Joty, and S. Poria,\\n“Chain of knowledge: A framework for grounding large language mod-\\nels with structured knowledge bases,”arXiv preprint arXiv:2305.13269,\\n2023.\\n[107] H. Yang, S. Yue, and Y . He, “Auto-gpt for online decision\\nmaking: Benchmarks and additional opinions,” arXiv preprint\\narXiv:2306.02224, 2023.\\n[108] T. Schick, J. Dwivedi-Yu, R. Dess `ı, R. Raileanu, M. Lomeli, L. Zettle-\\nmoyer, N. Cancedda, and T. Scialom, “Toolformer: Language models\\ncan teach themselves to use tools,” arXiv preprint arXiv:2302.04761 ,\\n2023.\\n[109] J. Zhang, “Graph-toolformer: To empower llms with graph rea-\\nsoning ability via prompt augmented by chatgpt,” arXiv preprint\\narXiv:2304.11116, 2023.\\n[110] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\\nC. Hesse, S. Jain, V . Kosaraju, W. Saunders et al., “Webgpt: Browser-\\nassisted question-answering with human feedback,” arXiv preprint\\narXiv:2112.09332, 2021.\\n[111] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh,\\nC. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee et al., “Natural\\nquestions: a benchmark for question answering research,” Transactions\\nof the Association for Computational Linguistics , vol. 7, pp. 453–466,\\n2019.\\n[112] Y . Liu, S. Yavuz, R. Meng, M. Moorthy, S. Joty, C. Xiong, and Y . Zhou,\\n“Exploring the integration strategies of retriever and large language\\nmodels,” arXiv preprint arXiv:2308.12574 , 2023.\\n[113] M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer, “Triviaqa: A large\\nscale distantly supervised challenge dataset for reading comprehen-\\nsion,” arXiv preprint arXiv:1705.03551 , 2017.\\n[114] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad: 100,000+\\nquestions for machine comprehension of text,” arXiv preprint\\narXiv:1606.05250, 2016.\\n[115] J. Berant, A. Chou, R. Frostig, and P. Liang, “Semantic parsing on\\nfreebase from question-answer pairs,” in Proceedings of the 2013\\nconference on empirical methods in natural language processing, 2013,\\npp. 1533–1544.\\n[116] A. Mallen, A. Asai, V . Zhong, R. Das, H. Hajishirzi, and D. Khashabi,\\n“When not to trust language models: Investigating effectiveness and\\nlimitations of parametric and non-parametric memories,” arXiv preprint\\narXiv:2212.10511, 2022.\\n[117] T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder,\\nand L. Deng, “Ms marco: A human-generated machine reading com-\\nprehension dataset,” 2016.\\n[118] Z. Yang, P. Qi, S. Zhang, Y . Bengio, W. W. Cohen, R. Salakhutdi-\\nnov, and C. D. Manning, “Hotpotqa: A dataset for diverse, explain-\\nable multi-hop question answering,” arXiv preprint arXiv:1809.09600,\\n2018.\\n[119] X. Ho, A.-K. D. Nguyen, S. Sugawara, and A. Aizawa, “Constructing a\\nmulti-hop qa dataset for comprehensive evaluation of reasoning steps,”\\narXiv preprint arXiv:2011.01060 , 2020.\\n[120] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Musique:\\nMultihop questions via single-hop question composition,” Transactions\\nof the Association for Computational Linguistics , vol. 10, pp. 539–554,\\n2022.\\n[121] A. Fan, Y . Jernite, E. Perez, D. Grangier, J. Weston, and M. Auli, “Eli5:\\nLong form question answering,” arXiv preprint arXiv:1907.09190 ,\\n2019.\\n[122] T. Ko ˇcisk`y, J. Schwarz, P. Blunsom, C. Dyer, K. M. Hermann, G. Melis,\\nand E. Grefenstette, “The narrativeqa reading comprehension chal-\\nlenge,” Transactions of the Association for Computational Linguistics ,\\nvol. 6, pp. 317–328, 2018.\\n[123] K.-H. Lee, X. Chen, H. Furuta, J. Canny, and I. Fischer, “A human-\\ninspired reading agent with gist memory of very long contexts,” arXiv\\npreprint arXiv:2402.09727, 2024.\\n[124] I. Stelmakh, Y . Luan, B. Dhingra, and M.-W. Chang, “Asqa: Factoid\\nquestions meet long-form answers,” arXiv preprint arXiv:2204.06092 ,\\n2022.\\n[125] M. Zhong, D. Yin, T. Yu, A. Zaidi, M. Mutuma, R. Jha, A. H.\\nAwadallah, A. Celikyilmaz, Y . Liu, X. Qiu et al. , “Qmsum: A new\\nbenchmark for query-based multi-domain meeting summarization,”\\narXiv preprint arXiv:2104.05938 , 2021.\\n[126] P. Dasigi, K. Lo, I. Beltagy, A. Cohan, N. A. Smith, and M. Gardner,\\n“A dataset of information-seeking questions and answers anchored in\\nresearch papers,” arXiv preprint arXiv:2105.03011 , 2021.\\n[127] T. M ¨oller, A. Reina, R. Jayakumar, and M. Pietsch, “Covid-qa: A\\nquestion answering dataset for covid-19,” in ACL 2020 Workshop on\\nNatural Language Processing for COVID-19 (NLP-COVID) , 2020.\\n[128] X. Wang, G. H. Chen, D. Song, Z. Zhang, Z. Chen, Q. Xiao, F. Jiang,\\nJ. Li, X. Wan, B. Wang et al. , “Cmb: A comprehensive medical\\nbenchmark in chinese,” arXiv preprint arXiv:2308.08833 , 2023.\\n[129] H. Zeng, “Measuring massive multitask chinese understanding,” arXiv\\npreprint arXiv:2304.12986, 2023.\\n[130] R. Y . Pang, A. Parrish, N. Joshi, N. Nangia, J. Phang, A. Chen, V . Pad-\\nmakumar, J. Ma, J. Thompson, H. He et al. , “Quality: Question an-\\nswering with long input texts, yes!” arXiv preprint arXiv:2112.08608 ,\\n2021.\\n[131] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\\nand O. Tafjord, “Think you have solved question answering? try arc,\\nthe ai2 reasoning challenge,” arXiv preprint arXiv:1803.05457 , 2018.\\n[132] A. Talmor, J. Herzig, N. Lourie, and J. Berant, “Commonsenseqa:\\nA question answering challenge targeting commonsense knowledge,”\\narXiv preprint arXiv:1811.00937 , 2018.\\n[133] E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, and J. Weston,\\n“Wizard of wikipedia: Knowledge-powered conversational agents,”\\narXiv preprint arXiv:1811.01241 , 2018.\\n[134] H. Wang, M. Hu, Y . Deng, R. Wang, F. Mi, W. Wang, Y . Wang, W.-\\nC. Kwan, I. King, and K.-F. Wong, “Large language models as source'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='20\\nplanner for personalized knowledge-grounded dialogue,” arXiv preprint\\narXiv:2310.08840, 2023.\\n[135] ——, “Large language models as source planner for personal-\\nized knowledge-grounded dialogue,” arXiv preprint arXiv:2310.08840,\\n2023.\\n[136] X. Xu, Z. Gou, W. Wu, Z.-Y . Niu, H. Wu, H. Wang, and S. Wang,\\n“Long time no see! open-domain conversation with long-term persona\\nmemory,” arXiv preprint arXiv:2203.05797 , 2022.\\n[137] T.-H. Wen, M. Gasic, N. Mrksic, L. M. Rojas-Barahona, P.-H.\\nSu, S. Ultes, D. Vandyke, and S. Young, “Conditional generation\\nand snapshot learning in neural dialogue systems,” arXiv preprint\\narXiv:1606.03352, 2016.\\n[138] R. He and J. McAuley, “Ups and downs: Modeling the visual evolution\\nof fashion trends with one-class collaborative filtering,” in proceedings\\nof the 25th international conference on world wide web , 2016, pp.\\n507–517.\\n[139] S. Li, H. Ji, and J. Han, “Document-level event argument extraction\\nby conditional generation,” arXiv preprint arXiv:2104.05919 , 2021.\\n[140] S. Ebner, P. Xia, R. Culkin, K. Rawlins, and B. Van Durme, “Multi-\\nsentence argument linking,” arXiv preprint arXiv:1911.03766 , 2019.\\n[141] H. Elsahar, P. V ougiouklis, A. Remaci, C. Gravier, J. Hare, F. Laforest,\\nand E. Simperl, “T-rex: A large scale alignment of natural language\\nwith knowledge base triples,” in Proceedings of the Eleventh Inter-\\nnational Conference on Language Resources and Evaluation (LREC\\n2018), 2018.\\n[142] O. Levy, M. Seo, E. Choi, and L. Zettlemoyer, “Zero-shot relation ex-\\ntraction via reading comprehension,” arXiv preprint arXiv:1706.04115,\\n2017.\\n[143] R. Zellers, A. Holtzman, Y . Bisk, A. Farhadi, and Y . Choi, “Hel-\\nlaswag: Can a machine really finish your sentence?” arXiv preprint\\narXiv:1905.07830, 2019.\\n[144] S. Kim, S. J. Joo, D. Kim, J. Jang, S. Ye, J. Shin, and M. Seo,\\n“The cot collection: Improving zero-shot and few-shot learning of\\nlanguage models via chain-of-thought fine-tuning,” arXiv preprint\\narXiv:2305.14045, 2023.\\n[145] A. Saha, V . Pahuja, M. Khapra, K. Sankaranarayanan, and S. Chandar,\\n“Complex sequential question answering: Towards learning to converse\\nover linked question answer pairs with a knowledge graph,” inProceed-\\nings of the AAAI conference on artificial intelligence , vol. 32, no. 1,\\n2018.\\n[146] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\\nJ. Steinhardt, “Measuring massive multitask language understanding,”\\narXiv preprint arXiv:2009.03300 , 2020.\\n[147] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer sentinel\\nmixture models,” arXiv preprint arXiv:1609.07843 , 2016.\\n[148] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant,\\n“Did aristotle use a laptop? a question answering benchmark with\\nimplicit reasoning strategies,” Transactions of the Association for\\nComputational Linguistics, vol. 9, pp. 346–361, 2021.\\n[149] J. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal, “Fever: a\\nlarge-scale dataset for fact extraction and verification,” arXiv preprint\\narXiv:1803.05355, 2018.\\n[150] N. Kotonya and F. Toni, “Explainable automated fact-checking for\\npublic health claims,” arXiv preprint arXiv:2010.09926 , 2020.\\n[151] R. Lebret, D. Grangier, and M. Auli, “Neural text generation from\\nstructured data with application to the biography domain,” arXiv\\npreprint arXiv:1603.07771, 2016.\\n[152] H. Hayashi, P. Budania, P. Wang, C. Ackerson, R. Neervannan,\\nand G. Neubig, “Wikiasp: A dataset for multi-domain aspect-based\\nsummarization,” Transactions of the Association for Computational\\nLinguistics, vol. 9, pp. 211–225, 2021.\\n[153] S. Narayan, S. B. Cohen, and M. Lapata, “Don’t give me the details,\\njust the summary! topic-aware convolutional neural networks for ex-\\ntreme summarization,” arXiv preprint arXiv:1808.08745 , 2018.\\n[154] S. Saha, J. A. Junaed, M. Saleki, A. S. Sharma, M. R. Rifat, M. Rahouti,\\nS. I. Ahmed, N. Mohammed, and M. R. Amin, “Vio-lens: A novel\\ndataset of annotated social network posts leading to different forms\\nof communal violence and its evaluation,” in Proceedings of the First\\nWorkshop on Bangla Language Processing (BLP-2023), 2023, pp. 72–\\n84.\\n[155] X. Li and D. Roth, “Learning question classifiers,” in COLING 2002:\\nThe 19th International Conference on Computational Linguistics, 2002.\\n[156] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y . Ng,\\nand C. Potts, “Recursive deep models for semantic compositionality\\nover a sentiment treebank,” in Proceedings of the 2013 conference on\\nempirical methods in natural language processing , 2013, pp. 1631–\\n1642.\\n[157] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,\\n“Codesearchnet challenge: Evaluating the state of semantic code\\nsearch,” arXiv preprint arXiv:1909.09436 , 2019.\\n[158] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\\nM. Plappert, J. Tworek, J. Hilton, R. Nakano et al., “Training verifiers\\nto solve math word problems,” arXiv preprint arXiv:2110.14168, 2021.\\n[159] R. Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Erjavec, D. Tufis,\\nand D. Varga, “The jrc-acquis: A multilingual aligned parallel corpus\\nwith 20+ languages,” arXiv preprint cs/0609058 , 2006.\\n[160] Y . Hoshi, D. Miyashita, Y . Ng, K. Tatsuno, Y . Morioka, O. Torii,\\nand J. Deguchi, “Ralle: A framework for developing and eval-\\nuating retrieval-augmented large language models,” arXiv preprint\\narXiv:2308.10633, 2023.\\n[161] J. Liu, “Building production-ready rag applications,” https://www.ai.\\nengineer/summit/schedule/building-production-ready-rag-applications,\\n2023.\\n[162] I. Nguyen, “Evaluating rag part i: How to evaluate document retrieval,”\\nhttps://www.deepset.ai/blog/rag-evaluation-retrieval, 2023.\\n[163] Q. Leng, K. Uhlenhuth, and A. Polyzotis, “Best practices for\\nllm evaluation of rag applications,” https://www.databricks.com/blog/\\nLLM-auto-eval-best-practices-RAG, 2023.\\n[164] S. Es, J. James, L. Espinosa-Anke, and S. Schockaert, “Ragas: Au-\\ntomated evaluation of retrieval augmented generation,” arXiv preprint\\narXiv:2309.15217, 2023.\\n[165] J. Saad-Falcon, O. Khattab, C. Potts, and M. Zaharia, “Ares: An\\nautomated evaluation framework for retrieval-augmented generation\\nsystems,” arXiv preprint arXiv:2311.09476 , 2023.\\n[166] C. Jarvis and J. Allard, “A survey of techniques for\\nmaximizing llm performance,” https://community.openai.\\ncom/t/openai-dev-day-2023-breakout-sessions/505213#\\na-survey-of-techniques-for-maximizing-llm-performance-2, 2023.\\n[167] J. Chen, H. Lin, X. Han, and L. Sun, “Benchmarking large lan-\\nguage models in retrieval-augmented generation,” arXiv preprint\\narXiv:2309.01431, 2023.\\n[168] Y . Liu, L. Huang, S. Li, S. Chen, H. Zhou, F. Meng, J. Zhou, and\\nX. Sun, “Recall: A benchmark for llms robustness against external\\ncounterfactual knowledge,” arXiv preprint arXiv:2311.08147 , 2023.\\n[169] Y . Lyu, Z. Li, S. Niu, F. Xiong, B. Tang, W. Wang, H. Wu, H. Liu,\\nT. Xu, and E. Chen, “Crud-rag: A comprehensive chinese benchmark\\nfor retrieval-augmented generation of large language models,” arXiv\\npreprint arXiv:2401.17043, 2024.\\n[170] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian,\\nE. Bakhturina, M. Shoeybi, and B. Catanzaro, “Retrieval meets long\\ncontext large language models,” arXiv preprint arXiv:2310.03025 ,\\n2023.\\n[171] C. Packer, V . Fang, S. G. Patil, K. Lin, S. Wooders, and J. E. Gon-\\nzalez, “Memgpt: Towards llms as operating systems,” arXiv preprint\\narXiv:2310.08560, 2023.\\n[172] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis, “Efficient\\nstreaming language models with attention sinks,” arXiv preprint\\narXiv:2309.17453, 2023.\\n[173] T. Zhang, S. G. Patil, N. Jain, S. Shen, M. Zaharia, I. Stoica, and J. E.\\nGonzalez, “Raft: Adapting language model to domain specific rag,”\\narXiv preprint arXiv:2403.10131 , 2024.\\n[174] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,\\nR. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws\\nfor neural language models,” arXiv preprint arXiv:2001.08361 , 2020.\\n[175] U. Alon, F. Xu, J. He, S. Sengupta, D. Roth, and G. Neubig, “Neuro-\\nsymbolic language modeling with automaton-augmented retrieval,” in\\nInternational Conference on Machine Learning . PMLR, 2022, pp.\\n468–485.\\n[176] M. Yasunaga, A. Aghajanyan, W. Shi, R. James, J. Leskovec, P. Liang,\\nM. Lewis, L. Zettlemoyer, and W.-t. Yih, “Retrieval-augmented multi-\\nmodal language modeling,” arXiv preprint arXiv:2211.12561 , 2022.\\n[177] J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language-\\nimage pre-training with frozen image encoders and large language\\nmodels,” arXiv preprint arXiv:2301.12597 , 2023.\\n[178] W. Zhu, A. Yan, Y . Lu, W. Xu, X. E. Wang, M. Eckstein, and W. Y .\\nWang, “Visualize before you write: Imagination-guided open-ended\\ntext generation,” arXiv preprint arXiv:2210.03765 , 2022.\\n[179] J. Zhao, G. Haffar, and E. Shareghi, “Generating synthetic speech from\\nspokenvocab for speech translation,” arXiv preprint arXiv:2210.08174,\\n2022.\\n[180] D. M. Chan, S. Ghosh, A. Rastrow, and B. Hoffmeister, “Using external\\noff-policy speech-to-text mappings in contextual end-to-end automated\\nspeech recognition,” arXiv preprint arXiv:2301.02736 , 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='21\\n[181] A. Yang, A. Nagrani, P. H. Seo, A. Miech, J. Pont-Tuset, I. Laptev,\\nJ. Sivic, and C. Schmid, “Vid2seq: Large-scale pretraining of a visual\\nlanguage model for dense video captioning,” in Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\\n2023, pp. 10 714–10 726.\\n[182] N. Nashid, M. Sintaha, and A. Mesbah, “Retrieval-based prompt\\nselection for code-related few-shot learning,” in 2023 IEEE/ACM 45th\\nInternational Conference on Software Engineering (ICSE) , 2023, pp.\\n2450–2462.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f90cbab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Text splitting get into chunks\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51d202bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 63 documents into 391 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz...\n",
      "Metadata: {'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='entirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='transduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='efﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\\nof continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\\nsequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='around each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\\nthe matrix of outputs as:\\nAttention(Q,K,V ) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='of 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = ∑dk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\\nwhere headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈Rdmodel×dk , WK\\ni ∈Rdmodel×dk , WV\\ni ∈Rdmodel×dv\\nand WO ∈Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='position in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='of the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0,xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Similarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 ·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel )\\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='PE(pos,2i) = sin(pos/100002i/dmodel )\\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='during training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='computational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 15], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+ n·d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='target vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate= d−0.5\\nmodel ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_stepstraining steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps= 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\nResidual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\n7'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0 ·1020\\nGNMT + RL [31] 24.6 39.92 2.3 ·1019 1.4 ·1020\\nConvS2S [8] 25.16 40.46 9.6 ·1018 1.5 ·1020\\nMoE [26] 26.03 40.56 2.0 ·1019 1.2 ·1020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0 ·1020\\nGNMT + RL Ensemble [31] 26.30 41.16 1.8 ·1020 1.1 ·1021\\nConvS2S Ensemble [8] 26.36 41.29 7.7 ·1019 1.2 ·1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.0 2.3 ·1019\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='dropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α= 0.6 [31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='single-precision ﬂoating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='multi-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='tensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='machine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Recognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='IEEE REVIEWS IN BIOMEDICAL ENGINEERING 1\\nNatural Language Processing for Smart Healthcare\\nBinggui Zhou, Guanghua Yang \\x00 , Zheng Shi, and Shaodan Ma \\x00\\nAbstract—Smart healthcare has achieved signiﬁcant progress\\nin recent years. Emerging artiﬁcial intelligence (AI) technologies\\nenable various smart applications across various healthcare\\nscenarios. As an essential technology powered by AI, natural\\nlanguage processing (NLP) plays a key role in smart healthcare\\ndue to its capability of analysing and understanding human\\nlanguage. In this work, we review existing studies that concern\\nNLP for smart healthcare from the perspectives of technique and\\napplication. We ﬁrst elaborate on different NLP approaches and\\nthe NLP pipeline for smart healthcare from the technical point\\nof view. Then, in the context of smart healthcare applications\\nemploying NLP techniques, we introduce representative smart\\nhealthcare scenarios, including clinical practice, hospital man-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='of view. Then, in the context of smart healthcare applications\\nemploying NLP techniques, we introduce representative smart\\nhealthcare scenarios, including clinical practice, hospital man-\\nagement, personal care, public health, and drug development. We\\nfurther discuss two speciﬁc medical issues, i.e., the coronavirus\\ndisease 2019 (COVID-19) pandemic and mental health, in which\\nNLP-driven smart healthcare plays an important role. Finally,\\nwe discuss the limitations of current works and identify the\\ndirections for future works.\\nIndex Terms—Natural Language Processing, Smart Health-\\ncare, Artiﬁcial Intelligence, NLP Techniques, Healthcare Appli-\\ncations\\nI. I NTRODUCTION\\nS\\nMART healthcare is a healthcare system that exploits\\nemerging technologies, such as artiﬁcial intelligence (AI),\\nblockchain, big data, cloud/edge computing, and the internet\\nof things (IOT), for realizing various intelligent systems to\\nconnect healthcare participants and promote the quality of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='blockchain, big data, cloud/edge computing, and the internet\\nof things (IOT), for realizing various intelligent systems to\\nconnect healthcare participants and promote the quality of\\nhealthcare [1]. Major participants in smart healthcare can\\nbe classiﬁed into three categories, i.e., the public, health-\\ncare service providers, and third-party healthcare participants.\\nRelated to the participants, representative smart healthcare\\nscenarios include smart homes, smart hospitals, intelligent\\nresearch and development for life science, health management,\\npublic health, rehabilitation therapy, and etc. Fig. 1 shows the\\nmajor participants, emerging technologies, and representative\\nscenarios of smart healthcare.\\nNatural language processing (NLP) is a subﬁeld of com-\\nputer science and artiﬁcial intelligence that is concerned with\\nBinggui Zhou is with the School of Intelligent Systems Science and\\nEngineering, Jinan University, Zhuhai 519070, China; and also with the State'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='Binggui Zhou is with the School of Intelligent Systems Science and\\nEngineering, Jinan University, Zhuhai 519070, China; and also with the State\\nKey Laboratory of Internet of Things for Smart City and the Department of\\nElectrical and Computer Engineering, University of Macau, Macao 999078,\\nChina.\\nGuanghua Yang is with the School of Intelligent Systems Science and\\nEngineering, Jinan University, Zhuhai 519070, China.\\nZheng Shi is with the School of Intelligent Systems Science and Engineer-\\ning, Jinan University, Zhuhai 519070, China; and also with the State Key\\nLaboratory of Internet of Things for Smart City, University of Macau, Macao\\n999078, China.\\nShaodan Ma is with the State Key Laboratory of Internet of Things for\\nSmart City and the Department of Electrical and Computer Engineering,\\nUniversity of Macau, Macao 999078, China.\\n\\x00 Corresponding authors: Guanghua Yang (ghyang@jnu.edu.cn), Shaodan\\nMa (shaodanma@um.edu.mo).\\nthe automatic analysis, representation and understanding of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='University of Macau, Macao 999078, China.\\n\\x00 Corresponding authors: Guanghua Yang (ghyang@jnu.edu.cn), Shaodan\\nMa (shaodanma@um.edu.mo).\\nthe automatic analysis, representation and understanding of\\nhuman language [2]. NLP has become a hot research area\\nand has attracted widespread attention from many research\\ncommunities in the past several years. As human language\\nis a general form of data entry for intelligent systems, NLP\\nenables machines to understand human language and interact\\nwith humans, making it essential to smart healthcare.\\nThe main manifestations of natural language are text and\\nspeech, where text encompasses text records, articles, book\\nchapters, dictionaries, and so forth, while speech occurs in\\nhuman-human and human-machine dialogues. NLP has been\\ndeveloped for several decades following the early origin of\\nartiﬁcial intelligence in the 1950s. Approaches to conduct\\nNLP are generally divided into three categories: rule-based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='developed for several decades following the early origin of\\nartiﬁcial intelligence in the 1950s. Approaches to conduct\\nNLP are generally divided into three categories: rule-based\\napproaches, statistical approaches, and deep learning-based\\napproaches. From the 1950s to 1980s, NLP research mainly\\nfocused on rule-based approaches, which required expertise in\\nboth computer science and linguistics to design rules that ﬁt\\nhuman language. However, even well-designed rules are quite\\nlimited for covering human language due to its ﬂexibility and\\ncomplex patterns. Since the 1980s, statistical NLP systems\\nhave been designed by extracting features from corpora using\\nstatistical and machine learning algorithms and have gradually\\nreplaced rule-based NLP systems due to their superiority in\\nperformance and robustness. With the early application of\\nthe neural probabilistic language model [3] and the rapid\\ndevelopment of deep learning since 2013, neural NLP, by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='performance and robustness. With the early application of\\nthe neural probabilistic language model [3] and the rapid\\ndevelopment of deep learning since 2013, neural NLP, by\\nusing neural networks and large corpora for automated feature\\nlearning, has dominated current research and achieved SOTA\\nperformance of many NLP tasks.\\nIn smart healthcare, NLP is applied to process text data and\\nis associated with human-machine/human-human communica-\\ntion. The text data can be classiﬁed into 2 categories: clinical\\ntext and other text data. Clinical text comes from all clinical\\nscenarios and mainly comprises of unstructured text records\\nfrom electronic health record (EHR) systems, including med-\\nical notes, diagnostic reports, electronic prescriptions, and\\netc. Other text data include all text that appears within other\\nhealthcare scenarios, e.g., surveys in population screening and\\narticles for evidence-based reference. Communication is com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='etc. Other text data include all text that appears within other\\nhealthcare scenarios, e.g., surveys in population screening and\\narticles for evidence-based reference. Communication is com-\\nmon in all smart healthcare scenarios, such as patient-provider\\ncommunication in clinical inquiry and human-robot interaction\\nin rehabilitation therapy, accompanied by applications such\\nas machine translations and user interfaces for rehabilitation\\nrobots.\\nAs well recognized, research on and applications of NLP\\nfor smart healthcare have received intensive attention in recent\\nyears. However, no study has offered a well-organized sum-\\nmary of existing works in a systematic way. In this paper, we\\nﬁrst provide a systematic review of NLP for smart healthcare\\n© 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='© 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media,\\nincluding reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers\\nor lists, or reuse of any copyrighted component of this work in other works.\\narXiv:2110.15803v3  [cs.CL]  26 Sep 2022'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 1, 'page_label': '2', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='2 IEEE REVIEWS IN BIOMEDICAL ENGINEERING\\nMajor Participants\\nThird Parties\\ngovernment\\n medical insurance\\nService Providers\\nhealthcare\\ninstitutions\\nlife science\\ncompanies\\nSmart\\nHealthcare\\nPublic\\nhealthy people patients\\nEmerging Technologies\\nAI\\n cloud computing\\nIoT\\n blockchain\\nRepresentative Scenarios\\nsmart hospitals\\nintelligent R&D\\nfor life science\\npublic health\\npromotion\\nhealth monitoringa\\nb\\nc\\nFig. 1. Smart healthcare. a, major participants in smart healthcare include the public, healthcare service providers, and third-party healthcare participants.\\nb, example emerging technologies enable smart healthcare applications include artiﬁcial intelligence, blockchain, cloud computing, the internet of things, and\\netc. c, representative smart healthcare scenarios include intelligent research and development for life science, public health promotion, smart hospitals, health\\nmonitoring, and etc.\\nfrom both technical and application perspectives. After that,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 1, 'page_label': '2', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='monitoring, and etc.\\nfrom both technical and application perspectives. After that,\\nwe discuss two speciﬁc medical issues, i.e., coronavirus dis-\\nease 2019 (COVID-19) pandemic and mental health, in which\\nNLP-driven smart healthcare plays an important role. Finally\\nwe discuss the limitations of existing works, identify the future\\ndirections of applying NLP to smart healthcare, and close the\\nreview with some conclusions.\\nII. NLP FOR SMART HEALTHCARE FROM TECHNICAL\\nPERSPECTIVE\\nNLP has been undergoing continuous development since the\\n1950s. Studies on NLP for smart healthcare have also been\\nconducted for decades and have attracted increased attention\\nin recent years with the advancement of artiﬁcial intelligence\\nand general NLP. To connect existing works from technical\\nperspective, in this section, we ﬁrst introduce the three kinds of\\nNLP approaches and their representative algorithms, and then\\nintroduce the NLP pipeline for smart healthcare to show how'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 1, 'page_label': '2', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='perspective, in this section, we ﬁrst introduce the three kinds of\\nNLP approaches and their representative algorithms, and then\\nintroduce the NLP pipeline for smart healthcare to show how\\nNLP techniques are used in real smart healthcare applications.\\nA. Comparisons of different NLP approaches\\nThe mainstream NLP approaches can be classiﬁed into three\\ncategories, i.e., rule-based NLP, statistical NLP and neural\\nNLP, which have different characteristics. Below, we discuss\\nthe advantage and disadvantages of the three categories and\\nintroduce the representative algorithms of them.\\nRule-based NLP approaches, e.g., pattern matching [4]\\nand parsing [5], could be quite accurate in speciﬁc cases if\\ndedicated studies by experts are conducted. In addition, rule-\\nbased NLP approaches are easy to interpret and understand.\\nHowever, rules are normally too limited to cover all cases\\nconsidering the ﬂexibility and complex patterns of human lan-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 1, 'page_label': '2', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='based NLP approaches are easy to interpret and understand.\\nHowever, rules are normally too limited to cover all cases\\nconsidering the ﬂexibility and complex patterns of human lan-\\nguage. In addition, rule-based NLP requires expertise in both\\ncomputer science and linguistics to design appropriate rules to\\nﬁt human language, hindering it from large-scale applications.\\nCurrently, rule-based approaches have been widely considered\\nobsolete by academia [6], and are occasionally used for better\\npreprocessing nowadays [7].\\nIn general, statistical NLP is superior to rule-based NLP in\\nperformance and robustness. However, it also requires domain\\nexpertise to create handcrafted features, and is therefore lim-\\nited to taking full advantage of available data and providing\\nenough accuracy in complex applications. Although statistical\\nNLP requires intensive feature engineering, it is this direct\\nfeature design that makes it transparent and interpretable as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 1, 'page_label': '2', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='enough accuracy in complex applications. Although statistical\\nNLP requires intensive feature engineering, it is this direct\\nfeature design that makes it transparent and interpretable as\\nrule-based NLP. In addition, statistical NLP does not rely on\\nlarge-scale datasets or large amounts of computational power,\\nand thus is much more efﬁcient than neural NLP. Furthermore,\\nrepresentative statistical NLP models, such as bag-of-words\\n[8], TF-IDF [9], [10], and n-gram [11]–[13], have different\\ncharacteristics. Bag-of-words is easy to implement, but it\\nonly considers the frequencies of words in a sentence, which\\nneglects the importance and sequential order of these words.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ZHOU et al.: NATURAL LANGUAGE PROCESSING FOR SMART HEALTHCARE 3\\nThrough the inverse document frequency, TF-IDF improves\\nthe measurement of a word’s importance, but still does not\\ntake sequential order information into consideration. N-gram\\nconsiders n − 1 words before a word, which makes it more\\naccurate than bag-of-words but with higher computational\\ncomplexity (increases exponentially with n). It is worth men-\\ntioning that despite the dominance of deep learning in recent\\nyears, statistical NLP is still active in many healthcare studies\\nand applications.\\nRecent years have witnessed the success of neural NLP, who\\nhas shown better performance than both rule-based NLP and\\nstatistical NLP in applications with abundant available data.\\nHowever, neural NLP is often blamed for low interpretability\\nand dependence on expensive computing platforms. It is\\nalso worth noting that, compared with rule-based NLP and\\nstatistical NLP, neural NLP usually fails to achieve satisfac-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='and dependence on expensive computing platforms. It is\\nalso worth noting that, compared with rule-based NLP and\\nstatistical NLP, neural NLP usually fails to achieve satisfac-\\ntory performance if limited data is available. Among neural\\nNLP models, recurrent neural network (RNN)-based models,\\nespecially long short-term memory (LSTM) [14]–[16]-based\\nmodels and gated recurrent unit (GRU)-based models [15],\\n[17], are more natural for processing sequential data such\\nas text and speech. They have the ability to remember his-\\ntorical information of the inputs, but suffer from gradient\\nvanishing/explosion, training issues and short-term memories.\\nConvolutional neural networks (CNN)-based models [18],\\n[19], combining with word embeddings, also show good\\nperformance in some tasks due to their ability in learning\\nlocal features and high computational efﬁciency which enables\\ndeep network architectures. Recently, graph neural network\\n(GNN)-based models have been applied to NLP-driven smart'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='local features and high computational efﬁciency which enables\\ndeep network architectures. Recently, graph neural network\\n(GNN)-based models have been applied to NLP-driven smart\\nhealthcare by incorporating knowledge from graph-structured\\nontology/entities [20]–[22]. When graphs are large in scale\\nor complex, GNN-based models are difﬁcult and costly to\\nimplement and train. Generally speaking, RNNs, CNNs, and\\nGNNs are all limited in tackling long-term dependencies in se-\\nquences. Through the self-attention mechanism, Transformer-\\nbased models [23], [24] are very efﬁcient in processing\\nlong sequences and support parallel training, but are lack\\nof ability in learning local features and position information.\\nWe have witnessed many combinations of the aforementioned\\nmodels for better feature extraction performance, including\\nCNN-LSTM networks [25], RNN-Attention networks [26],\\n[27], memory networks (MM) [28], [29], graph convolutional\\nnetworks (GCN) [30], CNN-LSTM-Attention networks [31],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='CNN-LSTM networks [25], RNN-Attention networks [26],\\n[27], memory networks (MM) [28], [29], graph convolutional\\nnetworks (GCN) [30], CNN-LSTM-Attention networks [31],\\n[32], graph convolutional attention networks (GCAN) [33],\\n[34], etc. In addition, to further leverage large unlabelled\\ncorpora, pretraining, a very effective method, has been widely\\nexploited to obtain non-contextual or contextual embeddings\\n[35]. Word2vec [36]–[39], and GloVe (Global Vectors) [36],\\n[40], as representative algorithms of non-contextual embed-\\ndings, provide distributed dense vectors as word embeddings,\\nand outperform statistical algorithms such as bag-of-words and\\nn-gram. The non-contextual embedding for a word is static\\nand does not dynamically change as its context changes [35].\\nBased on the Transformer architecture, contextual embed-\\ndings, e.g., ELMo (Embeddings from Language Models) [41],\\nBERT (Bidirectional Encoder Representations from Trans-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='Based on the Transformer architecture, contextual embed-\\ndings, e.g., ELMo (Embeddings from Language Models) [41],\\nBERT (Bidirectional Encoder Representations from Trans-\\nformers) [42]–[45], and GPT (Generative Pre-Training) [46],\\nare developed to embed dynamic contextual information into\\nword embeddings, achieving outstanding performance than\\nother word embedding algorithms. It should be noted that these\\nmodels are typically huge and expensive to pre-train, which\\nsomehow constraints their broad application in healthcare.\\nThe comparisons of different NLP approaches and repre-\\nsentative algorithms are shown in Table I.\\nB. NLP pipeline for smart healthcare\\nAs shown in Fig. 2, there are three parts in an NLP pipeline\\nfor smart healthcare, i.e., preprocessing, feature extraction, and\\nmodelling. An NLP pipeline takes text or speech as illustrated\\nbefore as the input. After that, preprocessing is conducted\\nconsidering various inputs and their qualities to facilitate'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='modelling. An NLP pipeline takes text or speech as illustrated\\nbefore as the input. After that, preprocessing is conducted\\nconsidering various inputs and their qualities to facilitate\\nfeature extraction and modelling. As the most important step,\\nfeature extraction is essential to NLP, which undoubtedly\\nexplains the attention it has received from researchers. Finally,\\nmodels for speciﬁc NLP tasks are built with the extracted\\nfeatures to yield the outputs accordingly.\\n1) Preprocessing: Preprocessing, including the procedures\\nof tokenization, stemming, lemmatization, stopword removal,\\nand etc., makes natural language normalized, machine-\\nreadable, and easy for postprocessing. Text preprocessing\\nmostly paves the way for feature extraction and modelling,\\nsince many NLP tasks require normalized text input to guar-\\nantee accuracy and efﬁciency due to signiﬁcant challenges\\ncoming from the ﬂexibility of natural languages and the\\nwide variety of morphological variants of medical terms in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='antee accuracy and efﬁciency due to signiﬁcant challenges\\ncoming from the ﬂexibility of natural languages and the\\nwide variety of morphological variants of medical terms in\\nmedical text [47]–[49]. However, with the development of\\nneural NLP, some text preprocessing procedures have become\\nunnecessary and may even cause problems. For example,\\nremoving stopwords may lead to the loss of informative\\ncontext information when using the BERT pre-trained model\\n[50]. As the preprocessing of speech, such as denoising, is\\ntypically regarded as a problem in signal processing, we do\\nnot discuss it in detail here.\\n2) Feature extraction: Apart from the increase in accessible\\ndigital data and the advances in computing platforms such\\nas graphics processing units, the development of NLP is\\nlargely attributed to the improvement in feature design or\\nfeature extraction methods. Both rule-based approaches and\\nstatistical approaches require expertise for rule design [4],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='largely attributed to the improvement in feature design or\\nfeature extraction methods. Both rule-based approaches and\\nstatistical approaches require expertise for rule design [4],\\n[5] or feature engineering [8]–[13]. For neural NLP, auto-\\nmated feature extraction via varieties of neural networks [14]–\\n[34] have greatly improved the efﬁciency of data utilization\\nand feature extraction. Automated feature engineering can be\\nconducted directly according to the downstream tasks using\\nsupervised learning, unsupervised learning or reinforcement\\nlearning. In addition, pretraining is also widely used in NLP\\nto automatically extract features from large unlabelled corpora\\nvia self-supervised learning in a generative, contrastive or\\ngenerative-contrastive manner [51] before the downstream\\ntasks begin. The extracted features, known as contextual or\\nnon-contextual embeddings, may encompass features such as\\nlexical meanings, syntactic features, semantic features, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='tasks begin. The extracted features, known as contextual or\\nnon-contextual embeddings, may encompass features such as\\nlexical meanings, syntactic features, semantic features, and\\neven pragmatics, which contribute to downstream tasks [35].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 3, 'page_label': '4', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='4 IEEE REVIEWS IN BIOMEDICAL ENGINEERING\\nTABLE I\\nCOMPARISONS OF DIFFERENT NLP APPROACHES AND REPRESENTATIVE ALGORITHMS .\\nNLP Approach Feature Extrac-\\ntion Method\\nAdvantages and Disadvantages Representative Algorithms\\nRule-based NLP rule design\\nadvantages :\\n- could be quite accurate in speciﬁc\\ncases;\\n- easy to interpret and understand\\ndisadvantages :\\n- rules are too limited to cover all\\ncases considering the ﬂexibility and\\ncomplex patterns of human language;\\n- require expertise in both computer\\nand linguistics to ﬁt human language\\npattern matching [4] and parsing [5]\\nStatistical NLP hand-crafted fea-\\nture engineering\\nadvantages :\\n- superior to rule-based NLP in\\nperformance and robustness;\\n- good interpretability\\ndisadvantages :\\n- require domain expertise to create\\nhandcrafted features;\\n- limited to taking full advantage of\\navailable data and providing enough\\naccuracy in complex applications\\nbag-of-words [8]:\\n- easy to implement;\\n- neglects the importance and sequential order of words'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 3, 'page_label': '4', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='available data and providing enough\\naccuracy in complex applications\\nbag-of-words [8]:\\n- easy to implement;\\n- neglects the importance and sequential order of words\\nTF-IDF [9], [10]:\\n- improves the measurement of a word’s importance;\\n- does not take sequential order information into consideration\\nn-gram [11]–[13]:\\n- more accurate than bag-of-words;\\n- high computational complexity (increasing exponentially with n)\\nNeural NLP automated\\nfeature extraction\\nadvantages :\\n- better performance than both\\nrule-based NLP and statistical NLP\\nin applications with abundant\\navailable data\\ndisadvantages :\\n- low interpretability;\\n- dependence on expensive computing\\nplatforms;\\n- usually fail to achieve satisfactory\\nperformance if limited data is\\navailable\\n1) RNN-based models (e.g., LSTM [14]–[16] and GRUs [15], [17]):\\n- more natural for processing text and speech input;\\n- capable of remembering historical information of the inputs;\\n- suffer from gradient vanishing/explosion, training issues and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 3, 'page_label': '4', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='- more natural for processing text and speech input;\\n- capable of remembering historical information of the inputs;\\n- suffer from gradient vanishing/explosion, training issues and\\nshort-term memories\\n2) CNN-based models [18], [19]:\\n- able to learn local features;\\n- high computational efﬁciency;\\n- limited in tackling long-term dependencies in sequences\\n3) GNN-based models [20]–[22]:\\n- efﬁcient in incorporating knowledge from graph-structured\\nontology/entities;\\n- limited in tackling long-term dependencies in sequences;\\n- difﬁcult and costly to implement and train with large-scale\\nor very complex graphs\\n4) Transformer-based models [23], [24]:\\n- efﬁcient in processing long sequences and parallel training;\\n- lack of ability in learning local features and position information\\n5) combinations: CNN-LSTM [25], RNN-Attention [26], [27],\\nMN [28], [29], GCN [30], CNN-LSTM-Attention [31], [32],\\nand GCAN [33], [34], etc.\\n6) non-contextual embedding-oriented pre-trained models'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 3, 'page_label': '4', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='5) combinations: CNN-LSTM [25], RNN-Attention [26], [27],\\nMN [28], [29], GCN [30], CNN-LSTM-Attention [31], [32],\\nand GCAN [33], [34], etc.\\n6) non-contextual embedding-oriented pre-trained models\\n(word2vec [36]–[39], GloVe [36], [40]):\\n- outperform statistical algorithms;\\n- the non-contextual embedding for a word is static and will not\\ndynamically change as its context change\\n7) contextual embedding-oriented pre-trained models (ELMo [41],\\nBERT [42]–[45], GPT [46]):\\n- able to embed dynamic contextual information into word embeddings;\\n- outstanding performance than other word embedding algorithms;\\n- typically huge and expensive to pre-train'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ZHOU et al.: NATURAL LANGUAGE PROCESSING FOR SMART HEALTHCARE 5\\nInputs OutputsFeature ExtractionPreprocessing\\n• Text Classification\\n• Information Extraction\\n• Machine Translation\\n• Text Generation\\n• Information Retrieval\\n• Question Answering and Dialogue\\n• Knowledge Engineering\\n• Natural Language Understanding\\n• Causal Inference\\n• Speech Recognition\\n• Speech Synthesis\\n• …\\nModelling\\nFig. 2. The NLP pipeline for smart healthcare . There are three parts in an NLP pipeline for smart healthcare, i.e., preprocessing, feature extraction, and\\nmodelling. NLP takes text or speech as the input, followed by preprocessing to facilitate feature extraction and modelling. Features can be extracted with\\nvarious methods and models. Models for speciﬁc NLP tasks are ﬁnally built with the extracted features to yield the outputs.\\n3) Modelling: For various smart healthcare applications,\\ndifferent models should be built to accomplish various NLP\\ntasks, such as text classiﬁcation, information extraction, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='3) Modelling: For various smart healthcare applications,\\ndifferent models should be built to accomplish various NLP\\ntasks, such as text classiﬁcation, information extraction, and\\nnatural language understanding. The extracted feature can be\\ndirectly processed by classiﬁers and regressors to yield outputs\\nfor simple tasks, e.g., medical text classiﬁcation [18], [52],\\nwhile further steps are required to complete complex tasks. In\\nthe following subsections, we ﬁrst introduce several text input-\\nbased NLP tasks according to their complexity. At the end of\\nthis section, we will introduce two speech-speciﬁc tasks, i.e.,\\nspeech recognition and speech synthesis.\\nInformation extraction. Information extraction (IE), a.k.a.\\ntext mining, enables harvesting information from text inputs,\\nand plays an important role in text analysis. Works related to\\ninformation extraction in smart healthcare focus on the ex-\\ntraction of diseases, drugs, events (mainly including temporal'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='and plays an important role in text analysis. Works related to\\ninformation extraction in smart healthcare focus on the ex-\\ntraction of diseases, drugs, events (mainly including temporal\\nexpressions, spatial expressions and participant information)\\nthrough name entity recognition [53], [54], relation extraction\\n[54]–[56], and event extraction [57] from medical text, includ-\\ning unstructured text in EHRs, articles, etc.\\nMachine translation . Machine translation (MT) aims to\\nautomatically translate text from one language to another\\n[58]. Currently, healthcare resources in various languages are\\nbecoming easily accessible as technologies evolve, and they\\nare all of great value in modern medical practice. Machine\\ntranslation therefore has drawn growing attention for building\\nbetter (multilingual) translation systems and further leveraging\\nmultilingual healthcare resources for other applications, either\\nto provide more accurate translations [59], [60] or to require'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='better (multilingual) translation systems and further leveraging\\nmultilingual healthcare resources for other applications, either\\nto provide more accurate translations [59], [60] or to require\\nless time [60] than human translations.\\nText generation. Text generation (TG) automatically gener-\\nates text with given inputs while pursuing the goal of appearing\\nindistinguishable from human-written text. Speciﬁcally, there\\nare 3 kinds of inputs and corresponding subtasks in smart\\nhealthcare: text inputs (e.g., routine reports) associated with\\ntext summarization [61]–[63], question generation [64]–[66],\\ndialogue generation [67]–[69], and etc.; data inputs (e.g.,\\nneonatal intensive care data) connected with data-to-text [70];\\nand image inputs (e.g., medical images) related to image cap-\\ntioning [71], [72], visual question answering (VQA) [73]–[75],\\nand etc. Note that for data-to-text and image-to-text generation,\\na combination of NLP with data analysis or computer vision'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='tioning [71], [72], visual question answering (VQA) [73]–[75],\\nand etc. Note that for data-to-text and image-to-text generation,\\na combination of NLP with data analysis or computer vision\\nis generally required, respectively.\\nInformation retrieval . Information retrieval (IR) obtains\\nmaterials that meet the query requirements from numerous\\ndocuments, and is a core of search engines for all applications.\\nTo ease the retrieval process [76], [77], improve the relevance\\nand diversity of the retrieval [78]–[80] or reduce the query\\ntime [81], current works aim to develop fast and efﬁcient\\ninformation retrieval methods to obtain useful retrieval from a\\nlarge collection of data sources, ranging from internal health\\ninformation system (HIS) systems and other digital documents\\nto online resources.\\nQuestion answering and dialogue systems . Question an-\\nswering (QA) involves automatically providing answers to\\nquestions raised by humans in a natural language. Question'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='to online resources.\\nQuestion answering and dialogue systems . Question an-\\nswering (QA) involves automatically providing answers to\\nquestions raised by humans in a natural language. Question\\nanswering requires the machine to understand natural language\\nand infer the answers, making it highly dependent on natural\\nlanguage understanding and information retrieval. To date,\\nQA systems for healthcare have developed from information\\nretrieval based QA systems [82]–[85] and knowledge-based\\nQA systems [86]–[89] to hybrid QA systems [90], [91].\\nCompared to question answering, dialogue is also presented\\nin an interactive manner between humans and machines.\\nCommon dialogue systems in smart healthcare include task-\\noriented dialogue systems [92]–[94], and non-task-oriented\\n(a.k.a. chat-oriented) [95] dialogue systems, which assume\\ndifferent functions in various applications.\\nKnowledge engineering . Knowledge engineering (KE) is\\na ﬁeld within artiﬁcial intelligence that tries to construct and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='different functions in various applications.\\nKnowledge engineering . Knowledge engineering (KE) is\\na ﬁeld within artiﬁcial intelligence that tries to construct and\\nuse knowledge-based systems [96]. It does not refer to a pure\\nNLP technique, but receives much attention in NLP for smart\\nhealthcare since medical text is one of the major sources\\nfor knowledge engineering. Within knowledge engineering,\\nknowledge acquisition and knowledge representation are cou-\\npling with information extraction, aiming at the acquisition\\nand representation of medical knowledge in a certain way,\\ne.g., knowledge graphs [97]–[99]. Besides, knowledge engi-\\nneering also concerns building knowledge-based systems to\\nexploit existing knowledge, such as knowledge-based ques-\\ntion answering (KBQA) systems [86]–[89], knowledge-based\\ninformation retrieval systems [100], text generation systems\\n[65], [101], etc.\\nNatural language understanding . Natural language un-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='tion answering (KBQA) systems [86]–[89], knowledge-based\\ninformation retrieval systems [100], text generation systems\\n[65], [101], etc.\\nNatural language understanding . Natural language un-\\nderstanding (NLU) focuses on machines’ comprehension of\\nhuman language in the form of unstructured text or speech.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='6 IEEE REVIEWS IN BIOMEDICAL ENGINEERING\\nMany of the aforementioned tasks, e.g., question answering,\\ninformation retrieval, require NLU to fully understand the in-\\nput queries. The difﬁculties of natural language understanding\\ncome from the diversity, ambiguity, and potential dependence\\nof natural language, making slow progress in natural language\\nunderstanding compared with other NLP techniques. After\\nyears of development in both general areas and smart health-\\ncare, the mainstream route of NLU is still to use various meth-\\nods to conduct slot ﬁlling and intent detection [102]–[104].\\nNLU is the core of multiple intelligent agents, assuming a\\nrole in understanding human intentions during human-machine\\ninteractions [102], [105], [106], medical queries [103], [104],\\netc.\\nCausal inference. Generally, causal inference is a discipline\\nconcerning the determination of actual effects of speciﬁc\\nthings, events or phenomena. Causal inference in NLP has'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='etc.\\nCausal inference. Generally, causal inference is a discipline\\nconcerning the determination of actual effects of speciﬁc\\nthings, events or phenomena. Causal inference in NLP has\\nlong received insufﬁcient attention since the goal of classical\\nNLP applications is simply to make accurate predictions with\\nall available statistical correlations regardless of the underlying\\ncausal relationship [107]. Recently, with growing concerns\\nabout uninterpretable black box models, the importance of\\ncausal inference has gradually been recognized by NLP re-\\nsearchers, especially in the area of healthcare. Speciﬁcally, re-\\ncent advances of causal inference in NLP for smart healthcare\\nhave been made in uncovering causality from medical text\\n[108]–[110] and realizing reliable NLP-driven applications\\nwith discovered causal effects [108]–[110].\\nSpeech recognition and speech synthesis. Speech recogni-\\ntion (SR) aims to convert human speech into text information.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='with discovered causal effects [108]–[110].\\nSpeech recognition and speech synthesis. Speech recogni-\\ntion (SR) aims to convert human speech into text information.\\nContrary to speech recognition, speech synthesis, a.k.a. text-to-\\nspeech (TTS), is concerned with representing text information\\nwith speech. Basically, SR-oriented and SS-oriented studies\\nattempt to build automatic computer systems for interconver-\\nsion between speech and text in the area of smart healthcare,\\nmaking human-machine interaction as natural and ﬂexible as\\nhuman-human interaction [111]. For speech recognition, these\\nefforts encompass the improvement in acoustic modelling\\n[112], [113], language modelling [114], and the whole system\\npipeline [115], [116] to enhance recognition accuracy. For\\nspeech synthesis, recent advancements have been made in\\ninvestigating and making synthesized speech natural [117],\\n[118], intelligible [119]–[123] and expressive [124]–[126],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='speech synthesis, recent advancements have been made in\\ninvestigating and making synthesized speech natural [117],\\n[118], intelligible [119]–[123] and expressive [124]–[126],\\nwhich will help stimulate the enthusiasm of human-machine\\ninteraction [127].\\nIII. A PPLICATIONS OF NLP FOR SMART HEALTHCARE\\nNLP has been widely applied in smart healthcare and\\nhas brought dramatic improvements in many applications. As\\nshown in Fig. 3, a typical NLP-driven application is composed\\nof two parts: user interface (UI) and backend. The user\\nprovides text or speech input to the backend through the UI,\\nand then, the backend processes these inputs with the NLP\\nmodels and feeds the results back to the user by providing\\nspeciﬁc services through the UI. Knowledge bases are also\\nrequired at the backend for applications that essentially rely on\\nknowledge, for example, the aforementioned KBQA systems.\\nThe NLP techniques described in the previous section play a\\nkey role in both UI and backend.\\ntext\\n speech'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='knowledge, for example, the aforementioned KBQA systems.\\nThe NLP techniques described in the previous section play a\\nkey role in both UI and backend.\\ntext\\n speech\\n speech\\nrecognition\\nmachine\\ntranslation\\nBackend\\nUser Interfaces\\nUser\\nSupporting \\nModels\\nKnowledge \\nBases\\nServices\\nInputs\\nFig. 3. Basic architecture of NLP-driven applications . A typical NLP-\\ndriven application is composed of user interface and backend, where the UI\\ntakes inputs from the user and feedback the results to the user, and the backend\\nprocesses these inputs with the NLP models with or without the knowledge\\nbases according to the speciﬁc task type.\\nThe UI enables information exchange between users and\\nintelligent systems through speech, text, etc. Easily accessible\\nUIs are critical for enhancing the experience of using intelli-\\ngent systems and realizing smart healthcare. Such user inter-\\nfaces can be implemented by using NLP techniques, especially\\nspeech recognition and natural language understanding.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='gent systems and realizing smart healthcare. Such user inter-\\nfaces can be implemented by using NLP techniques, especially\\nspeech recognition and natural language understanding.\\nAccording to their application scenarios, smart healthcare\\napplications employing NLP techniques can be classiﬁed into\\n5 major categories, i.e., clinical practice, hospital manage-\\nment, personal care, public health, and drug development. A\\nsummary of the applications and related NLP techniques is\\npresented in Table II. Below we introduce the ﬁve categories\\nin detail.\\nA. Clinical practice\\nClinical communication and data collection . Clinical\\ndata, including but not limited to demographics, medical\\nhistory, comorbidities, medical notes, physical examination\\nnotes, electronic recordings from medical devices, and clinical\\nlaboratory testing data and medical images [128], are the most\\nimportant data for diagnosis, treatment and even further retro-\\nspection. Patient-provider communication is an important way'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='laboratory testing data and medical images [128], are the most\\nimportant data for diagnosis, treatment and even further retro-\\nspection. Patient-provider communication is an important way\\nto obtain ﬁrst-hand clinical data. When necessary, machine\\ntranslation may assist doctors in communicating with patients\\nwho speak different languages or have low literacy and limited\\nlevels of health education [129], [130]. Meanwhile, free text\\nnotes can be taken through speech recognition [131]–[133],\\nwhich can signiﬁcantly reduce medical staff’s time on labour-\\nintensive clinical documentation.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ZHOU et al.: NATURAL LANGUAGE PROCESSING FOR SMART HEALTHCARE 7\\nClinical decision support. Clinical decision support (CDS)\\nsystems can provide physicians with diagnosis and treatment\\nsuggestions, which play an increasingly important role in\\nclinical medicine with the surge of clinical cases and growing\\nconcerns regarding public healthcare. With the development of\\nquestion answering systems, clinical decision support based\\non question answering [84], [134], [135] has emerged and\\nbecome common, as it is closer to traditional patient-provider\\ncommunication. NLP techniques have shown great ability to\\nbuild clinical decision support systems by extracting various\\nuseful information for making diagnosis and therapeutic de-\\ncisions, such as family history information [136], entities and\\nrelations [137], [138], treatment and prognosis data [139],\\nclinical data concepts and features [140], and even causal\\nrelations [109], [110]. In addition, to ensure quality control'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='relations [137], [138], treatment and prognosis data [139],\\nclinical data concepts and features [140], and even causal\\nrelations [109], [110]. In addition, to ensure quality control\\nand future quality improvement, NLP can also contribute to\\nthe assessment of clinical procedures [141], [142], warning\\nof potentially harmful adverse drug events (ADEs) [143],\\ndisease symptoms [144], [145], and outcome-related causal\\neffects [146]. Finally, NLP is also powerful in enhancing\\nthe interpretability and reliability of clinical decision support\\nsystems for practical deployment, by providing supporting\\nevidence for diagnosis or treatment decisions in an evidence-\\nbased fashion [108], [147]–[150].\\nB. Hospital management\\nMedical resource allocation . Due to limited medical re-\\nsources, including hospital spaces, personnel, and materi-\\nals, efﬁcient resource allocation is critical in hospitals and\\nother medical facilities. By building patient triage systems,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='sources, including hospital spaces, personnel, and materi-\\nals, efﬁcient resource allocation is critical in hospitals and\\nother medical facilities. By building patient triage systems,\\nmedical resources can attend to critical cases with priority\\nand enhance medical resource allocation effectiveness and\\nefﬁciency [151], [152]. Virtual assistants [153]–[155], hospital\\nautomation systems [156], [157] and collaborative robots\\n[158], [159] with voice control can further reduce the burden\\non medical staff, thereby improving hospital management\\nefﬁciency. There are also some interesting works that have\\nexplored the prediction of patient readmission to rearrange\\nmedical resources/interventions and reduce the readmission\\nrate [160]–[162]. In addition, by leveraging text generation\\ntechniques, part of text writing in healthcare, especially routine\\nreports, can be taken over by machines, freeing medical staff\\nfrom many administrative duties and making them available'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='techniques, part of text writing in healthcare, especially routine\\nreports, can be taken over by machines, freeing medical staff\\nfrom many administrative duties and making them available\\nfor direct patient care [70], [163].\\nData management . To manage large volumes of medical\\ndocumentation, text classiﬁcation, information extraction and\\ntext summarization can be used to generate category labels,\\ninformative keywords and simpliﬁed summaries [18], [52],\\n[62], [63], [164] for management, while information retrieval\\nsystems, especially those systems based on semantic search\\n[76], [165] and question answering [77], can be used in\\nhealthcare information systems to ease the retrieval process.\\nService quality control . Sentiment analysis with patient\\nexperience feedback will help hospitals improve their service\\nquality and patient experience. Such analysis required sub-\\nstantial personnel resources in the past, while NLP makes this\\nwork easier and greatly improves the efﬁciency of sentiment'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='quality and patient experience. Such analysis required sub-\\nstantial personnel resources in the past, while NLP makes this\\nwork easier and greatly improves the efﬁciency of sentiment\\nanalysis [166]–[168].\\nC. Personal care\\nPersonal health assistants . Personal health assistants en-\\nable people to easily access useful medical information and\\nhealthcare services without visiting the healthcare institutions.\\nPersonal health assistants may incorporate several subsystems,\\nsuch as medical information access systems [169] and remote\\nhealthcare systems [170], for various purposes.\\nAssisting elderly individuals and disabled individuals .\\nNLP techniques can help elderly individuals and disabled\\nindividuals to greatly enhance their quality of life and so-\\ncial integration. V oice-controlled home automation systems\\nand robots may assist the elderly and the disabled in their\\ndaily lives [171], while robots (especially androids and other\\nrobots that communicate with people) can even encourage and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='and robots may assist the elderly and the disabled in their\\ndaily lives [171], while robots (especially androids and other\\nrobots that communicate with people) can even encourage and\\naccompany them through social interactions [172], [173]. In\\naddition, NLP techniques are also of great value for providing\\nessential aids to people with various disabilities, e.g., speech\\nimpairments [122], [174]–[178], hearing loss [179], dyslexia\\n[180], or neurological disorders [119]–[121].\\nD. Public health\\nHealth knowledge popularization and medical educa-\\ntion. Health knowledge popularization and medical education\\nare essential public health interventions since they can im-\\nprove people’s health literacy and help them develop healthy\\nliving habits. Through knowledge engineering, accurate and\\ncomplete medical knowledge bases can be established to\\npromote the popularization of medical knowledge among the\\npopulation [86]–[89], [97]–[100], [181]. Speciﬁcally, people'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='complete medical knowledge bases can be established to\\npromote the popularization of medical knowledge among the\\npopulation [86]–[89], [97]–[100], [181]. Speciﬁcally, people\\ncan easily access medical knowledge through question an-\\nswering systems [182], [183], information retrieval systems\\n[79], [81], and machine translation systems [1], [130], [184],\\n[185], facilitating the popularization and education of medical\\nknowledge. In addition, text generation techniques, such as\\nquestion generation and text summarization, can also be used\\nin medical education to generate medical case-based questions\\n[186] and construct simpliﬁed summaries [61].\\nPopulation screening. In addition to the health knowledge\\npopularization, population screening, which refers to the pro-\\ncess of assessing the prevalence of a disease or condition in\\na population or subgroup, is also an important intervention\\nfor delivering public health. The population screening starts'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='cess of assessing the prevalence of a disease or condition in\\na population or subgroup, is also an important intervention\\nfor delivering public health. The population screening starts\\nwith identifying target populations, followed by the screening\\ntest. After that, further actions such as further tests, advice, or\\ntreatment can be taken considering the screening results [187].\\nNLP can play two main roles in population screening. First,\\nNLP helps identify populations with higher health risk factors,\\nwhich may improve the efﬁciency of population screening\\n[188]. Second, NLP can also assist in the analysis of healthcare\\nquestionnaires and surveys [189], especially for open-ended\\nquestions.\\nE. Drug development\\nDrug discovery . NLP helps construct textual representa-\\ntions of biochemical entities for mapping the interactions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='8 IEEE REVIEWS IN BIOMEDICAL ENGINEERING\\nbetween diseases, drugs/chemical compounds, and biomacro-\\nmolecules (e.g., genes, proteins); predicting molecular prop-\\nerties; and designing novel molecules. Readers are referred to\\nthe comprehensive review by ¨Ozt¨urk et al [190] for a deeper\\nunderstanding of NLP methodologies for drug discovery.\\nPreclinical research. NLP techniques, especially informa-\\ntion extraction, are also able to identify the relations between\\nchemical structures and biological activity [191] and further\\nhelp researchers search for potentially effective chemical com-\\npounds, i.e., virtual screening [192], [193], in a huge chemical\\nspace. In addition, they are also applied in the prediction\\nof adverse drug reactions, including side effect prediction\\n[194], toxicity prediction [195], [196], and etc., in preclinical\\nresearch.\\nClinical research. Across the clinical research stage, NLP\\nmay enable efﬁcient clinical trial design [110], patient recruit-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='research.\\nClinical research. Across the clinical research stage, NLP\\nmay enable efﬁcient clinical trial design [110], patient recruit-\\nment [197]–[199], clinical trial analytics [200], and etc.\\nDrug review and safety monitoring . Recently, the FDA\\nand other institutions have reported being interested in using\\nNLP for adverse drug event discovery and drug safety moni-\\ntoring [201]–[203], showing the full range of NLP’s key role\\nin drug development.\\nIV. NLP- DRIVEN SMART HEALTHCARE FOR SPECIFIC\\nMEDICAL ISSUES\\nNLP-driven smart healthcare plays an important role in\\nmany medical issues. In this section, we discuss how NLP-\\ndriven smart healthcare works in medical issues by taking two\\nspeciﬁc medical issues, i.e., COVID-19 pandemic and mental\\nhealth, as examples.\\nA. COVID-19 pandemic\\nWorldwide outbreak of COVID-19 has triggered an unprece-\\ndented global health crisis and has attracted much attention\\nfrom researchers [204]. No wonder, the COVID-19 pandemic'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='A. COVID-19 pandemic\\nWorldwide outbreak of COVID-19 has triggered an unprece-\\ndented global health crisis and has attracted much attention\\nfrom researchers [204]. No wonder, the COVID-19 pandemic\\nhas become one of the most inﬂuential medical issues over\\nthe past few years. In the COVID-19 pandemic, NLP-driven\\nsmart healthcare can be utilized for pandemic prevention,\\ndiagnosing, and drug development.\\nEarly forecasts of COVID-19 cases and pandemic knowl-\\nedge popularization are crucial to the prevention of the\\nCOVID-19 pandemic. In [205], an NLP module is embedded\\ninto an improved susceptible–infected model to build the\\nproposed hybrid AI model for COVID-19 prediction, showing\\nthat the forecasting accuracy of COVID-19 cases can be im-\\nproved by incorporating text inputs and with NLP techniques.\\nIn [206], the authors conclude that NLP techniques, e.g.,\\nNLP-aided information retrieval, literature-based discovery,\\nquestion answering and etc., can be applied to address the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='In [206], the authors conclude that NLP techniques, e.g.,\\nNLP-aided information retrieval, literature-based discovery,\\nquestion answering and etc., can be applied to address the\\ninformation/knowledge needs of both researchers and the\\npublic in the COVID-19 pandemic.\\nIn clinical practice, NLP can be utilized to identify posi-\\ntively diagnosed COVID19 patients from free text narratives\\n[207], assess thoracic CT imaging reports [208], and identify\\nindividuals with the greatest risk of severe complications due\\nto COVID-19 [209], and provide COVID-19 testing advice\\n[210]. Such applications would be very useful to accelerate\\nthe diagnosis of COVID-19, mitigate its worst effects, and\\nalso reduce costs for combating the COVID-19 pandemic.\\nNLP has also been applied to drug development confronting\\nCOVID-19. In [211], the authors developed an NLP method\\nto automatically recognize the associations among potential\\ntargeted host organ systems, associated clinical manifestations'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='COVID-19. In [211], the authors developed an NLP method\\nto automatically recognize the associations among potential\\ntargeted host organ systems, associated clinical manifestations\\nand pathways, and suggest potential drug candidates. NLP\\nmodels have also made great impacts in COVID-19 vaccine\\ndiscovery through protein interaction prediction, molecular\\nreaction modelling [212]. In addition, great opportunities\\nfor NLP can also be found in clinical design, regulatory\\ndecision-making, and pharmacovigilance [213]. These appli-\\ncations would signiﬁcantly reduce the time and cost of drug\\ndevelopment for COVID-19.\\nB. Mental health\\nThe mental health issues have received widespread and\\ncontinuously increasing attention for many years. Specially,\\nthe World Health Organization (WHO) claimed that the pan-\\ndemic and the resulting lockdowns, economic security, fear\\nand uncertainty would further cause devastating impacts on\\npeople’s mental health the world over in the past several'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='demic and the resulting lockdowns, economic security, fear\\nand uncertainty would further cause devastating impacts on\\npeople’s mental health the world over in the past several\\nyears [214]. NLP-driven smart healthcare has great value in\\npredicting/diagnosing and treating mental health conditions.\\nNLP techniques have been applied to early predict or\\nidentify/screen various mental disorders, such as psychiatric\\nillness [215], late-life depression [216], severe mental illness\\n(schizophrenia, schizoaffective disorder and bipolar disorder)\\n[217]. In addition, some works have shown that NLP tech-\\nniques can predict risk-taking behaviours (e.g., suicide) with\\ngood discrimination [218], [219] so that early interventions\\ncan be taken to save lives. The data collected for such\\nanalysis may include text data such as social media posts,\\nscreening surveys, EHRs [220], and also speech data come\\nfrom narrative interviews [221], etc.\\nNLP techniques could also (automatically) provide effective'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='screening surveys, EHRs [220], and also speech data come\\nfrom narrative interviews [221], etc.\\nNLP techniques could also (automatically) provide effective\\npsychotherapeutic interventions through web-based psycho-\\neducational interventions, online counseling, etc., to augment\\ntherapist-based mental health interventions, showing potential\\nfuture opportunities for their integration into online men-\\ntal health tools [222]. For example, the insights of [223]\\ncould help improve counselor training and generate real-time\\ncounseling quality monitoring and answer suggestion support\\ntools. In addition, several mental health related areas that may\\nbeneﬁt from NLP techniques, including characterizing and\\nunderstanding mental disorders, measuring health outcomes,\\nstudying of social and occupational functioning, etc, were\\nshown in [219]. Speciﬁcally, [224] showed that the older\\nwould respond better to digital assistants employing a socially-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='studying of social and occupational functioning, etc, were\\nshown in [219]. Speciﬁcally, [224] showed that the older\\nwould respond better to digital assistants employing a socially-\\noriented interaction style rather than the one with a task-\\noriented style, which is promising to promote mental health\\nin older adults by providing social interaction and company.\\nV. L IMITATIONS AND OUTLOOK\\nAlthough recent advancements in deep learning and neural\\nNLP have brought extraordinary enhancement to smart health-\\ncare, there are still some limitations that current methods have\\nyet to overcome.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ZHOU et al.: NATURAL LANGUAGE PROCESSING FOR SMART HEALTHCARE 9\\nTABLE II\\nAPPLICATIONS DRIVEN BY NLP IN ALL SMART HEALTHCARE SCENARIOS .\\nCategory Sub-Category Representative Applications Related Techniques\\nClinical Practice\\nclinical communication\\nand data collection\\npatient-provider communication [129], [130] machine translation\\nclinical documentation [131]–[133] speech recognition\\nclinical decision support\\nbuild QA-based clinical decision support systems [84], [134], [135] information extraction\\nbuild clinical decision support systems with extracted information: family\\nhistory information [136], entities and relations [137], [138], treatment and\\nprognosis data [139], clinical data concepts and features [140], causal relations\\n[109], [110]\\nquestion answering\\nhealthcare quality control: assess clinical procedures [141], [142], warning of\\nADE [143], disease symptoms [144], [145], and outcome-related causal effects\\n[146]\\ninformation extraction, causal in-\\nference'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ADE [143], disease symptoms [144], [145], and outcome-related causal effects\\n[146]\\ninformation extraction, causal in-\\nference\\nprovide supporting evidence for decisions under evidence-based fashion [108],\\n[147]–[150]\\ninformation retrieval, causal infer-\\nence\\nHospital Management\\nmedical resource allocation\\npatient triage [151], [152] information extraction\\nenable users to communicate and control intelligent systems through virtual\\nassistants [153]–[155], hospital automation systems [156], [157] and collabo-\\nrative robots [158], [159]\\nspeech recognition, natural lan-\\nguage understanding\\npredict and reduce readmission rate [160]–[162] information extraction\\nfree medical staff from routine text writing [70], [163] information extraction\\ndata management manage clinical documents [18], [52], [62], [63], [164] text generation\\nease the HIS retrieval process based on semantic search [76], [165] and question\\nanswering [77]\\ntext classiﬁcation, text summariza-\\ntion, information extraction'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ease the HIS retrieval process based on semantic search [76], [165] and question\\nanswering [77]\\ntext classiﬁcation, text summariza-\\ntion, information extraction\\nservice quality control improve service quality and patient experience [166]–[168] information retrieval, question an-\\nswering\\nPersonal Care\\npersonal health assistants access online medical information [169] information retrieval\\nenable remote healthcare [170] speech recognition\\nassisting the elderly and\\nthe disabled\\ndaily assistance [171] speech recognition, natural lan-\\nguage understanding\\nsocial interaction and company [172], [173] speech recognition, speech synthe-\\nsis\\nassist people with speech impairments [122], [174]–[178], hearing loss [179],\\ndyslexia [180], or neurological disorders [119]–[121]\\nspeech recognition, speech synthe-\\nsis\\nPublic Health\\nhealth knowledge\\npopularization and\\nmedical education\\nacquisition and representation of medical knowledge [86]–[89], [97]–[100],\\n[181]\\nknowledge engineering'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='sis\\nPublic Health\\nhealth knowledge\\npopularization and\\nmedical education\\nacquisition and representation of medical knowledge [86]–[89], [97]–[100],\\n[181]\\nknowledge engineering\\nease the access of medical knowledge [1], [79], [81], [130], [182], [184], [185]question answering, information\\nretrieval, machine translation\\ngenerate medical case-based questions [186] question generation\\nconstruct simpliﬁed summaries [61] text summarization\\npopulation screening identify target populations [188] information extraction\\nanalyse of healthcare questionnaire and surveys [189] information extraction\\nDrug Development\\ndrug discovery map the interactions between diseases, chemical compounds, and biomacro-\\nmolecules, predict molecular properties, and design novel molecules [190]\\ninformation extraction, information\\nretrieval, knowledge engineering\\npreclinical research drug screening [191]–[193] information extraction\\npredict adverse drug reactions: side effect prediction [194], and toxicity'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='retrieval, knowledge engineering\\npreclinical research drug screening [191]–[193] information extraction\\npredict adverse drug reactions: side effect prediction [194], and toxicity\\nprediction [195], [196]\\ninformation extraction\\nclinical research\\nclinical trial design [110] information extraction, causal in-\\nference\\npatient recruitment [197]–[199] information extraction\\nclinical trial analytics [200] information extraction\\ndrug review and safety\\nmonitoring\\nadverse drug events discovery and drug safety monitoring [201]–[203] information extraction\\nUnderstanding human language . Although substantial\\nefforts have been made to enable natural language understand-\\ning, the ﬂexibility of human language still makes full under-\\nstanding difﬁcult, especially when ambiguity in biomedical\\ntexts is encountered. Misunderstanding could lead to inaccu-\\nrate actions taken by robots, useless information returned by\\nengines, and even wrong decisions made by decision support'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='texts is encountered. Misunderstanding could lead to inaccu-\\nrate actions taken by robots, useless information returned by\\nengines, and even wrong decisions made by decision support\\nsystems, leading to economic loss, time wasting, and even\\nmore serious consequences.\\nInterpretability. Although applications that rely on neural\\nNLP to extract features and make decisions show excellent\\nperformance in real tasks, they are usually challenged by\\nusers due to their weakness in interpretability. Interpretability\\nis essential for smart healthcare applications, especially in\\nclinical scenarios that require quality assurance in cases of\\nlow conﬁdence. One of the major interpretability issues is\\nthat the learned features are usually not understood by hu-\\nmans. In addition, when tuning pre-trained language models\\nto downstream tasks, no enough intuitions on data for ﬁne-\\ntuning or types of applications can be given to guarantee good\\nperformance. Although efforts have been made to achieve'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='to downstream tasks, no enough intuitions on data for ﬁne-\\ntuning or types of applications can be given to guarantee good\\nperformance. Although efforts have been made to achieve\\ninterpretable NLP-driven applications, existing theories and\\nmethodologies are still not convincing and acceptable for\\nmany healthcare researchers and institutions. Before the inter-\\npretability issue is fully explored, the role of decision support\\nsystems in clinical practice can only be auxiliary from the\\nperspectives of medical ethics and practical application.\\nImplementation. There are still many issues concerning the\\nimplementation of NLP-driven applications in smart health-\\ncare. With the development of neural NLP, large deep neu-\\nral networks (e.g., pre-trained language models) have been\\nquickly migrated to smart healthcare. What followed are the\\nincreased requirements in computing power and training cost,\\nand the concerns about the reliability of neural NLP systems.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='quickly migrated to smart healthcare. What followed are the\\nincreased requirements in computing power and training cost,\\nand the concerns about the reliability of neural NLP systems.\\nPatient privacy also prevents these models from achieving\\nmore prominent effects in smart healthcare for further practice.\\nThe consideration of medical ethics when applying such\\nsystems makes practical implementation more difﬁcult.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='10 IEEE REVIEWS IN BIOMEDICAL ENGINEERING\\nIn addition to tackling the aforementioned limitations, there\\nare some other directions to enhance existing NLP systems for\\nsmart healthcare.\\nCombining multiple NLP techniques . One direction to\\nenhance existing NLP systems can be the combination of\\nmultiple NLP techniques. For example, text generation can\\nwork as a data augmentation method for achieving comparable\\nresults in many applications with limited original data, such\\nas training QA systems [65], [85] and other clinically relevant\\ntasks [225], [226]. Through automatic question generation,\\nquestionnaires and surveys for population screening can be\\ngenerated from EHRs, which may outperform handcrafted\\nones. Machine translation has also proven beneﬁcial for var-\\nious text-based tasks by increasing the availability of mul-\\ntilingual healthcare information [227]–[229], implying the\\npossibility of improving the performance of current CDS'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ious text-based tasks by increasing the availability of mul-\\ntilingual healthcare information [227]–[229], implying the\\npossibility of improving the performance of current CDS\\nsystems. In addition, exploration of general knowledge and\\ndomain knowledge in the ﬁeld of NLP for smart healthcare\\ndeserves further attention and veriﬁcation.\\nEnd-to-end applications. Current NLP driven applications\\nfor smart healthcare usually focus on dealing with tasks step\\nby step and do not fully explore the feature extraction capa-\\nbility of advanced neural NLP for complex smart healthcare\\ntasks. A deeper integration of NLP techniques and healthcare\\napplications in an end-to-end manner can map the inputs\\nand outputs directly, signiﬁcantly simplify traditional pipelines\\nfor complex applications, eliminate the biases of intermediate\\ncomponents, and therefore achieve better performance. Taking\\npopulation screening as an example, although NLP has been\\napplied to identify populations and analyse screening test'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='components, and therefore achieve better performance. Taking\\npopulation screening as an example, although NLP has been\\napplied to identify populations and analyse screening test\\nresults in traditional screening procedures, NLP techniques can\\nbe further applied to build end-to-end population screening\\nsystems, with which the correlations between populations\\nand optimal actions can be found to improve the screening\\nperformance and the quality of healthcare. Another example\\nwould be reducing the readmission rate. As mentioned before,\\nsome works have revealed that NLP has the ability to predict\\npatient readmission, but further studies on providing appropri-\\nate interventions to reduce the readmission rate are not fully\\nconducted. We look forward to studies that integrate the two\\nparts to reveal every possibility for readmission rate reducing.\\nFew-shot learning and incorporating domain knowledge.\\nBy exploiting the learning capability of neural networks and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='parts to reveal every possibility for readmission rate reducing.\\nFew-shot learning and incorporating domain knowledge.\\nBy exploiting the learning capability of neural networks and\\nlarge available corpora, neural NLP has shown powerful ability\\nin learning language representations. However, for downstream\\ntasks or smart healthcare applications, there is still a long\\nway for NLP to go. Taking clinical decision support as an\\nexample, there are a lot of rare diseases with only a small\\nnumber of observations available for training a clinical deci-\\nsion support system to distinguish rare diseases from common\\ndiseases. This is a quite challenging task, especially when\\nthere are similar outcomes among some rare diseases and\\ncommon diseases. In addition, high-quality labelled data are\\nundoubtedly essential to guarantee task accuracy in developing\\npractical applications for smart healthcare. However, quality-\\ncontrolled annotation not only requires a large amount of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='undoubtedly essential to guarantee task accuracy in developing\\npractical applications for smart healthcare. However, quality-\\ncontrolled annotation not only requires a large amount of\\ncost, but is also challenging due to the bias of experts’ level\\nof expertise. Therefore, even with well-learned pre-trained\\nlanguage models, few-shot learning algorithms and domain\\nknowledge are expected to be applied so that the ﬁne-tuned\\nmodels would be effective in learning from few rare disease\\nobservations or limited high-quality labelled data.\\nIncorporating multimodal and longitudinal data. Finally,\\nwe also anticipate future intelligent systems to utilize all avail-\\nable AI techniques, not only NLP, for practical applications\\nwith high accuracy and reliability. The past few years have\\nwitnessed the dominance of data-driven approaches in many\\napplications across various ﬁelds. NLP, computer vision, and\\nother machine learning algorithms can be applied to analyse'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='witnessed the dominance of data-driven approaches in many\\napplications across various ﬁelds. NLP, computer vision, and\\nother machine learning algorithms can be applied to analyse\\nmedical text, medical images, electronic recordings (e.g., heart\\nsound), sensors data, laboratory results, and even genetic\\ninformation. With multimodal learning, useful information\\nextracted from these modalities can be combined together to\\nperfectly ﬁt the need for a complete and accurate analysis\\nof available healthcare data and patients’ health status. In\\naddition, all of these data and clinical events can be longi-\\ntudinal, where time series analysis can be applied to extract\\nlong-term dependencies and improve health care delivery. By\\ncombining these techniques to analyse multimodal and lon-\\ngitudinal data, future intelligent systems would become more\\npowerful and reliable for patients, physicians, and healthcare\\ninstitutions for applications such as 24/7 health monitoring,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='gitudinal data, future intelligent systems would become more\\npowerful and reliable for patients, physicians, and healthcare\\ninstitutions for applications such as 24/7 health monitoring,\\nchronic-condition management, healthy lifestyle promotion,\\nand precision medicine.\\nVI. C ONCLUSION\\nIn the context of smart healthcare, NLP takes text or speech\\nas the input in various scenarios involving humans and ma-\\nchines, and realizes the functions of analysing and understand-\\ning human language. In this paper, we review existing studies\\nconcerning NLP for smart healthcare from the perspectives\\nof technique and application. We elaborate on different NLP\\napproaches and the NLP pipeline for smart healthcare from the\\ntechnical point of view. Table I provides the comparisons of\\ndifferent NLP approaches and their representative algorithms.\\nVarious text-oriented and speech-oriented NLP tasks are elab-\\norated to conclude existing methodologies for tackling such'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='different NLP approaches and their representative algorithms.\\nVarious text-oriented and speech-oriented NLP tasks are elab-\\norated to conclude existing methodologies for tackling such\\ntasks. By introducing smart healthcare applications employing\\nNLP techniques in various smart healthcare scenarios (in-\\ncluding clinical practice, hospital management, personal care,\\npublic health, and drug development), we show the strength\\nand possibility of NLP for delivering smart healthcare. Table II\\nprovides a detailed list of representative applications in smart\\nhealthcare and their related NLP techniques. We further dis-\\ncuss two speciﬁc medical issues, i.e., COVID-19 pandemic and\\nmental health, in which NLP-driven smart healthcare plays an\\nimportant role. After that, we discuss the limitations of current\\nworks across understanding human language, interpretability,\\nand implementation of NLP systems for smart healthcare.\\nFinally, we identify several directions for future works, notably'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='works across understanding human language, interpretability,\\nand implementation of NLP systems for smart healthcare.\\nFinally, we identify several directions for future works, notably\\ncombining multiple NLP techniques, developing end-to-end\\napplications, few-shot learning, and incorporating multimodal\\nand longitudinal data.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ZHOU et al.: NATURAL LANGUAGE PROCESSING FOR SMART HEALTHCARE 11\\nREFERENCES\\n[1] S. Tian, W. Yang, J. M. L. Grange, P. Wang, W. Huang, and Z. Ye,\\n“Smart healthcare: Making medical care more intelligent,” Global\\nHealth Journal, vol. 3, no. 3, pp. 62–65, Sep. 2019.\\n[2] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent trends in\\ndeep learning based natural language processing,” arXiv:1708.02709\\n[cs], Nov. 2018.\\n[3] Y . Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural prob-\\nabilistic language model,” Journal of Machine Learning Research ,\\nvol. 3, no. Feb, pp. 1137–1155, 2003.\\n[4] J. Crim, R. McDonald, and F. Pereira, “Automatically annotating\\ndocuments with normalized gene lists,” BMC Bioinformatics , vol. 6,\\nno. 1, p. S13, May 2005.\\n[5] J. Vilares, M. A. Alonso, and M. Vilares, “Extraction of complex\\nindex terms in non-English IR: A shallow parsing based approach,”\\nInformation Processing & Management, vol. 44, no. 4, pp. 1517–1537,\\nJul. 2008.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='index terms in non-English IR: A shallow parsing based approach,”\\nInformation Processing & Management, vol. 44, no. 4, pp. 1517–1537,\\nJul. 2008.\\n[6] L. Chiticariu, Y . Li, and F. R. Reiss, “Rule-based information extraction\\nis dead! Long live rule-based information extraction systems!” in\\nProceedings of the 2013 Conference on Empirical Methods in Natural\\nLanguage Processing . Seattle, Washington, USA: Association for\\nComputational Linguistics, Oct. 2013, pp. 827–832.\\n[7] N. Kang, B. Singh, Z. Afzal, E. M. van Mulligen, and J. Kors,\\n“Using rule-based natural language processing to improve disease\\nnormalization in biomedical text,” Journal of the American Medical\\nInformatics Association : JAMIA , vol. 20, Oct. 2012.\\n[8] W.-H. Weng, K. B. Wagholikar, A. T. McCray, P. Szolovits, and H. C.\\nChueh, “Medical subdomain classiﬁcation of clinical notes using a\\nmachine learning-based natural language processing approach,” BMC\\nMedical Informatics and Decision Making , vol. 17, p. 155, Dec. 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='machine learning-based natural language processing approach,” BMC\\nMedical Informatics and Decision Making , vol. 17, p. 155, Dec. 2017.\\n[9] D. Dessi, R. Helaoui, V . Kumar, D. R. Recupero, and D. Riboni, “TF-\\nIDF vs word embeddings for morbidity identiﬁcation in clinical notes:\\nAn initial study,” arXiv:2105.09632 [cs], Mar. 2020.\\n[10] O. Ozyegen, D. Kabe, and M. Cevik, “Word-level text highlighting\\nof medical texts forTelehealth services,” arXiv:2105.10400 [cs] , May\\n2021.\\n[11] M. Rahimian, J. L. Warner, S. K. Jain, R. B. Davis, J. A. Zerillo, and\\nR. M. Joyce, “Signiﬁcant and distinctive n-grams in oncology notes:\\nA text-mining method to analyze the effect of OpenNotes on clinical\\ndocumentation,” JCO Clinical Cancer Informatics, no. 3, pp. 1–9, Dec.\\n2019.\\n[12] A. Yazdani, R. Safdari, A. Golkar, and S. Rostam Niakan Kalhori,\\n“Words prediction based on N-gram model for free-text entry in\\nelectronic health records,” Health Information Science and Systems ,\\nvol. 7, Feb. 2019.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='“Words prediction based on N-gram model for free-text entry in\\nelectronic health records,” Health Information Science and Systems ,\\nvol. 7, Feb. 2019.\\n[13] V . Yip, M. Mete, U. Topaloglu, and S. Kockara, “Concept discovery for\\npathology reports using an N-gram model,” Summit on Translational\\nBioinformatics, vol. 2010, pp. 43–47, Mar. 2010.\\n[14] M. Beeksma, S. Verberne, A. van den Bosch, E. Das, I. Hendrickx,\\nand S. Groenewoud, “Predicting life expectancy with a long short-term\\nmemory recurrent neural network using electronic medical records,”\\nBMC Medical Informatics and Decision Making , vol. 19, no. 1, p. 36,\\nFeb. 2019.\\n[15] A. N. Jagannatha and H. Yu, “Bidirectional RNN for medical event\\ndetection in electronic health records,” in Proceedings of the 2016\\nConference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies . San\\nDiego, California: Association for Computational Linguistics, Jun.\\n2016, pp. 473–482.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='Computational Linguistics: Human Language Technologies . San\\nDiego, California: Association for Computational Linguistics, Jun.\\n2016, pp. 473–482.\\n[16] C. Liu, H. Sun, N. Du, S. Tan, H. Fei, W. Fan, T. Yang, H. Wu, Y . Li,\\nand C. Zhang, “Augmented LSTM framework to construct medical\\nself-diagnosis android,” in 2016 IEEE 16th International Conference\\non Data Mining (ICDM) , Dec. 2016, pp. 251–260.\\n[17] Y .-S. Zhao, K.-L. Zhang, H.-C. Ma, and K. Li, “Leveraging text\\nskeleton for de-identiﬁcation of electronic medical records,” BMC\\nMedical Informatics and Decision Making , vol. 18, no. Suppl 1, p. 18,\\nMar. 2018.\\n[18] M. Hughes, I. Li, S. Kotoulas, and T. Suzumura, “Medical text\\nclassiﬁcation using convolutional neural networks,” Studies in Health\\nTechnology and Informatics, vol. 235, pp. 246–250, 2017.\\n[19] X. Li, H. Wang, H. He, J. Du, J. Chen, and J. Wu, “Intelligent diagnosis\\nwith chinese electronic medical records based on convolutional neural'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='[19] X. Li, H. Wang, H. He, J. Du, J. Chen, and J. Wu, “Intelligent diagnosis\\nwith chinese electronic medical records based on convolutional neural\\nnetworks,” BMC Bioinformatics, vol. 20, no. 1, p. 62, Feb. 2019.\\n[20] Y . Li, B. Qian, X. Zhang, and H. Liu, “Graph neural network-based\\ndiagnosis prediction,” Big Data, vol. 8, no. 5, pp. 379–390, Oct. 2020.\\n[21] Z. Sun, H. Yin, H. Chen, T. Chen, L. Cui, and F. Yang, “Disease\\nprediction via graph neural networks,” IEEE Journal of Biomedical\\nand Health Informatics , vol. 25, no. 3, pp. 818–826, Mar. 2021.\\n[22] T. Wu, Y . Wang, Y . Wang, E. Zhao, and Y . Yuan, “Leveraging graph-\\nbased hierarchical medical entity embedding for healthcare applica-\\ntions,” Scientiﬁc Reports, vol. 11, no. 1, p. 5858, Mar. 2021.\\n[23] T. Mayer, E. Cabrio, and S. Villata, “Transformer-based argument\\nmining for healthcare applications,” in ECAI 2020 - 24th European\\nConference on Artiﬁcial Intelligence, Santiago de Compostela / Online,\\nSpain, Aug. 2020.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='mining for healthcare applications,” in ECAI 2020 - 24th European\\nConference on Artiﬁcial Intelligence, Santiago de Compostela / Online,\\nSpain, Aug. 2020.\\n[24] D. Zhang, J. Thadajarassiri, C. Sen, and E. Rundensteiner, “Time-\\naware transformer-based network for clinical notes series prediction,”\\nin Machine Learning for Healthcare Conference . PMLR, Sep. 2020,\\npp. 566–588.\\n[25] S. Tokala, V . Gambhir, and A. Mukherjee, “Deep learning for social\\nmedia health text classiﬁcation,” in Proceedings of the 2018 EMNLP\\nWorkshop SMM4H: The 3rd Social Media Mining for Health Applica-\\ntions Workshop & Shared Task . Brussels, Belgium: Association for\\nComputational Linguistics, Oct. 2018, pp. 61–64.\\n[26] E. Choi, M. T. Bahadori, J. A. Kulas, A. Schuetz, W. F. Stewart, and\\nJ. Sun, “RETAIN: An interpretable predictive model for healthcare\\nusing reverse time attention mechanism,” arXiv:1608.05745 [cs], Feb.\\n2017.\\n[27] J. Chu, W. Dong, K. He, H. Duan, and Z. Huang, “Using neural'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='using reverse time attention mechanism,” arXiv:1608.05745 [cs], Feb.\\n2017.\\n[27] J. Chu, W. Dong, K. He, H. Duan, and Z. Huang, “Using neural\\nattention networks to detect adverse medical events from electronic\\nhealth records,” Journal of Biomedical Informatics , vol. 87, pp. 118–\\n130, Nov. 2018.\\n[28] P. Chakraborty, F. Wang, J. Hu, and D. Sow, “Explicit-blurred\\nmemory network for analyzing patient electronic health records,”\\narXiv:1911.06472 [cs, stat] , Jul. 2020.\\n[29] J. Song, Y . Wang, S. Tang, Y . Zhang, Z. Chen, Z. Zhang, T. Zhang,\\nand F. Wu, “Local–Global memory neural network for medication\\nprediction,” IEEE Transactions on Neural Networks and Learning\\nSystems, vol. 32, no. 4, pp. 1723–1736, Apr. 2021.\\n[30] H.-J. Yoon, J. Gounley, M. T. Young, and G. Tourassi, “Information ex-\\ntraction from cancer pathology reports with graph convolution networks\\nfor natural language texts,” in 2019 IEEE International Conference on\\nBig Data (Big Data) , Dec. 2019, pp. 4561–4564.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='traction from cancer pathology reports with graph convolution networks\\nfor natural language texts,” in 2019 IEEE International Conference on\\nBig Data (Big Data) , Dec. 2019, pp. 4561–4564.\\n[31] R. Cai, B. Zhu, L. Ji, T. Hao, J. Yan, and W. Liu, “An CNN-LSTM\\nattention approach to understanding user query intent from online\\nhealth communities,” in 2017 IEEE International Conference on Data\\nMining Workshops (ICDMW), Nov. 2017, pp. 430–437.\\n[32] B. Tang, X. Wang, J. Yan, and Q. Chen, “Entity recognition in chinese\\nclinical text using attention-based CNN-LSTM-CRF,” BMC Medical\\nInformatics and Decision Making , vol. 19, no. 3, p. 74, Apr. 2019.\\n[33] E. Choi, Z. Xu, Y . Li, M. W. Dusenberry, G. Flores, Y . Xue, and A. M.\\nDai, “Learning the graphical structure of electronic health records with\\ngraph convolutional transformer,” arXiv:1906.04716 [cs, stat] , Jan.\\n2020.\\n[34] J. Wang, X. Chen, Y . Zhang, Y . Zhang, J. Wen, H. Lin, Z. Yang, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='graph convolutional transformer,” arXiv:1906.04716 [cs, stat] , Jan.\\n2020.\\n[34] J. Wang, X. Chen, Y . Zhang, Y . Zhang, J. Wen, H. Lin, Z. Yang, and\\nX. Wang, “Document-level biomedical relation extraction using graph\\nconvolutional network and multihead attention: Algorithm development\\nand validation,” JMIR Medical Informatics , vol. 8, no. 7, p. e17638,\\nJul. 2020.\\n[35] X. Qiu, T. Sun, Y . Xu, Y . Shao, N. Dai, and X. Huang, “Pre-trained\\nmodels for natural language processing: A survey,” arXiv:2003.08271\\n[cs], Apr. 2020.\\n[36] A. L. Beam, B. Kompa, A. Schmaltz, I. Fried, G. Weber, N. P. Palmer,\\nX. Shi, T. Cai, and I. S. Kohane, “Clinical concept embeddings learned\\nfrom massive sources of multimodal medical data,” arXiv:1804.01486\\n[cs, stat], Aug. 2019.\\n[37] X. Cai, J. Gao, K. Y . Ngiam, B. C. Ooi, Y . Zhang, and X. Yuan, “Med-\\nical concept embedding with time-aware attention,” in Proceedings of\\nthe 27th International Joint Conference on Artiﬁcial Intelligence , ser.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ical concept embedding with time-aware attention,” in Proceedings of\\nthe 27th International Joint Conference on Artiﬁcial Intelligence , ser.\\nIJCAI’18. Stockholm, Sweden: AAAI Press, Jul. 2018, pp. 3984–\\n3990.\\n[38] M. Kholghi, L. De Vine, L. Sitbon, G. Zuccon, and A. Nguyen, “The\\nbeneﬁts of word embeddings features for active learning in clinical\\ninformation extraction,” in Proceedings of the Australasian Language\\nTechnology Association Workshop 2016 , Melbourne, Australia, Dec.\\n2016, pp. 25–34.\\n[39] Y . Zhang, Q. Chen, Z. Yang, H. Lin, and Z. Lu, “BioWordVec,\\nimproving biomedical word embeddings with subword information and\\nMeSH,” Scientiﬁc Data, vol. 6, no. 1, p. 52, May 2019.\\n[40] S. Dubois, N. Romano, D. C. Kale, N. Shah, and K. Jung, “Effective\\nrepresentations of clinical notes,” arXiv:1705.07025 [cs, stat] , Aug.\\n2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='12 IEEE REVIEWS IN BIOMEDICAL ENGINEERING\\n[41] Q. Jin, B. Dhingra, W. W. Cohen, and X. Lu, “Probing biomedical\\nembeddings from language models,”arXiv:1904.02181 [cs], Apr. 2019.\\n[42] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang,\\n“BioBERT: A pre-trained biomedical language representation model\\nfor biomedical text mining,” Bioinformatics, vol. 36, no. 4, pp. 1234–\\n1240, Feb. 2020.\\n[43] K. Huang, J. Altosaar, and R. Ranganath, “ClinicalBERT: Modeling\\nclinical notes and predicting hospital readmission,” arXiv:1904.05342\\n[cs], Nov. 2020.\\n[44] L. Rasmy, Y . Xiang, Z. Xie, C. Tao, and D. Zhi, “Med-BERT: Pre-\\ntrained contextualized embeddings on large-scale structured electronic\\nhealth records for disease prediction,” npj Digital Medicine , vol. 4,\\nno. 1, pp. 1–13, May 2021.\\n[45] Y . Li, S. Rao, J. R. A. Solares, A. Hassaine, R. Ramakrishnan,\\nD. Canoy, Y . Zhu, K. Rahimi, and G. Salimi-Khorshidi, “BEHRT:\\nTransformer for electronic health records,” Scientiﬁc Reports, vol. 10,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='D. Canoy, Y . Zhu, K. Rahimi, and G. Salimi-Khorshidi, “BEHRT:\\nTransformer for electronic health records,” Scientiﬁc Reports, vol. 10,\\nno. 1, p. 7155, Apr. 2020.\\n[46] E. T. R. Schneider, J. V . A. de Souza, Y . B. Gumiel, C. Moro, and E. C.\\nParaiso, “A GPT-2 language model for biomedical texts in portuguese,”\\nin 2021 IEEE 34th International Symposium on Computer-Based\\nMedical Systems (CBMS) , Jun. 2021, pp. 474–479.\\n[47] A. Akkasi, E. Varo ˘glu, and N. Dimililer, “ChemTok: A new rule based\\ntokenizer for chemical named entity recognition,” BioMed Research\\nInternational, vol. 2016, 2016.\\n[48] H.-J. Dai, P.-T. Lai, Y .-C. Chang, and R. T.-H. Tsai, “Enhancing of\\nchemical compound and drug name recognition using representative\\ntag scheme and ﬁne-grained tokenization,”Journal of Cheminformatics,\\nvol. 7, no. Suppl 1 Text mining for chemistry and the CHEMDNER\\ntrack, p. S14, 2015.\\n[49] H. Liu, T. Christiansen, W. A. Baumgartner, and K. Verspoor, “Bi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='vol. 7, no. Suppl 1 Text mining for chemistry and the CHEMDNER\\ntrack, p. S14, 2015.\\n[49] H. Liu, T. Christiansen, W. A. Baumgartner, and K. Verspoor, “Bi-\\noLemmatizer: A lemmatization tool for morphological processing of\\nbiomedical text,” Journal of Biomedical Semantics , vol. 3, p. 3, Apr.\\n2012.\\n[50] Y . Qiao, C. Xiong, Z. Liu, and Z. Liu, “Understanding the Behaviors\\nof BERT in Ranking,” arXiv:1904.07531 [cs], Apr. 2019.\\n[51] X. Liu, F. Zhang, Z. Hou, Z. Wang, L. Mian, J. Zhang, and J. Tang,\\n“Self-supervised learning: Generative or contrastive,” IEEE Transac-\\ntions on Knowledge and Data Engineering , pp. 1–1, 2021.\\n[52] Y . Wang, S. Sohn, S. Liu, F. Shen, L. Wang, E. J. Atkinson, S. Amin,\\nand H. Liu, “A clinical text classiﬁcation paradigm using weak supervi-\\nsion and deep representation,” BMC Medical Informatics and Decision\\nMaking, vol. 19, no. 1, p. 1, Jan. 2019.\\n[53] S. Hassanpour and C. P. Langlotz, “Information extraction from multi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='sion and deep representation,” BMC Medical Informatics and Decision\\nMaking, vol. 19, no. 1, p. 1, Jan. 2019.\\n[53] S. Hassanpour and C. P. Langlotz, “Information extraction from multi-\\ninstitutional radiology reports,” Artiﬁcial intelligence in medicine ,\\nvol. 66, pp. 29–39, Jan. 2016.\\n[54] N. Perera, M. Dehmer, and F. Emmert-Streib, “Named entity recog-\\nnition and relation detection for biomedical information extraction,”\\nFrontiers in Cell and Developmental Biology , vol. 8, p. 673, Aug.\\n2020.\\n[55] A. Thillaisundaram and T. Togia, “Biomedical relation extraction\\nwith pre-trained language representations and minimal task-speciﬁc\\narchitecture,” in Proceedings of The 5th Workshop on BioNLP Open\\nShared Tasks. Hong Kong, China: Association for Computational\\nLinguistics, Nov. 2019, pp. 84–89.\\n[56] S. ˇZitnik, M. ˇZitnik, B. Zupan, and M. Bajec, “Sieve-based relation\\nextraction of gene regulatory networks from biological literature,”BMC'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='Linguistics, Nov. 2019, pp. 84–89.\\n[56] S. ˇZitnik, M. ˇZitnik, B. Zupan, and M. Bajec, “Sieve-based relation\\nextraction of gene regulatory networks from biological literature,”BMC\\nBioinformatics, vol. 16, no. Suppl 16, p. S1, Oct. 2015.\\n[57] P. Jindal and D. Roth, “Extraction of events and temporal expressions\\nfrom clinical narratives,” Journal of Biomedical Informatics , vol. 46,\\npp. S13–S19, Dec. 2013.\\n[58] A. Garg and M. Agarwal, “Machine translation: A literature review,”\\narXiv:1901.01122 [cs], Dec. 2018.\\n[59] A. B ´erard, Z. M. Kim, V . Nikoulina, E. L. Park, and M. Gall ´e, “A\\nmultilingual neural machine translation model for biomedical data,” in\\nProceedings of the 1st Workshop on NLP for COVID-19 (Part 2) at\\nEMNLP 2020 . Online: Association for Computational Linguistics,\\nDec. 2020.\\n[60] K. Kirchhoff, A. M. Turner, A. Axelrod, and F. Saavedra, “Application\\nof statistical machine translation to public health information: A feasi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='Dec. 2020.\\n[60] K. Kirchhoff, A. M. Turner, A. Axelrod, and F. Saavedra, “Application\\nof statistical machine translation to public health information: A feasi-\\nbility study,” Journal of the American Medical Informatics Association\\n: JAMIA, vol. 18, no. 4, pp. 473–478, 2011.\\n[61] M. Afzal, F. Alam, K. M. Malik, and G. M. Malik, “Clinical Con-\\ntext–Aware biomedical text summarization using deep neural network:\\nModel development and validation,” Journal of Medical Internet Re-\\nsearch, vol. 22, no. 10, p. e19810, Oct. 2020.\\n[62] J. Lopez, “Automatic summarization of medical conversations, a\\nreview,” in TALN-RECITAL 2019-PFIA 2019 . Toulouse, France:\\nATALA, Jul. 2019, pp. 487–498.\\n[63] G. Manas, V . Aribandi, U. Kursuncu, A. Alambo, V . L. Shalin,\\nK. Thirunarayan, J. Beich, M. Narasimhan, and A. Sheth, “Knowledge-\\ninfused abstractive summarization of clinical diagnostic interviews:\\nFramework development study,” JMIR Mental Health , vol. 8, no. 5,\\np. e20865, May 2021.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='infused abstractive summarization of clinical diagnostic interviews:\\nFramework development study,” JMIR Mental Health , vol. 8, no. 5,\\np. e20865, May 2021.\\n[64] I. Pistol, D. Trandab ˘at,, and M. R ˘aschip, “Medi-test: Generating tests\\nfrom medical reference texts,” Data, vol. 3, no. 4, p. 70, Dec. 2018.\\n[65] S. Shen, Y . Li, N. Du, X. Wu, Y . Xie, S. Ge, T. Yang, K. Wang,\\nX. Liang, and W. Fan, “On the generation of medical question-answer\\npairs,” arXiv:1811.00681 [cs], Dec. 2019.\\n[66] W. Wang, T. Hao, and W. Liu, “Automatic question generation for\\nlearning evaluation in medicine,” in Advances in Web Based Learning\\n– ICWL 2007 , ser. Lecture Notes in Computer Science, H. Leung,\\nF. Li, R. Lau, and Q. Li, Eds. Berlin, Heidelberg: Springer, 2008, pp.\\n242–251.\\n[67] D. Li, Z. Ren, P. Ren, Z. Chen, M. Fan, J. Ma, and M. de Rijke, “Semi-\\nsupervised variational reasoning for medical dialogue generation,”\\nProceedings of the 44th International ACM SIGIR Conference on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='supervised variational reasoning for medical dialogue generation,”\\nProceedings of the 44th International ACM SIGIR Conference on\\nResearch and Development in Information Retrieval , pp. 544–554, Jul.\\n2021.\\n[68] S. Lin, P. Zhou, X. Liang, J. Tang, R. Zhao, Z. Chen, and L. Lin,\\n“Graph-evolving meta-learning for low-resource medical dialogue gen-\\neration,” arXiv:2012.11988 [cs], Dec. 2020.\\n[69] W. Yang, G. Zeng, B. Tan, Z. Ju, S. Chakravorty, X. He, S. Chen,\\nX. Yang, Q. Wu, Z. Yu, E. Xing, and P. Xie, “On the generation of\\nmedical dialogues for COVID-19,” arXiv:2005.05442 [cs], Jun. 2020.\\n[70] S. Pauws, A. Gatt, E. Krahmer, and E. Reiter, “Making effective use of\\nhealthcare data using data-to-text technology,” arXiv:1808.03507 [cs],\\nAug. 2018.\\n[71] V . Kougia, J. Pavlopoulos, and I. Androutsopoulos, “A survey on\\nbiomedical image captioning,” arXiv:1905.13302 [cs], May 2019.\\n[72] Y . Xiong, B. Du, and P. Yan, “Reinforced transformer for medical'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='biomedical image captioning,” arXiv:1905.13302 [cs], May 2019.\\n[72] Y . Xiong, B. Du, and P. Yan, “Reinforced transformer for medical\\nimage captioning,” in Machine Learning in Medical Imaging , ser.\\nLecture Notes in Computer Science, H.-I. Suk, M. Liu, P. Yan, and\\nC. Lian, Eds. Cham: Springer International Publishing, 2019, pp.\\n673–680.\\n[73] X. He, Z. Cai, W. Wei, Y . Zhang, L. Mou, E. Xing, and P. Xie,\\n“Towards visual question answering on pathology images,” inProceed-\\nings of the 59th Annual Meeting of the Association for Computational\\nLinguistics and the 11th International Joint Conference on Natural\\nLanguage Processing (Volume 2: Short Papers) . Online: Association\\nfor Computational Linguistics, Aug. 2021, pp. 708–718.\\n[74] L.-M. Zhan, B. Liu, L. Fan, J. Chen, and X.-M. Wu, “Medical visual\\nquestion answering via conditional reasoning,” in Proceedings of the\\n28th ACM International Conference on Multimedia, ser. MM ’20. New'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='question answering via conditional reasoning,” in Proceedings of the\\n28th ACM International Conference on Multimedia, ser. MM ’20. New\\nYork, NY , USA: Association for Computing Machinery, Oct. 2020, pp.\\n2345–2354.\\n[75] B. Afrae, D. Yousra, A. Imane, B. A. Mohamed, and A. B. Abdel-\\nhakim, “A new visual question answering system for medical images\\ncharacterization,” in Proceedings of the 4th International Conference\\non Smart City Applications , ser. SCA ’19. New York, NY , USA:\\nAssociation for Computing Machinery, Oct. 2019, pp. 1–7.\\n[76] H. Wu, G. Toti, K. I. Morley, Z. M. Ibrahim, A. Folarin, R. Jackson,\\nI. Kartoglu, A. Agrawal, C. Stringer, D. Gale, G. Gorrell, A. Roberts,\\nM. Broadbent, R. Stewart, and R. J. Dobson, “SemEHR: A general-\\npurpose semantic search system to surface semantic data from clinical\\nnotes for tailored care, trial recruitment, and clinical research,” Journal\\nof the American Medical Informatics Association : JAMIA , vol. 25,\\nno. 5, pp. 530–537, Jan. 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='notes for tailored care, trial recruitment, and clinical research,” Journal\\nof the American Medical Informatics Association : JAMIA , vol. 25,\\nno. 5, pp. 530–537, Jan. 2018.\\n[77] J. Gobeill, A. Gaudinat, E. Pasche, D. Vishnyakova, P. Gaudet,\\nA. Bairoch, and P. Ruch, “Deep question answering for protein anno-\\ntation,” Database: The Journal of Biological Databases and Curation ,\\nvol. 2015, Sep. 2015.\\n[78] A. Montazeralghaem, R. Rahimi, and J. Allan, “Relevance ranking\\nbased on query-aware context analysis,” Advances in Information\\nRetrieval, vol. 12035, pp. 446–460, Mar. 2020.\\n[79] B. Xu, H. Lin, Y . Lin, Y . Ma, L. Yang, J. Wang, and Z. Yang,\\n“Improve biomedical information retrieval using modiﬁed learning to\\nrank methods,”IEEE/ACM Transactions on Computational Biology and\\nBioinformatics, vol. 15, no. 6, pp. 1797–1809, Nov. 2018.\\n[80] J. Urbain, O. Frieder, and N. Goharian, “Passage relevance models for\\ngenomics search,” BMC Bioinformatics , vol. 10, no. Suppl 3, p. S3,\\nMar. 2009.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ZHOU et al.: NATURAL LANGUAGE PROCESSING FOR SMART HEALTHCARE 13\\n[81] S. Mohan, N. Fiorini, S. Kim, and Z. Lu, “A fast deep learning\\nmodel for textual relevance in biomedical information retrieval,” in\\nProceedings of the 2018 World Wide Web Conference, ser. WWW ’18.\\nRepublic and Canton of Geneva, CHE: International World Wide Web\\nConferences Steering Committee, Apr. 2018, pp. 77–86.\\n[82] D. Hristovski, D. Dinevski, A. Kastrin, and T. C. Rindﬂesch, “Biomedi-\\ncal question answering using semantic relations,” BMC Bioinformatics,\\nvol. 16, no. 1, p. 6, Jan. 2015.\\n[83] V . Vinod, S. Agrawal, V . Gaurav, P. R, and S. Choudhary, “Multilingual\\nmedical question answering and information retrieval for rural health\\nintelligence access,” arXiv:2106.01251 [cs], Jun. 2021.\\n[84] M. A. H. Zahid, A. Mittal, R. C. Joshi, and G. Atluri, “CLINIQA:\\nA machine intelligence based clinical question answering system,”\\narXiv:1805.05927 [cs], May 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='[84] M. A. H. Zahid, A. Mittal, R. C. Joshi, and G. Atluri, “CLINIQA:\\nA machine intelligence based clinical question answering system,”\\narXiv:1805.05927 [cs], May 2018.\\n[85] P. Wang, T. Shi, and C. K. Reddy, “Text-to-SQL generation for question\\nanswering on electronic medical records,” in Proceedings of The Web\\nConference 2020. Taipei Taiwan: ACM, Apr. 2020, pp. 350–361.\\n[86] Z. Jiang, C. Chi, and Y . Zhan, “Research on medical question an-\\nswering system based on knowledge graph,” IEEE Access, vol. 9, pp.\\n21 094–21 101, 2021.\\n[87] H. Liu, Q. Hu, Y . Zhang, C. Xing, and M. Sheng, “A knowledge-\\nbased health question answering system,” in Smart Health, ser. Lecture\\nNotes in Computer Science, H. Chen, D. D. Zeng, E. Karahanna, and\\nI. Bardhan, Eds. Cham: Springer International Publishing, 2017, pp.\\n286–291.\\n[88] D. Demner-Fushman and J. Lin, “Answering clinical questions with\\nknowledge-based and statistical techniques,” Computational Linguis-\\ntics, vol. 33, no. 1, pp. 63–103, Mar. 2007.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='286–291.\\n[88] D. Demner-Fushman and J. Lin, “Answering clinical questions with\\nknowledge-based and statistical techniques,” Computational Linguis-\\ntics, vol. 33, no. 1, pp. 63–103, Mar. 2007.\\n[89] R. M. Terol, P. Mart ´ınez-Barco, and M. Palomar, “A knowledge based\\nmethod for the medical question answering problem,” Computers in\\nBiology and Medicine , vol. 37, no. 10, pp. 1511–1521, Oct. 2007.\\n[90] Z. Liu, E. Peng, S. Yan, G. Li, and T. Hao, “T-know: A knowledge\\ngraph-based question answering and infor-mation retrieval system for\\ntraditional chinese medicine,” in Proceedings of the 27th International\\nConference on Computational Linguistics: System Demonstrations .\\nSanta Fe, New Mexico: Association for Computational Linguistics,\\nAug. 2018, pp. 15–19.\\n[91] E. Mutabazi, J. Ni, G. Tang, and W. Cao, “A review on medical textual\\nquestion answering systems based on deep learning approaches,”\\nApplied Sciences, vol. 11, no. 12, p. 5456, Jan. 2021.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='[91] E. Mutabazi, J. Ni, G. Tang, and W. Cao, “A review on medical textual\\nquestion answering systems based on deep learning approaches,”\\nApplied Sciences, vol. 11, no. 12, p. 5456, Jan. 2021.\\n[92] H. Shim, D. Lowet, S. Luca, and B. Vanrumste, “Building blocks\\nof a task-oriented dialogue system in the healthcare domain,” in\\nProceedings of the Second Workshop on Natural Language Processing\\nfor Medical Conversations . Online: Association for Computational\\nLinguistics, Jun. 2021, pp. 47–57.\\n[93] Z. Wei, Q. Liu, B. Peng, H. Tou, T. Chen, X. Huang, K.-f. Wong,\\nand X. Dai, “Task-oriented dialogue system for automatic diagnosis,”\\nin Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics (Volume 2: Short Papers) . Melbourne,\\nAustralia: Association for Computational Linguistics, Jul. 2018, pp.\\n201–207.\\n[94] L. Xu, Q. Zhou, K. Gong, X. Liang, J. Tang, and L. Lin, “End-to-end\\nknowledge-routed relational dialogue system for automatic diagnosis,”'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='201–207.\\n[94] L. Xu, Q. Zhou, K. Gong, X. Liang, J. Tang, and L. Lin, “End-to-end\\nknowledge-routed relational dialogue system for automatic diagnosis,”\\narXiv:1901.10623 [cs], Mar. 2019.\\n[95] H. Kawata, K. Ookawara, M. Muta, S. Masuko, and J. Hoshino,\\n“Lifestyle agent: The chat-oriented dialogue system for lifestyle man-\\nagement,” in Entertainment Computing – ICEC 2017 , ser. Lecture\\nNotes in Computer Science, N. Munekata, I. Kunita, and J. Hoshino,\\nEds. Cham: Springer International Publishing, 2017, pp. 396–399.\\n[96] R. Studer, V . R. Benjamins, and D. Fensel, “Knowledge engineering:\\nPrinciples and methods,” Data & Knowledge Engineering , vol. 25,\\nno. 1, pp. 161–197, Mar. 1998.\\n[97] T. Goodwin and S. M. Harabagiu, “Automatic generation of a qualiﬁed\\nmedical knowledge graph and its usage for retrieving patient cohorts\\nfrom electronic medical records,” in 2013 IEEE Seventh International\\nConference on Semantic Computing , Sep. 2013, pp. 363–370.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='medical knowledge graph and its usage for retrieving patient cohorts\\nfrom electronic medical records,” in 2013 IEEE Seventh International\\nConference on Semantic Computing , Sep. 2013, pp. 363–370.\\n[98] A. Rossanez, J. C. dos Reis, R. d. S. Torres, and H. de Ribaupierre,\\n“KGen: A knowledge graph generator from biomedical scientiﬁc\\nliterature,” BMC Medical Informatics and Decision Making , vol. 20,\\nno. 4, p. 314, Dec. 2020.\\n[99] M. Rotmensch, Y . Halpern, A. Tlimat, S. Horng, and D. Sontag,\\n“Learning a health knowledge graph from electronic medical records,”\\nScientiﬁc Reports, vol. 7, no. 1, p. 5994, Jul. 2017.\\n[100] H. Wang, Q. Zhang, and J. Yuan, “Semantically enhanced medical\\ninformation retrieval system: A tensor factorization based approach,”\\nIEEE Access, vol. 5, pp. 7584–7593, 2017.\\n[101] Y . Pan, Q. Chen, W. Peng, X. Wang, B. Hu, X. Liu, J. Chen, and\\nW. Zhou, “MedWriter: Knowledge-aware medical text generation,” in\\nProceedings of the 28th International Conference on Computational'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='W. Zhou, “MedWriter: Knowledge-aware medical text generation,” in\\nProceedings of the 28th International Conference on Computational\\nLinguistics. Barcelona, Spain (Online): International Committee on\\nComputational Linguistics, Dec. 2020, pp. 2363–2368.\\n[102] A. Stoica, T. Kadar, C. Lemnaru, R. Potolea, and M. D ˆıns ¸oreanu,\\n“Intent detection and slot ﬁlling with capsule net architectures for a\\nromanian home assistant,” Sensors, vol. 21, no. 4, p. 1230, Jan. 2021.\\n[103] A. Neuraz, L. C. Llanos, A. Burgun, and S. Rosset, “Natural language\\nunderstanding for task oriented dialog in the biomedical domain in a\\nlow resources context,” arXiv:1811.09417 [cs], Nov. 2018.\\n[104] C. Zhang, N. Du, W. Fan, Y . Li, C.-T. Lu, and P. S. Yu, “Bringing\\nsemantic structures to user intent detection in online medical queries,”\\nin 2017 IEEE International Conference on Big Data (Big Data) , Dec.\\n2017, pp. 1019–1026.\\n[105] I. Giachos, E. C. Papakitsos, and G. Chorozoglou, “Exploring natural'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='in 2017 IEEE International Conference on Big Data (Big Data) , Dec.\\n2017, pp. 1019–1026.\\n[105] I. Giachos, E. C. Papakitsos, and G. Chorozoglou, “Exploring natural\\nlanguage understanding in robotic interfaces,” International Journal of\\nAdvances in Intelligent Informatics, vol. 3, no. 1, pp. 10–19, Mar. 2017.\\n[106] J. Thomason, A. Padmakumar, J. Sinapov, N. Walker, Y . Jiang,\\nH. Yedidsion, J. Hart, P. Stone, and R. J. Mooney, “Improving grounded\\nnatural language understanding through human-robot dialog,” 2019\\nInternational Conference on Robotics and Automation (ICRA) , pp.\\n6934–6941, May 2019.\\n[107] A. Feder, K. A. Keith, E. Manzoor, R. Pryzant, D. Sridhar, Z. Wood-\\nDoughty, J. Eisenstein, J. Grimmer, R. Reichart, M. E. Roberts, B. M.\\nStewart, V . Veitch, and D. Yang, “Causal inference in natural lan-\\nguage processing: Estimation, prediction, interpretation and beyond,”\\narXiv:2109.00725 [cs], Sep. 2021.\\n[108] J. Zeng, M. F. Gensheimer, D. L. Rubin, S. Athey, and R. D. Shachter,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='guage processing: Estimation, prediction, interpretation and beyond,”\\narXiv:2109.00725 [cs], Sep. 2021.\\n[108] J. Zeng, M. F. Gensheimer, D. L. Rubin, S. Athey, and R. D. Shachter,\\n“Uncovering interpretable potential confounders in electronic medical\\nrecords,” medRxiv : the preprint server for health sciences , 2021.\\n[109] S. Doan, E. W. Yang, S. S. Tilak, P. W. Li, D. S. Zisook, and M. Torii,\\n“Extracting health-related causality from twitter messages using natural\\nlanguage processing,” BMC Medical Informatics and Decision Making,\\nvol. 19, no. 3, p. 79, Apr. 2019.\\n[110] G. Nordon, G. Koren, V . Shalev, B. Kimelfeld, U. Shalit, and K. Radin-\\nsky, “Building causal graphs from medical literature and electronic\\nmedical records,” Proceedings of the AAAI Conference on Artiﬁcial\\nIntelligence, vol. 33, no. 01, pp. 1102–1109, Jul. 2019.\\n[111] R. Stiefelhagen, C. Fugen, R. Gieselmann, H. Holzapfel, K. Nickel,\\nand A. Waibel, “Natural human-robot interaction using speech, head'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='Intelligence, vol. 33, no. 01, pp. 1102–1109, Jul. 2019.\\n[111] R. Stiefelhagen, C. Fugen, R. Gieselmann, H. Holzapfel, K. Nickel,\\nand A. Waibel, “Natural human-robot interaction using speech, head\\npose and gestures,” in 2004 IEEE/RSJ International Conference on\\nIntelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566) ,\\nvol. 3, Sep. 2004, pp. 2422–2427 vol.3.\\n[112] G. P. Finley, E. Edwards, W. Salloum, A. Robinson, N. Sadoughi,\\nN. Axtmann, M. Korenevsky, M. Brenndoerfer, M. Miller, and\\nD. Suendermann-Oeft, “Semi-supervised acoustic model retraining\\nfor medical ASR,” in Speech and Computer , ser. Lecture Notes in\\nComputer Science, A. Karpov, O. Jokisch, and R. Potapova, Eds.\\nCham: Springer International Publishing, 2018, pp. 177–187.\\n[113] J. Sas and T. Poreba, “Optimal acoustic model complexity selection in\\npolish medical speech recognition,” Journal of Medical Informatics &\\nTechnologies, vol. V ol. 17, 2011.\\n[114] J. M. Paulett and C. P. Langlotz, “Improving language models for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='polish medical speech recognition,” Journal of Medical Informatics &\\nTechnologies, vol. V ol. 17, 2011.\\n[114] J. M. Paulett and C. P. Langlotz, “Improving language models for\\nradiology speech recognition,” Journal of Biomedical Informatics ,\\nvol. 42, no. 1, pp. 53–58, Feb. 2009.\\n[115] C.-C. Chiu, A. Tripathi, K. Chou, C. Co, N. Jaitly, D. Jaunzeikare,\\nA. Kannan, P. Nguyen, H. Sak, A. Sankar, J. Tansuwan, N. Wan,\\nY . Wu, and X. Zhang, “Speech recognition for medical conversations,”\\narXiv:1711.07274 [cs, eess, stat] , Jun. 2018.\\n[116] E. Edwards, W. Salloum, G. P. Finley, J. Fone, G. Cardiff, M. Miller,\\nand D. Suendermann-Oeft, “Medical speech recognition: Reaching\\nparity with humans,” in Speech and Computer , ser. Lecture Notes\\nin Computer Science, A. Karpov, R. Potapova, and I. Mporas, Eds.\\nCham: Springer International Publishing, 2017, pp. 512–524.\\n[117] T. He, W. Zhao, and L. Xu, “DOP-tacotron: A fast chinese TTS system'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='in Computer Science, A. Karpov, R. Potapova, and I. Mporas, Eds.\\nCham: Springer International Publishing, 2017, pp. 512–524.\\n[117] T. He, W. Zhao, and L. Xu, “DOP-tacotron: A fast chinese TTS system\\nwith local-based attention,” in 2020 Chinese Control And Decision\\nConference (CCDC), Aug. 2020, pp. 4345–4350.\\n[118] K. Sugiura, Y . Shiga, H. Kawai, T. Misu, and C. Hori, “Non-monologue\\nHMM-based speech synthesis for service robots: A cloud robotics\\napproach,” in 2014 IEEE International Conference on Robotics and\\nAutomation (ICRA), May 2014, pp. 2237–2242.\\n[119] H. Akbari, B. Khalighinejad, J. L. Herrero, A. D. Mehta, and N. Mes-\\ngarani, “Towards reconstructing intelligible speech from the human\\nauditory cortex,” Scientiﬁc Reports, vol. 9, no. 1, p. 874, Jan. 2019.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='14 IEEE REVIEWS IN BIOMEDICAL ENGINEERING\\n[120] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, “Speech synthesis\\nfrom neural decoding of spoken sentences,” Nature, vol. 568, no. 7753,\\npp. 493–498, Apr. 2019.\\n[121] C. Herff, L. Diener, M. Angrick, E. Mugler, M. C. Tate, M. A. Goldrick,\\nD. J. Krusienski, M. W. Slutzky, and T. Schultz, “Generating natural,\\nintelligible speech from brain activity in motor, premotor, and inferior\\nfrontal cortices,” Frontiers in Neuroscience, vol. 13, p. 1267, 2019.\\n[122] C. Jreige, R. Patel, and H. T. Bunnell, “V ocaliD: Personalizing text-\\nto-speech synthesis for individuals with severe speech impairment,” in\\nProceedings of the 11th International ACM SIGACCESS Conference\\non Computers and Accessibility, ser. Assets ’09. New York, NY , USA:\\nAssociation for Computing Machinery, Oct. 2009, pp. 259–260.\\n[123] M. Marge, C. Espy-Wilson, N. G. Ward, A. Alwan, Y . Artzi, M. Bansal,\\nG. Blankenship, J. Chai, H. Daum ´e, D. Dey, M. Harper, T. Howard,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='[123] M. Marge, C. Espy-Wilson, N. G. Ward, A. Alwan, Y . Artzi, M. Bansal,\\nG. Blankenship, J. Chai, H. Daum ´e, D. Dey, M. Harper, T. Howard,\\nC. Kennington, I. Kruijff-Korbayov ´a, D. Manocha, C. Matuszek,\\nR. Mead, R. Mooney, R. K. Moore, M. Ostendorf, H. Pon-Barry, A. I.\\nRudnicky, M. Scheutz, R. S. Amant, T. Sun, S. Tellex, D. Traum, and\\nZ. Yu, “Spoken language interaction with robots: Recommendations for\\nfuture research,” Computer Speech & Language , vol. 71, p. 101255,\\nJan. 2022.\\n[124] J. James, B. T. Balamurali, C. I. Watson, and B. MacDonald, “Empa-\\nthetic speech synthesis and testing for healthcare robots,” International\\nJournal of Social Robotics , Sep. 2020.\\n[125] X. Li, B. MacDonald, and C. I. Watson, “Expressive facial speech\\nsynthesis on a robotic platform,” in Proceedings of the 2009 IEEE/RSJ\\nInternational Conference on Intelligent Robots and Systems , ser.\\nIROS’09. St. Louis, MO, USA: IEEE Press, Oct. 2009, pp. 5009–\\n5014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='International Conference on Intelligent Robots and Systems , ser.\\nIROS’09. St. Louis, MO, USA: IEEE Press, Oct. 2009, pp. 5009–\\n5014.\\n[126] S. Roehling, B. Macdonald, and C. Watson, “Towards expressive\\nspeech synthesis in english on a robotic platform,” in In Proceedings\\nof the Australasian International Conference on Speech Science and\\nTechnology, 2006, pp. 130–135.\\n[127] K. K ¨uhne, M. H. Fischer, and Y . Zhou, “The human takes it all: Hu-\\nmanlike synthesized voices are perceived as less eerie and more likable.\\nevidence from a subjective ratings study,” Frontiers in Neurorobotics,\\nvol. 14, p. 105, 2020.\\n[128] F. Jiang, Y . Jiang, H. Zhi, Y . Dong, H. Li, S. Ma, Y . Wang, Q. Dong,\\nH. Shen, and Y . Wang, “Artiﬁcial intelligence in healthcare: Past,\\npresent and future,” Stroke and Vascular Neurology, vol. 2, no. 4, Dec.\\n2017.\\n[129] K. N. Dew, A. M. Turner, Y . K. Choi, A. Bosold, and K. Kirchhoff,\\n“Development of machine translation technology for assisting health'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='2017.\\n[129] K. N. Dew, A. M. Turner, Y . K. Choi, A. Bosold, and K. Kirchhoff,\\n“Development of machine translation technology for assisting health\\ncommunication: A systematic review,”Journal of Biomedical Informat-\\nics, vol. 85, pp. 56–67, Sep. 2018.\\n[130] G. Randhawa, M. Ferreyra, R. Ahmed, O. Ezzat, and K. Pottie, “Using\\nmachine translation in clinical practice,” Canadian Family Physician ,\\nvol. 59, no. 4, pp. 382–383, Apr. 2013.\\n[131] F. Goss, S. Blackley, C. Ortega, L. Kowalski, A. Landman, C. Lin,\\nM. Meteer, S. Bakes, S. Gradwohl, D. Bates, and Z. Li, “A clinician\\nsurvey of using speech recognition for clinical documentation in the\\nelectronic health record,” International Journal of Medical Informatics,\\nvol. 130, Jul. 2019.\\n[132] K. Saxena, R. Diamond, R. F. Conant, T. H. Mitchell, i. G. Gallopyn,\\nand K. E. Yakimow, “Provider adoption of speech recognition and its\\nimpact on satisfaction, documentation quality, efﬁciency, and cost in an'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='and K. E. Yakimow, “Provider adoption of speech recognition and its\\nimpact on satisfaction, documentation quality, efﬁciency, and cost in an\\ninpatient EHR,” AMIA Summits on Translational Science Proceedings ,\\nvol. 2018, pp. 186–195, May 2018.\\n[133] Y . Zhao, “Speech-recognition technology in health care and special-\\nneeds assistance [life sciences],” IEEE Signal Processing Magazine ,\\nvol. 26, no. 3, pp. 87–90, May 2009.\\n[134] T. R. Goodwin and S. M. Harabagiu, “Medical question answering for\\nclinical decision support,” Proceedings of the ... ACM International\\nConference on Information & Knowledge Management. ACM Interna-\\ntional Conference on Information and Knowledge Management , vol.\\n2016, pp. 297–306, Oct. 2016.\\n[135] G. Xu, W. Rong, Y . Wang, Y . Ouyang, and Z. Xiong, “External\\nfeatures enriched model for biomedical question answering,” BMC\\nBioinformatics, vol. 22, no. 1, p. 272, May 2021.\\n[136] X. Shi, D. Jiang, Y . Huang, X. Wang, Q. Chen, J. Yan, and B. Tang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='features enriched model for biomedical question answering,” BMC\\nBioinformatics, vol. 22, no. 1, p. 272, May 2021.\\n[136] X. Shi, D. Jiang, Y . Huang, X. Wang, Q. Chen, J. Yan, and B. Tang,\\n“Family history information extraction via deep joint learning,” BMC\\nMedical Informatics and Decision Making , vol. 19, no. Suppl 10, Dec.\\n2019.\\n[137] A. Gupta, I. Banerjee, and D. L. Rubin, “Automatic information\\nextraction from unstructured mammography reports using distributed\\nsemantics,” Journal of Biomedical Informatics, vol. 78, pp. 78–86, Feb.\\n2018.\\n[138] J. Yang, Y . Liu, M. Qian, C. Guan, and X. Yuan, “Information\\nextraction from electronic medical records using multitask recurrent\\nneural network with contextual word embedding,” Applied Sciences ,\\nvol. 9, no. 18, p. 3658, Jan. 2019.\\n[139] S. Zheng, S. K. Jabbour, S. E. O’Reilly, J. J. Lu, L. Dong, L. Ding,\\nY . Xiao, N. Yue, F. Wang, and W. Zou, “Automated information\\nextraction on treatment and prognosis for Non–Small cell lung can-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='Y . Xiao, N. Yue, F. Wang, and W. Zou, “Automated information\\nextraction on treatment and prognosis for Non–Small cell lung can-\\ncer radiotherapy patients: Clinical study,” JMIR Medical Informatics ,\\nvol. 6, no. 1, p. e8, Feb. 2018.\\n[140] H. Liang, B. Y . Tsui, H. Ni, C. C. S. Valentim, S. L. Baxter, G. Liu,\\nW. Cai, D. S. Kermany, X. Sun, J. Chen, L. He, J. Zhu, P. Tian, H. Shao,\\nL. Zheng, R. Hou, S. Hewett, G. Li, P. Liang, X. Zang, Z. Zhang,\\nL. Pan, H. Cai, R. Ling, S. Li, Y . Cui, S. Tang, H. Ye, X. Huang,\\nW. He, W. Liang, Q. Zhang, J. Jiang, W. Yu, J. Gao, W. Ou, Y . Deng,\\nQ. Hou, B. Wang, C. Yao, Y . Liang, S. Zhang, Y . Duan, R. Zhang,\\nS. Gibson, C. L. Zhang, O. Li, E. D. Zhang, G. Karin, N. Nguyen,\\nX. Wu, C. Wen, J. Xu, W. Xu, B. Wang, W. Wang, J. Li, B. Pizzato,\\nC. Bao, D. Xiang, W. He, S. He, Y . Zhou, W. Haw, M. Goldbaum,\\nA. Tremoulet, C.-N. Hsu, H. Carter, L. Zhu, K. Zhang, and H. Xia,\\n“Evaluation and accurate diagnoses of pediatric diseases using artiﬁcial'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='A. Tremoulet, C.-N. Hsu, H. Carter, L. Zhu, K. Zhang, and H. Xia,\\n“Evaluation and accurate diagnoses of pediatric diseases using artiﬁcial\\nintelligence,” Nature Medicine, vol. 25, no. 3, pp. 433–438, Mar. 2019.\\n[141] H. Harkema, W. W. Chapman, M. Saul, E. S. Dellon, R. E. Schoen, and\\nA. Mehrotra, “Developing a natural language processing application\\nfor measuring the quality of colonoscopy procedures,” Journal of the\\nAmerican Medical Informatics Association: JAMIA , vol. 18 Suppl 1,\\npp. i150–156, Dec. 2011.\\n[142] A. Mehrotra, E. S. Dellon, R. E. Schoen, M. Saul, F. Bishehsari,\\nC. Farmer, and H. Harkema, “Applying a natural language processing\\ntool to electronic health records to assess performance on colonoscopy\\nquality measures,” Gastrointestinal Endoscopy , vol. 75, no. 6, pp.\\n1233–1239.e14, Jun. 2012.\\n[143] S. Wunnava, X. Qin, T. Kakar, C. Sen, E. A. Rundensteiner, and\\nX. Kong, “Adverse drug event detection from electronic health records'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='1233–1239.e14, Jun. 2012.\\n[143] S. Wunnava, X. Qin, T. Kakar, C. Sen, E. A. Rundensteiner, and\\nX. Kong, “Adverse drug event detection from electronic health records\\nusing hierarchical recurrent neural networks with dual-level embed-\\nding,” Drug Safety, vol. 42, no. 1, pp. 113–122, Jan. 2019.\\n[144] R. G. Jackson, R. Patel, N. Jayatilleke, A. Kolliakou, M. Ball,\\nG. Gorrell, A. Roberts, R. J. Dobson, and R. Stewart, “Natural\\nlanguage processing to extract symptoms of severe mental illness from\\nclinical text: The clinical record interactive search comprehensive data\\nextraction (CRIS-CODE) project,”BMJ Open, vol. 7, no. 1, p. e012012,\\nJan. 2017.\\n[145] J. Luo, L. Lan, D. Yang, S. Huang, M. Li, J. Yin, J. Xiao, and X. Zhou,\\n“Early prediction of organ failures in patients with acute pancreatitis\\nusing text mining,” Scientiﬁc Programming, vol. 2021, p. e6683942,\\nMay 2021.\\n[146] X. Wang, X. Xu, W. Tong, R. Roberts, and Z. Liu, “InferBERT: A'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='using text mining,” Scientiﬁc Programming, vol. 2021, p. e6683942,\\nMay 2021.\\n[146] X. Wang, X. Xu, W. Tong, R. Roberts, and Z. Liu, “InferBERT: A\\ntransformer-based causal inference framework for enhancing pharma-\\ncovigilance,” Frontiers in Artiﬁcial Intelligence , vol. 4, p. 67, 2021.\\n[147] S. V . Wang, O. V . Patterson, J. J. Gagne, J. S. Brown, R. Ball,\\nP. Jonsson, A. Wright, L. Zhou, W. Goettsch, and A. Bate, “Transparent\\nreporting on research using unstructured electronic health record data to\\ngenerate ‘real world’ evidence of comparative effectiveness and safety,”\\nDrug Safety, vol. 42, no. 11, pp. 1297–1309, Nov. 2019.\\n[148] A. Lee, B. E. Alving, M. B. Horup, and L. Thrysoee, “Information\\nretrieval as a part of evidence-based practice: Retrieval skills, behavior\\nand needs among nurses at a large university hospital:,” Nordic Journal\\nof Nursing Research , Aug. 2019.\\n[149] T. B. Patrick, G. Demiris, L. C. Folk, D. E. Moxley, J. A. Mitchell, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='and needs among nurses at a large university hospital:,” Nordic Journal\\nof Nursing Research , Aug. 2019.\\n[149] T. B. Patrick, G. Demiris, L. C. Folk, D. E. Moxley, J. A. Mitchell, and\\nD. Tao, “Evidence-based retrieval in evidence-based medicine,”Journal\\nof the Medical Library Association , vol. 92, no. 2, pp. 196–199, Apr.\\n2004.\\n[150] N. Ford, D. Miller, A. Booth, A. O’rourke, J. Ralph, and E. Turnock,\\n“Information retrieval for evidence-based decision making,” JOURNAL\\nOF DOCUMENTATION, vol. 55, Oct. 1999.\\n[151] N. W. Sterling, R. E. Patzer, M. Di, and J. D. Schrager, “Prediction\\nof emergency department patient disposition based on natural language\\nprocessing of triage notes,” International Journal of Medical Informat-\\nics, vol. 129, pp. 184–188, Sep. 2019.\\n[152] B. Tahayori, N. Chini-Foroush, and H. Akhlaghi, “Advanced natural\\nlanguage processing technique to predict patient disposition based on\\nemergency triage notes,” Emergency Medicine Australasia , vol. 33,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='language processing technique to predict patient disposition based on\\nemergency triage notes,” Emergency Medicine Australasia , vol. 33,\\nno. 3, pp. 480–484, 2021.\\n[153] E. Sezgin, Y . Huang, U. Ramtekkar, and S. Lin, “Readiness for voice\\nassistants to support healthcare delivery during a health crisis and\\npandemic,” npj Digital Medicine , vol. 3, no. 1, pp. 1–4, Sep. 2020.\\n[154] S. Sp ¨anig, A. Emberger-Klein, J.-P. Sowa, A. Canbay, K. Menrad, and\\nD. Heider, “The virtual doctor: An interactive artiﬁcial intelligence'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ZHOU et al.: NATURAL LANGUAGE PROCESSING FOR SMART HEALTHCARE 15\\nbased on deep learning for non-invasive prediction of diabetes,” Arti-\\nﬁcial Intelligence in Medicine , vol. 100, p. 101706, Sep. 2019.\\n[155] M. Gandhi, V . K. Singh, and V . Kumar, “IntelliDoctor - AI based\\nmedical assistant,” in 2019 Fifth International Conference on Science\\nTechnology Engineering and Mathematics (ICONSTEM) , vol. 1, Mar.\\n2019, pp. 162–168.\\n[156] E. I. Agustin, R. T. Yunardi, and A. A. Firdaus, “V oice recognition\\nsystem for controlling electrical appliances in smart hospital room,”\\nTELKOMNIKA (Telecommunication Computing Electronics and Con-\\ntrol), vol. 17, no. 2, pp. 965–972, Apr. 2019.\\n[157] A. Ismail, S. Abdlerazek, and I. M. El-Henawy, “Development of smart\\nhealthcare system based on speech recognition using support vector\\nmachine and dynamic time warping,” Sustainability, vol. 12, no. 6, p.\\n2403, Jan. 2020.\\n[158] L. Grasse, S. J. Boutros, and M. S. Tata, “Speech interaction to control a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='machine and dynamic time warping,” Sustainability, vol. 12, no. 6, p.\\n2403, Jan. 2020.\\n[158] L. Grasse, S. J. Boutros, and M. S. Tata, “Speech interaction to control a\\nhands-free delivery robot for high-risk health care scenarios,” Frontiers\\nin Robotics and AI , vol. 8, p. 40, 2021.\\n[159] J. Holland, L. Kingston, C. McCarthy, E. Armstrong, P. O’Dwyer,\\nF. Merz, and M. McConnell, “Service robots in the healthcare sector,”\\nRobotics, vol. 10, no. 1, p. 47, Mar. 2021.\\n[160] C. M. Lineback, R. Garg, E. Oh, A. M. Naidech, J. L. Holl, and\\nS. Prabhakaran, “Prediction of 30-day readmission after stroke using\\nmachine learning and natural language processing,” Frontiers in Neu-\\nrology, vol. 12, p. 1069, 2021.\\n[161] A. Rajkomar, E. Oren, K. Chen, A. M. Dai, N. Hajaj, M. Hardt, P. J.\\nLiu, X. Liu, J. Marcus, M. Sun, P. Sundberg, H. Yee, K. Zhang,\\nY . Zhang, G. Flores, G. E. Duggan, J. Irvine, Q. Le, K. Litsch,\\nA. Mossin, J. Tansuwan, D. Wang, J. Wexler, J. Wilson, D. Ludwig,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='Liu, X. Liu, J. Marcus, M. Sun, P. Sundberg, H. Yee, K. Zhang,\\nY . Zhang, G. Flores, G. E. Duggan, J. Irvine, Q. Le, K. Litsch,\\nA. Mossin, J. Tansuwan, D. Wang, J. Wexler, J. Wilson, D. Ludwig,\\nS. L. V olchenboum, K. Chou, M. Pearson, S. Madabushi, N. H. Shah,\\nA. J. Butte, M. D. Howell, C. Cui, G. S. Corrado, and J. Dean,\\n“Scalable and accurate deep learning with electronic health records,”\\nnpj Digital Medicine , vol. 1, no. 1, p. 18, Dec. 2018.\\n[162] A. Rumshisky, M. Ghassemi, T. Naumann, P. Szolovits, V . M. Cas-\\ntro, T. H. McCoy, and R. H. Perlis, “Predicting early psychiatric\\nreadmission with natural language processing of narrative discharge\\nsummaries,” Translational Psychiatry , vol. 6, no. 10, p. e921, Oct.\\n2016.\\n[163] O. Alfarghaly, R. Khaled, A. Elkorany, M. Helal, and A. Fahmy, “Au-\\ntomated radiology report generation using conditioned transformers,”\\nInformatics in Medicine Unlocked , vol. 24, p. 100557, Jan. 2021.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='tomated radiology report generation using conditioned transformers,”\\nInformatics in Medicine Unlocked , vol. 24, p. 100557, Jan. 2021.\\n[164] B. Chintagunta, N. Katariya, X. Amatriain, and A. Kannan, “Medically\\naware GPT-3 as a data generator for medical dialogue summarization,”\\nin Proceedings of the Second Workshop on Natural Language Process-\\ning for Medical Conversations. Online: Association for Computational\\nLinguistics, Jun. 2021, pp. 66–76.\\n[165] A. M. Ibrahim, “Ontology-driven information retrieval for healthcare\\ninformation system : A case study,” International Journal of Network\\nSecurity & Its Applications , vol. 5, no. 1, pp. 61–69, Jan. 2013.\\n[166] M. Khanbhai, P. Anyadi, J. Symons, K. Flott, A. Darzi, and E. Mayer,\\n“Applying natural language processing and machine learning tech-\\nniques to patient experience feedback: A systematic review,” BMJ\\nHealth & Care Informatics , vol. 28, no. 1, p. e100262, Mar. 2021.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='“Applying natural language processing and machine learning tech-\\nniques to patient experience feedback: A systematic review,” BMJ\\nHealth & Care Informatics , vol. 28, no. 1, p. e100262, Mar. 2021.\\n[167] K. Nawab, G. Ramsey, and R. Schreiber, “Natural language processing\\nto extract meaningful information from patient experience feedback,”\\nApplied Clinical Informatics , vol. 11, no. 2, pp. 242–252, Mar. 2020.\\n[168] K. Doing-Harris, D. L. Mowery, C. Daniels, W. W. Chapman, and\\nM. Conway, “Understanding patient satisfaction with received health-\\ncare services: A natural language processing approach,” AMIA Annual\\nSymposium Proceedings, vol. 2016, pp. 524–533, Feb. 2017.\\n[169] D. Rodger, A. Skuse, M. Wilmore, S. Humphreys, J. Dalton,\\nM. Flabouris, V . L. Clifton, D. Rodger, A. Skuse, M. Wilmore,\\nS. Humphreys, J. Dalton, M. Flabouris, and V . L. Clifton, “Preg-\\nnant women’s use of information and communications technologies\\nto access pregnancy-related health information in South Australia,”'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='S. Humphreys, J. Dalton, M. Flabouris, and V . L. Clifton, “Preg-\\nnant women’s use of information and communications technologies\\nto access pregnancy-related health information in South Australia,”\\nAustralian Journal of Primary Health , vol. 19, no. 4, pp. 308–312,\\nDec. 2013.\\n[170] B. Zhou, K. Wu, P. Lv, J. Wang, G. Chen, B. Ji, and S. Liu, “A\\nnew remote health-care system based on moving robot intended for\\nthe elderly at home,” Journal of Healthcare Engineering, vol. 2018, p.\\n4949863, Feb. 2018.\\n[171] P. J. Rani, J. Bakthakumar, B. P. Kumaar, U. P. Kumaar, and S. Kumar,\\n“V oice controlled home automation system using natural language\\nprocessing (NLP) and internet of things (IoT),” in 2017 Third Inter-\\nnational Conference on Science Technology Engineering Management\\n(ICONSTEM), Mar. 2017, pp. 368–373.\\n[172] A. Tapus, M. J. Mataric, and B. Scassellati, “Socially assistive\\nrobotics,” IEEE Robotics Automation Magazine , vol. 14, no. 1, pp.\\n35–42, Mar. 2007.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='(ICONSTEM), Mar. 2017, pp. 368–373.\\n[172] A. Tapus, M. J. Mataric, and B. Scassellati, “Socially assistive\\nrobotics,” IEEE Robotics Automation Magazine , vol. 14, no. 1, pp.\\n35–42, Mar. 2007.\\n[173] N. Mavridis, “A review of verbal and non-verbal human–robot interac-\\ntive communication,” Robotics and Autonomous Systems , vol. 63, pp.\\n22–35, Jan. 2015.\\n[174] J. R. Green, R. L. MacDonald, P.-P. Jiang, J. Cattiau, R. Heywood,\\nR. Cave, K. Seaver, M. A. Ladewig, J. Tobin, M. P. Brenner, P. C.\\nNelson, and K. Tomanek, “Automatic speech recognition of disordered\\nspeech: Personalized models outperforming human listeners on short\\nphrases,” in Interspeech 2021. ISCA, Aug. 2021, pp. 4778–4782.\\n[175] K. Hux, K. Knollman-Porter, J. Brown, and S. E. Wallace, “Compre-\\nhension of synthetic speech and digitized natural speech by adults with\\naphasia,”Journal of Communication Disorders, vol. 69, pp. 15–26, Sep.\\n2017.\\n[176] K. Hux, J. A. Brown, S. Wallace, K. Knollman-Porter, A. Saylor, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='aphasia,”Journal of Communication Disorders, vol. 69, pp. 15–26, Sep.\\n2017.\\n[176] K. Hux, J. A. Brown, S. Wallace, K. Knollman-Porter, A. Saylor, and\\nE. Lapp, “Effect of text-to-speech rate on reading comprehension by\\nadults with aphasia,” American Journal of Speech-Language Pathology,\\nvol. 29, no. 1, pp. 168–184, Jul. 2020.\\n[177] S. Cassidy, B. Stenger, L. Van Dongen, K. Yanagisawa, R. Anderson,\\nV . Wan, S. Baron-Cohen, and R. Cipolla, “Expressive visual text-to-\\nspeech as an assistive technology for individuals with autism spectrum\\nconditions,” Computer Vision and Image Understanding , vol. 148, pp.\\n193–200, Jul. 2016.\\n[178] B. Repova, M. Zabrodsky, J. Plzak, D. Kalfert, J. Matousek, and\\nJ. Betka, “Text-to-speech synthesis as an alternative communication\\nmeans after total laryngectomy,” Biomedical Papers of the Medical\\nFaculty of the University Palacky, Olomouc, Czechoslovakia, Apr. 2020.\\n[179] F. C. Lyall, P. J. Clamp, and D. Hajioff, “Smartphone speech-to-text'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='Faculty of the University Palacky, Olomouc, Czechoslovakia, Apr. 2020.\\n[179] F. C. Lyall, P. J. Clamp, and D. Hajioff, “Smartphone speech-to-text\\napplications for communication with profoundly deaf patients,” The\\nJournal of Laryngology and Otology , vol. 130, no. 1, pp. 104–106,\\nJan. 2016.\\n[180] S. Nittrouer, L. M. Krieg, and J. H. Lowenstein, “Speech recognition\\nin noise by children with and without dyslexia: How is it related to\\nreading?” Research in developmental disabilities , vol. 77, pp. 98–113,\\nJun. 2018.\\n[181] D. Ria ˜no, M. Peleg, and A. ten Teije, “Ten years of knowledge repre-\\nsentation for health care (2009–2018): Topics, trends, and challenges,”\\nArtiﬁcial Intelligence in Medicine , vol. 100, p. 101713, Sep. 2019.\\n[182] Q. Bao, L. Ni, and J. Liu, “HHH: An online medical chatbot system\\nbased on knowledge graph and hierarchical bi-directional attention,”\\nProceedings of the Australasian Computer Science Week Multiconfer-\\nence, pp. 1–10, Feb. 2020.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='based on knowledge graph and hierarchical bi-directional attention,”\\nProceedings of the Australasian Computer Science Week Multiconfer-\\nence, pp. 1–10, Feb. 2020.\\n[183] W. Xie, R. Ding, J. Yan, and Y . Qu, “A mobile-based question-\\nanswering and early warning system for assisting diabetes manage-\\nment,” Wireless Communications and Mobile Computing, vol. 2018, p.\\ne9163160, Jun. 2018.\\n[184] P. Peters, Y . Qian, and J. Ding, “Translating medical terminology and\\nbilingual terminography,” Lexicography: Journal of ASIALEX , vol. 3,\\nno. 2, pp. 99–113, 2016.\\n[185] A. Renato, J. Casta ˜no, M. d. P. A. Williams, H. Berinsky, M. L.\\nGambarte, H. Park, D. P ´erez-Rey, C. Otero, and D. Luna, “A machine\\ntranslation approach for medical terms,” in HEALTHINF, 2018.\\n[186] J. Leo, G. Kurdi, N. Matentzoglu, B. Parsia, U. Sattler, S. Forge,\\nG. Donato, and W. Dowling, “Ontology-based generation of medical,\\nmulti-term MCQs,” International Journal of Artiﬁcial Intelligence in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='G. Donato, and W. Dowling, “Ontology-based generation of medical,\\nmulti-term MCQs,” International Journal of Artiﬁcial Intelligence in\\nEducation, vol. 29, no. 2, pp. 145–188, May 2019.\\n[187] NHS, “Nhs population screening explained,” nhs population screening\\nexplained, Feb 2013. [Online]. Available: https://www.gov.uk/guidance/\\nnhs-population-screening-explained\\n[188] P. M, G. M, Newton-DameRemle, T. E, P. E, M. H, and G. N, “The\\nstate of population health surveillance using electronic health records:\\nA narrative review,” Population Health Management, Jun. 2015.\\n[189] D. Georgiou, A. MacFarlane, and T. Russell-Rose, “Extracting senti-\\nment from healthcare survey data: An evaluation of sentiment analysis\\ntools,” in 2015 Science and Information Conference (SAI) , Jul. 2015,\\npp. 352–361.\\n[190] H. ¨Ozt¨urk, A. ¨Ozg¨ur, P. Schwaller, T. Laino, and E. Ozkirimli, “Explor-\\ning chemical space using natural language processing methodologies'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='pp. 352–361.\\n[190] H. ¨Ozt¨urk, A. ¨Ozg¨ur, P. Schwaller, T. Laino, and E. Ozkirimli, “Explor-\\ning chemical space using natural language processing methodologies\\nfor drug discovery,”Drug Discovery Today, vol. 25, no. 4, pp. 689–705,\\nApr. 2020.\\n[191] F. Lake, “Artiﬁcial intelligence in drug discovery: What is new, and\\nwhat is next?” Future Drug Discovery , vol. 1, no. 2, p. FDD19, Oct.\\n2019.\\n[192] T.-H. Pham, Y . Qiu, J. Zeng, L. Xie, and P. Zhang, “A deep learning\\nframework for high-throughput mechanism-driven phenotype com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='16 IEEE REVIEWS IN BIOMEDICAL ENGINEERING\\npound screening and its application to COVID-19 drug repurposing,”\\nNature Machine Intelligence , vol. 3, no. 3, pp. 247–257, Mar. 2021.\\n[193] J. Schwartz, M. Awale, and J.-L. Reymond, “SMIfp (SMILES ﬁnger-\\nprint) chemical space for virtual screening and visualization of large\\ndatabases of organic molecules,” Journal of Chemical Information and\\nModeling, vol. 53, no. 8, pp. 1979–1989, Aug. 2013.\\n[194] F. Zhang, B. Sun, X. Diao, W. Zhao, and T. Shu, “Prediction of adverse\\ndrug reactions based on knowledge graph embedding,” BMC Medical\\nInformatics and Decision Making , vol. 21, no. 1, p. 38, Feb. 2021.\\n[195] K. Bouhedjar, A. Boukelia, A. K. Nacereddine, A. Boucheham, A. Be-\\nlaidi, and A. Djerourou, “A natural language processing approach\\nbased on embedding deep learning from heterogeneous compounds\\nfor quantitative structure–activity relationship modeling,” Chemical\\nBiology & Drug Design , vol. 96, no. 3, pp. 961–972, 2020.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='based on embedding deep learning from heterogeneous compounds\\nfor quantitative structure–activity relationship modeling,” Chemical\\nBiology & Drug Design , vol. 96, no. 3, pp. 961–972, 2020.\\n[196] W. Jeon and D. Kim, “FP2VEC: A new molecular featurizer for\\nlearning molecular properties,” Bioinformatics, vol. 35, no. 23, pp.\\n4979–4985, Dec. 2019.\\n[197] L. Chen, Y . Gu, X. Ji, C. Lou, Z. Sun, H. Li, Y . Gao, and Y . Huang,\\n“Clinical trial cohort selection based on multi-level rule-based natural\\nlanguage processing system,” Journal of the American Medical Infor-\\nmatics Association : JAMIA, vol. 26, no. 11, pp. 1218–1226, Jul. 2019.\\n[198] S. Harrer, P. Shah, B. Antony, and J. Hu, “Artiﬁcial intelligence for\\nclinical trial design,” Trends in Pharmacological Sciences , vol. 40,\\nno. 8, pp. 577–591, Aug. 2019.\\n[199] H. Tissot, F. Asselbergs, A. Shah, D. Brealey, S. Harris, R. Agbakoba,\\nA. Folarin, L. Romao, L. Roguski, and R. Dobson, “Natural language'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='no. 8, pp. 577–591, Aug. 2019.\\n[199] H. Tissot, F. Asselbergs, A. Shah, D. Brealey, S. Harris, R. Agbakoba,\\nA. Folarin, L. Romao, L. Roguski, and R. Dobson, “Natural language\\nprocessing for mimicking clinical trial recruitment in critical care:\\nA semi-automated simulation based on the LeoPARDS trial,” IEEE\\nJournal of Biomedical and Health Informatics , vol. PP, pp. 1–1, Mar.\\n2020.\\n[200] X. Chen, H. Xie, G. Cheng, L. K. M. Poon, M. Leng, and F. L. Wang,\\n“Trends and features of the applications of natural language processing\\ntechniques for clinical trials text analysis,” Applied Sciences , vol. 10,\\nno. 6, p. 2157, Jan. 2020.\\n[201] C. L. Ventola, “Big data and pharmacovigilance: Data mining for\\nadverse drug events and interactions,” Pharmacy and Therapeutics ,\\nvol. 43, no. 6, pp. 340–351, Jun. 2018.\\n[202] X. Wang, G. Hripcsak, M. Markatou, and C. Friedman, “Active\\ncomputerized pharmacovigilance using natural language processing,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='vol. 43, no. 6, pp. 340–351, Jun. 2018.\\n[202] X. Wang, G. Hripcsak, M. Markatou, and C. Friedman, “Active\\ncomputerized pharmacovigilance using natural language processing,\\nstatistics, and electronic health records: A feasibility study,” Journal\\nof the American Medical Informatics Association : JAMIA , vol. 16,\\nno. 3, pp. 328–337, 2009.\\n[203] F. Liu, A. Jagannatha, and H. Yu, “Towards drug safety surveillance\\nand pharmacovigilance: Current progress in detecting medication and\\nadverse drug events from electronic health records,” Drug Safety ,\\nvol. 42, no. 1, pp. 95–97, Jan. 2019.\\n[204] B. Zhou, G. Yang, Z. Shi, and S. Ma, “Interpretable Temporal Attention\\nNetwork for COVID-19 forecasting,”Applied Soft Computing, vol. 120,\\np. 108691, May 2022.\\n[205] N. Zheng, S. Du, J. Wang, H. Zhang, W. Cui, Z. Kang, T. Yang, B. Lou,\\nY . Chi, H. Long, M. Ma, Q. Yuan, S. Zhang, D. Zhang, F. Ye, and\\nJ. Xin, “Predicting COVID-19 in china using hybrid AI model,” IEEE'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='Y . Chi, H. Long, M. Ma, Q. Yuan, S. Zhang, D. Zhang, F. Ye, and\\nJ. Xin, “Predicting COVID-19 in china using hybrid AI model,” IEEE\\nTransactions on Cybernetics, vol. 50, no. 7, pp. 2891–2904, Jul. 2020.\\n[206] Q. Chen, R. Leaman, A. Allot, L. Luo, C.-H. Wei, S. Yan, and Z. Lu,\\n“Artiﬁcial intelligence in action: Addressing the COVID-19 pandemic\\nwith natural language processing,” Annual Review of Biomedical Data\\nScience, vol. 4, no. 1, pp. 313–339, 2021.\\n[207] A. Chapman, K. Peterson, A. Turano, T. Box, K. Wallace, and\\nM. Jones, “A natural language processing system for national COVID-\\n19 surveillance in the US department of veterans affairs,” in Proceed-\\nings of the 1st Workshop on NLP for COVID-19 at ACL 2020. Online:\\nAssociation for Computational Linguistics, 2020.\\n[208] R. C. Cury, I. Megyeri, T. Lindsey, R. Macedo, J. Batlle, S. Kim,\\nB. Baker, R. Harris, and R. H. Clark, “Natural language processing\\nand machine learning for detection of respiratory illness by chest CT'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='B. Baker, R. Harris, and R. H. Clark, “Natural language processing\\nand machine learning for detection of respiratory illness by chest CT\\nimaging and tracking of COVID-19 pandemic in the united states,”\\nRadiology: Cardiothoracic Imaging , vol. 3, no. 1, p. e200596, Feb.\\n2021.\\n[209] D. DeCaprio, J. Gartner, C. J. McCall, T. Burgess, K. Garcia,\\nS. Kothari, and S. Sayed, “Building a COVID-19 vulnerability index,”\\nJournal of Medical Artiﬁcial Intelligence , vol. 3, no. 0, Dec. 2020.\\n[210] S. M. Meystre, P. M. Heider, Y . Kim, M. Davis, J. Obeid, J. Madory,\\nand A. V . Alekseyenko, “Natural language processing enabling\\nCOVID-19 predictive analytics to support data-driven patient advising\\nand pooled testing,” Journal of the American Medical Informatics\\nAssociation: JAMIA, vol. 29, no. 1, pp. 12–21, Dec. 2021.\\n[211] L. Wang, L. Jiang, D. Pan, Q. Wang, Z. Yin, Z. Kang, H. Tian, X. Geng,\\nJ. Shao, W. Pan, J. Yin, L. Fang, Y . Wang, W. Zhang, Z. Li, J. Zheng,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='[211] L. Wang, L. Jiang, D. Pan, Q. Wang, Z. Yin, Z. Kang, H. Tian, X. Geng,\\nJ. Shao, W. Pan, J. Yin, L. Fang, Y . Wang, W. Zhang, Z. Li, J. Zheng,\\nW. Hu, Y . Pan, D. Yu, S. Guo, W. Lu, Q. Li, Y . Zhou, and H. Xu, “Novel\\napproach by natural language processing for COVID-19 knowledge\\ndiscovery,” Biomedical Journal, Apr. 2022.\\n[212] A. Keshavarzi Arshadi, J. Webb, M. Salem, E. Cruz, S. Calad-\\nThomson, N. Ghadirian, J. Collins, E. Diez-Cecilia, B. Kelly,\\nH. Goodarzi, and J. S. Yuan, “Artiﬁcial Intelligence for COVID-19\\nDrug Discovery and Vaccine Development,” Frontiers in Artiﬁcial\\nIntelligence, vol. 3, 2020.\\n[213] Z. Liu, R. A. Roberts, M. Lal-Nag, X. Chen, R. Huang, and W. Tong,\\n“AI-based language models powering drug discovery and develop-\\nment,” Drug Discovery Today , vol. 26, no. 11, pp. 2593–2607, Nov.\\n2021.\\n[214] WHO, “10 global health issues to track in 2021,”\\nhttps://www.who.int/news-room/spotlight/10-global-health-issues-\\nto-track-in-2021, 2020.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='2021.\\n[214] WHO, “10 global health issues to track in 2021,”\\nhttps://www.who.int/news-room/spotlight/10-global-health-issues-\\nto-track-in-2021, 2020.\\n[215] H.-J. Dai, C.-H. Su, Y .-Q. Lee, Y .-C. Zhang, C.-K. Wang, C.-J. Kuo,\\nand C.-S. Wu, “Deep learning-based natural language processing for\\nscreening psychiatric patients,” Frontiers in Psychiatry, vol. 11, 2021.\\n[216] D. D. DeSouza, J. Robin, M. Gumus, and A. Yeung, “Natural language\\nprocessing as an emerging tool to detect late-life depression,” Frontiers\\nin Psychiatry, vol. 12, 2021.\\n[217] R. G. Jackson, R. Patel, N. Jayatilleke, A. Kolliakou, M. Ball,\\nG. Gorrell, A. Roberts, R. J. Dobson, and R. Stewart, “Natural\\nlanguage processing to extract symptoms of severe mental illness from\\nclinical text: The clinical record interactive search comprehensive data\\nextraction (CRIS-CODE) project,”BMJ Open, vol. 7, no. 1, p. e012012,\\nJan. 2017.\\n[218] J. Cohen, J. Wright-Berryman, L. Rohlfs, D. Trocinski, L. Daniel, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='extraction (CRIS-CODE) project,”BMJ Open, vol. 7, no. 1, p. e012012,\\nJan. 2017.\\n[218] J. Cohen, J. Wright-Berryman, L. Rohlfs, D. Trocinski, L. Daniel, and\\nT. W. Klatt, “Integration and validation of a natural language processing\\nmachine learning suicide risk prediction model based on open-ended\\ninterview language in the emergency department,” Frontiers in Digital\\nHealth, vol. 4, 2022.\\n[219] D. Harvey, F. Lobban, P. Rayson, A. Warner, and S. Jones, “Natural\\nlanguage processing methods and bipolar disorder: Scoping review,”\\nJMIR Mental Health , vol. 9, no. 4, p. e35928, Apr. 2022.\\n[220] T. Zhang, A. M. Schoene, S. Ji, and S. Ananiadou, “Natural language\\nprocessing applied to mental illness detection: A narrative review,” npj\\nDigital Medicine, vol. 5, no. 1, pp. 1–13, Apr. 2022.\\n[221] G. Bedi, F. Carrillo, G. A. Cecchi, D. F. Slezak, M. Sigman, N. B. Mota,\\nS. Ribeiro, D. C. Javitt, M. Copelli, and C. M. Corcoran, “Automated'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='[221] G. Bedi, F. Carrillo, G. A. Cecchi, D. F. Slezak, M. Sigman, N. B. Mota,\\nS. Ribeiro, D. C. Javitt, M. Copelli, and C. M. Corcoran, “Automated\\nanalysis of free speech predicts psychosis onset in high-risk youths,”\\nnpj Schizophrenia, vol. 1, no. 1, pp. 1–7, Aug. 2015.\\n[222] R. A. Calvo, D. N. Milne, M. S. Hussain, and H. Christensen, “Natural\\nlanguage processing in mental health applications using non-clinical\\ntexts†,” Natural Language Engineering , vol. 23, no. 5, pp. 649–685,\\nSep. 2017.\\n[223] T. Althoff, K. Clark, and J. Leskovec, “Large-scale analysis of coun-\\nseling conversations: An application of natural language processing\\nto mental health,” Transactions of the Association for Computational\\nLinguistics, vol. 4, pp. 463–476, 2016.\\n[224] V . Chattaraman, W.-S. Kwon, J. E. Gilbert, and K. Ross, “Should AI-\\nBased, conversational digital assistants employ social- or task-oriented\\ninteraction style? A task-competency and reciprocity perspective for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='Based, conversational digital assistants employ social- or task-oriented\\ninteraction style? A task-competency and reciprocity perspective for\\nolder adults,” Computers in Human Behavior , vol. 90, pp. 315–330,\\nJan. 2019.\\n[225] A. Amin-Nejad, J. Ive, and S. Velupillai, “Exploring transformer text\\ngeneration for medical dataset augmentation,” in Proceedings of the\\n12th Language Resources and Evaluation Conference . Marseille,\\nFrance: European Language Resources Association, May 2020, pp.\\n4699–4708.\\n[226] J. Ive, N. Viani, J. Kam, L. Yin, S. Verma, S. Puntis, R. N. Cardinal,\\nA. Roberts, R. Stewart, and S. Velupillai, “Generation and evaluation\\nof artiﬁcial mental health records for natural language processing,” npj\\nDigital Medicine, vol. 3, no. 1, pp. 1–9, May 2020.\\n[227] X. Soto, O. Perez-de-Vi ˜naspre, G. Labaka, and M. Oronoz, “Neural\\nmachine translation of clinical texts between long distance languages,”\\nJournal of the American Medical Informatics Association , vol. 26,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='machine translation of clinical texts between long distance languages,”\\nJournal of the American Medical Informatics Association , vol. 26,\\nno. 12, pp. 1478–1487, Dec. 2019.\\n[228] K. Wołk and K. Marasek, “Neural-based machine translation for\\nmedical text domain. based on european medicines agency leaﬂet\\ntexts,” Procedia Computer Science , vol. 64, pp. 2–9, Jan. 2015.\\n[229] K. Wolk and K. P. Marasek, “Translation of medical texts using neural\\nnetworks,” International Journal of Reliable and Quality E-Healthcare\\n(IJRQEH), vol. 5, no. 4, pp. 51–66, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 16, 'page_label': '17', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ZHOU et al.: NATURAL LANGUAGE PROCESSING FOR SMART HEALTHCARE 17\\nBinggui Zhou received the B.Eng. degree from\\nJinan University, Zhuhai, China, in 2018, and the\\nM.Sc. degree from the University of Macau, Macao,\\nChina, in 2021, respectively. He is currently working\\ntoward the Ph.D. degree in Electrical and Computer\\nEngineering with the University of Macau, Macao,\\nChina. He also serves as a Research Assistant\\nwith the School of Intelligent Systems Science and\\nEngineering, Jinan University, Zhuhai, China. His\\nresearch interests include Natural Language Process-\\ning, Artiﬁcial Intelligence, and AI assisted Wireless\\nCommunications.\\nGuanghua Yang received his Ph.D. degree in elec-\\ntrical and electronic engineering from the University\\nof Hong Kong, Hong Kong, in 2006. From 2006\\nto 2013, he served as post-doctoral fellow, research\\nassociate at the University of Hong Kong. Since\\nApril 2017, he has been with Jinan University, where\\nhe is currently a Full Professor in the School of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 16, 'page_label': '17', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='associate at the University of Hong Kong. Since\\nApril 2017, he has been with Jinan University, where\\nhe is currently a Full Professor in the School of\\nIntelligent Systems Science and Engineering. His\\nresearch interests are in the general areas of AI and\\nits applications.\\nZheng Shi received his B.S. degree in communi-\\ncation engineering from Anhui Normal University,\\nChina, in 2010 and his M.S. degree in communi-\\ncation and information system from Nanjing Uni-\\nversity of Posts and Telecommunications (NUPT),\\nChina, in 2013. He obtained his Ph.D. degree in\\nElectrical and Computer Engineering from Univer-\\nsity of Macau, Macao, in 2017. He is currently an\\nAssociate Professor with the School of Intelligent\\nSystems Science and Engineering, Jinan University,\\nZhuhai, China. His current research interests include\\nhybrid automatic repeat request, non-orthogonal multiple access, machine\\nlearning and Internet of Things.\\nShaodan Ma received the double Bachelor’s degrees'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 16, 'page_label': '17', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='hybrid automatic repeat request, non-orthogonal multiple access, machine\\nlearning and Internet of Things.\\nShaodan Ma received the double Bachelor’s degrees\\nin science and economics and the M.Eng. degree\\nin electronic engineering from Nankai University,\\nTianjin, China, in 1999 and 2002, respectively, and\\nthe Ph.D. degree in electrical and electronic engi-\\nneering from The University of Hong Kong, Hong\\nKong, in 2006. From 2006 to 2011, she was a post-\\ndoctoral fellow at The University of Hong Kong.\\nSince August 2011, she has been with the University\\nof Macau, where she is currently a Professor. Her\\nresearch interests include array signal processing,\\nmachine learning, wireless sensing and mmwave communications.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='arXiv:2506.05176v3  [cs.CL]  11 Jun 2025\\nTechnical Report\\nQwen3 Embedding: Advancing Text Embedding and\\nReranking Through Foundation Models\\nYanzhao Zhang* Mingxin Li* Dingkun Long* Xin Zhang*\\nHuan Lin Baosong Yang Pengjun Xie An Yang\\nDayiheng Liu Junyang Lin Fei Huang Jingren Zhou\\nTongyi Lab Alibaba Group\\nhttps://huggingface.co/Qwen\\nhttps://modelscope.cn/organization/qwen\\nhttps://github.com/QwenLM/Qwen3-Embedding\\nAbstract\\nIn this work, we introduce the Qwen3 Embedding series, a significant advancement\\nover its predecessor, the GTE-Qwen series, in text embedding and reranking capabili-\\nties, built upon the Qwen3 foundation models. Leveraging the Qwen3 LLMs’ robust\\ncapabilities in multilingual text understanding and generation, our innovative multi-\\nstage training pipeline combines large-scale unsupervised pre-training with supervised\\nfine-tuning on high-quality datasets. Effective model merging strategies further ensure'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='stage training pipeline combines large-scale unsupervised pre-training with supervised\\nfine-tuning on high-quality datasets. Effective model merging strategies further ensure\\nthe robustness and adaptability of the Qwen3 Embedding series. During the training\\nprocess, the Qwen3 LLMs serve not only as backbone models but also play a crucial role\\nin synthesizing high-quality, rich, and diverse training data across multiple domains\\nand languages, thus enhancing the training pipeline. The Qwen3 Embedding series\\noffers a spectrum of model sizes (0.6B, 4B, 8B) for both embedding and reranking tasks,\\naddressing diverse deployment scenarios where users can optimize for either efficiency\\nor effectiveness. Empirical evaluations demonstrate that the Qwen3 Embedding series\\nachieves state-of-the-art results across diverse benchmarks. Notably, it excels on the\\nmultilingual evaluation benchmark MTEB for text embedding, as well as in various'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='achieves state-of-the-art results across diverse benchmarks. Notably, it excels on the\\nmultilingual evaluation benchmark MTEB for text embedding, as well as in various\\nretrieval tasks, including code retrieval, cross-lingual retrieval and multilingual retrieval.\\nTo facilitate reproducibility and promote community-driven research and development,\\nthe Qwen3 Embedding models are publicly available under the Apache 2.0 license.\\n1 Introduction\\nText embedding and reranking are fundamental components in numerous natural language pro-\\ncessing and information retrieval applications, including web search, question answering, recom-\\nmendation systems, and beyond (Karpukhin et al., 2020; Huang et al., 2020; Zhao et al., 2023; 2024).\\nHigh-quality embeddings enable models to capture semantic relationships between texts, while\\neffective reranking mechanisms ensure that the most relevant results are prioritized. Recently,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='High-quality embeddings enable models to capture semantic relationships between texts, while\\neffective reranking mechanisms ensure that the most relevant results are prioritized. Recently,\\nemerging application paradigms such as Retrieval-Augmented Generation (RAG) and agent sys-\\ntems, driven by the advancement of large language models (e.g., Qwen3 (Yang et al., 2025), GPT-4o\\n(Hurst et al., 2024)), have introduced new requirements and challenges for text embedding and\\nreranking, both in terms of model training paradigms and application scenarios. Despite significant\\nadvancements, training embedding and reranking models that perform well in scalability, contextual\\nunderstanding, and alignment with specific downstream tasks remains challenging.\\nThe emergence of large language models (LLMs) has significantly advanced the development of text\\nembedding and reranking models. Prior to the introduction of LLMs, the predominant approach\\n∗ Equal contribution\\n1'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\ninvolved using encoder-only pretrained language models like BERT as the foundational model\\nfor training (Reimers & Gurevych, 2019). The richer world knowledge, text understanding, and\\nreasoning abilities inherent in LLMs have led to further enhancements in models trained on these\\narchitectures. Additionally, there has been considerable research facilitating the integration of LLMs\\ninto processes such as training data synthesis and quality data filtering (Wang et al., 2024; Lee et al.,\\n2024; 2025b). The fundamental characteristics of LLMs have also inspired the introduction of new\\ntraining paradigms. For instance, during the embedding model training process, incorporating\\ndifferentiated tasks across aspects such as instruction type, domain, and language has yielded\\nimproved performance in downstream tasks (Su et al., 2023). Similarly, for reranking model training,\\nadvancements have been realized through both zero-shot methods based on user prompts and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='improved performance in downstream tasks (Su et al., 2023). Similarly, for reranking model training,\\nadvancements have been realized through both zero-shot methods based on user prompts and\\napproaches combining supervised fine-tuning (Ma et al., 2023; Pradeep et al., 2023; Zhang et al.,\\n2024a; Zhuang et al., 2024).\\nIn this work, we introduce the Qwen3 Embedding series models, which are constructed on top\\nof the Qwen3 foundation models. The Qwen3 foundation has simultaneously released base and\\ninstruct model versions, and we exploit the robust multilingual text understanding and generation\\ncapabilities of these models to fully realize their potential in training embedding and reranking\\nmodels. To train the embedding models, we implement a multi-stage training pipeline that involves\\nlarge-scale unsupervised pre-training followed by supervised fine tuning on high-quality datasets.\\nWe also employ model merging with various model checkpoints to enhance robustness and general-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='large-scale unsupervised pre-training followed by supervised fine tuning on high-quality datasets.\\nWe also employ model merging with various model checkpoints to enhance robustness and general-\\nization. The Qwen3 instruct model allows for efficient synthesis of a vast, high-quality, multilingual,\\nand multi-task text relevance dataset. This synthetic data is utilized in the initial unsupervised\\ntraining stage, while a subset of high-quality, small-scale data is selected for the second stage of\\nsupervised training. For the reranking models, we adopt a two-stage training scheme in a similar\\nmanner, consisting of high-quality supervised fine tuning and a model merging stage. Based on\\ndifferent sizes of the Qwen3 backbone models (including 0.6B, 4B, and 8B), we ultimately trained\\nthree text embedding models and three text reranking models. To facilitate their application in\\ndownstream tasks, the Qwen3 Embedding series supports several practical features, such as flexible'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='three text embedding models and three text reranking models. To facilitate their application in\\ndownstream tasks, the Qwen3 Embedding series supports several practical features, such as flexible\\ndimension representation for embedding models and customizable instructions for both embedding\\nand reranking models.\\nWe evaluate the Qwen3 Embedding series across a comprehensive set of benchmarks spanning\\nmultiple tasks and domains. Experimental results demonstrate that our embedding and reranking\\nmodels achieve state-of-the-art performance, performing competitively against leading proprietary\\nmodels in several retrieval tasks. For example, the flagship model Qwen3-8B-Embedding attains a\\nscore of 70.58 on the MTEB Multilingual benchmark (Enevoldsen et al., 2025) and 80.68 on the MTEB\\nCode benchmark (Enevoldsen et al., 2025), surpassing the previous state-of-the-art proprietary\\nembedding model, Gemini-Embedding (Lee et al., 2025b). Moreover, our reranking model delivers'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Code benchmark (Enevoldsen et al., 2025), surpassing the previous state-of-the-art proprietary\\nembedding model, Gemini-Embedding (Lee et al., 2025b). Moreover, our reranking model delivers\\ncompetitive results across a range of retrieval tasks. The Qwen3-Reranker-0.6B model exceeds\\npreviously top-performing models in numerous retrieval tasks, while the larger Qwen3-Reranker-8B\\nmodel demonstrates even superior performance, improving ranking results by 3.0 points over the\\n0.6B model across multiple tasks. Furthermore, we include a constructive ablation study to elucidate\\nthe key factors contributing to the superior performance of the Qwen3 Embedding series, providing\\ninsights into its effectiveness.\\nIn the following sections, we describe the design of the model architecture, detail the training\\nprocedures, present the experimental results for both the embedding and reranking models of the\\nQwen3 Embedding Series, and conclude this technical report by summarizing the key findings and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='procedures, present the experimental results for both the embedding and reranking models of the\\nQwen3 Embedding Series, and conclude this technical report by summarizing the key findings and\\noutlining potential directions for future research.\\n2 Model Architecture\\nThe core idea behind embedding and reranking models is to evaluate relevance in a task-aware\\nmanner. Given a query q and a document d, embedding and reranking models assess their relevance\\nbased on a similarity criterion defined by instruction I. To enable the models for task-aware rele-\\nvance estimation, training data is often organized as {Ii, qi, d+\\ni , d−\\ni,1, · · ·, d−\\ni,n}, where d+\\ni represents a\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nFigure 1: Model architecture of Qwen3-Embedding (left) and Qwen3-Reranker (right).\\npositive (relevant) document for query qi, and d−\\ni,j are negative (irrelevant) documents. Training the\\nmodel on diverse text pairs broadens its applicability to a range of downstream tasks, including\\nretrieval, semantic textual similarity, classification, and clustering.\\nArchitecture The Qwen3 embedding and reranking models are built on the dense version of\\nQwen3 foundation models and are available in three sizes: 0.6B, 4B, and 8B parameters. We initialize\\nthese models using the Qwen3 foundation models to leverage their capabilities in text modeling\\nand instruction following. The model layers, hidden size, and context length for each model\\nconfiguration are detailed in Table 1.\\nEmbedding Models For text embeddings, we utilize LLMs with causal attention, appending an\\n[EOS] token at the end of the input sequence. The final embedding is derived from the hidden state'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Embedding Models For text embeddings, we utilize LLMs with causal attention, appending an\\n[EOS] token at the end of the input sequence. The final embedding is derived from the hidden state\\nof the last layer corresponding to this [EOS] token.\\nTo ensure embeddings follow instructions during downstream tasks, we concatenate the instruction\\nand the query into a single input context, while leaving the document unchanged before processing\\nwith LLMs. The input format for queries is as follows:\\n{Instruction} {Query}<|endoftext|>\\nReranking Models To more accurately evaluate text similarity, we employ LLMs for point-wise\\nreranking within a single context. Similar to the embedding model, to enable instruction-following\\ncapability, we include the instruction in the input context. We use the LLM chat template and frame\\nthe similarity assessment task as a binary classification problem. The input to LLMs adheres to the\\ntemplate shown below:\\n<|im_start|>system'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='the similarity assessment task as a binary classification problem. The input to LLMs adheres to the\\ntemplate shown below:\\n<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the\\nInstruct provided. Note that the answer can only be \"yes\" or\\n\"no\".<|im_end|>\\n,→\\n,→\\n<|im_start|>user\\n<Instruct>: {Instruction}\\n<Query>: {Query}\\n<Document>: {Document}<|im_end|>\\n<|im_start|>assistant\\n<think>\\\\n\\\\n</think>\\\\n\\\\n\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nModel Type Models Size Layers Sequence\\nLength\\nEmbedding\\nDimension\\nMRL\\nSupport\\nInstruction\\nAware\\nText Embedding\\nQwen3-Embedding-0.6B0.6B 28 32K 1024 Yes Yes\\nQwen3-Embedding-4B 4B 36 32K 2560 Yes Yes\\nQwen3-Embedding-8B 8B 36 32K 4096 Yes Yes\\nText Reranking\\nQwen3-Reranker-0.6B 0.6B 28 32K - - Yes\\nQwen3-Reranker-4B 4B 36 32K - - Yes\\nQwen3-Reranker-8B 8B 36 32K - - Yes\\nTable 1: Model architecture of Qwen3 Embedding models. “MRL Support” indicates whether the\\nembedding model supports custom dimensions for the final embedding. “Instruction Aware” notes\\nwhether the embedding or reranker model supports customizing the input instruction according to\\ndifferent tasks.\\nFigure 2: Training pipeline of Qwen3 Embedding and Reranking models.\\nTo calculate the relevance score based on the given input, we assess the likelihood of the next token\\nbeing ”yes” or ”no.” This is expressed mathematically as follows:\\nscore(q, d) = eP(yes|I,q,d)\\neP(yes|I,q,d) + eP(no|I,q,d)\\n3 Models Training'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='being ”yes” or ”no.” This is expressed mathematically as follows:\\nscore(q, d) = eP(yes|I,q,d)\\neP(yes|I,q,d) + eP(no|I,q,d)\\n3 Models Training\\nIn this section, we describe the multi-stage training pipeline adopted and present the key elements of\\nthis training recipe, including training objective, training data synthesis, and filtering of high-quality\\ntraining data.\\n3.1 Training Objective\\nBefore introducing our training pipeline, we first outline the optimized loss functions used for the\\nembedding and reranking models during the training process. For the embedding model, we utilize\\nan improved contrastive loss based on the InfoNCE framework (Oord et al., 2018). Given a batch of\\nN training instances, the loss is defined as:\\nLembedding = − 1\\nN\\nN\\n∑\\ni\\nlog e(s(qi,d+\\ni )/τ)\\nZi\\n, (1)\\nwhere s(·, ·) is a similarity function (we use cosine similarity), τ is a temperature parameter, and Zi\\nis the normalization factor that aggregates the similarity scores of the positive pair against various'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='is the normalization factor that aggregates the similarity scores of the positive pair against various\\nnegative pairs:\\nZi = e(s(qi,d+\\ni )/τ) +\\nK\\n∑\\nk\\nmik e(s(qi,d−\\ni,k)/τ) + ∑\\nj̸=i\\nmij e(s(qi,qj)/τ) + ∑\\nj̸=i\\nmij e(s(d+\\ni ,dj)/τ) + ∑\\nj̸=i\\nmij e(s(qi,dj)/τ)\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nwhere these terms represent similarities with: (1) the positive document d+\\ni , (2) K hard negatives\\nd−\\ni,k, (3) other in-batch queries qj, (4) other in-batch documents dj compared against the positive\\ndocument d+\\ni . (5) other in-batch documents dj compared against the query qi. The mask factor mij is\\ndesigned to mitigate the impact of false negatives and is defined as:\\nmij =\\n(\\n0 if sij > s(qi, d+\\ni ) +0.1 or dj == d+\\ni ,\\n1 otherwise,\\namong which sij is the corresponding score of qi, dj or qi, qj.\\nFor the reranking model, we optimize the Supervised Fine-Tuning (SFT) loss defined as:\\nLreranking = −log p(l|P(q, d)), (2)\\nwhere p(·|∗) denotes the probability assigned by LLM. The label l is “yes” for positive documents\\nand “no” for negatives. This loss function encourages the model to assign higher probabilities to\\ncorrect labels, thereby improving the ranking performance.\\n3.2 Multi-stage Training'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='and “no” for negatives. This loss function encourages the model to assign higher probabilities to\\ncorrect labels, thereby improving the ranking performance.\\n3.2 Multi-stage Training\\nThe multi-stage training approach is a common practice for training text embedding models (Li et al.,\\n2023; Wang et al., 2022; Chen et al., 2024). This strategy typically begins with initial training on large-\\nscale, semi-supervised data that includes noise, followed by fine-tuning using smaller, high-quality\\nsupervised datasets. This two-step process enhances the performance and generalization capabilities\\nof embedding models. Large-scale weakly supervised training data contribute significantly to\\nthe model’s generalization, while fine-tuning with high-quality data in subsequent stages further\\nimproves model performance. Both stages of training for embedding models utilize the optimization\\nobjective defined in Equation 1, whereas the reranking model training employs the loss function'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='improves model performance. Both stages of training for embedding models utilize the optimization\\nobjective defined in Equation 1, whereas the reranking model training employs the loss function\\ndefined in Equation 2 as the optimization target.\\nBuilding upon the existing multi-stage training framework, the Qwen3 Embedding series introduces\\nthe following key innovations:\\n• Large-Scale Synthetic Data-Driven Weak Supervision Training: Unlike previous works (e.g.,\\nGTE, E5, BGE models), where weakly supervised training data are primarily collected from\\nopen-source communities such as Q&A forums or academic papers, we propose leveraging\\nthe text understanding and generation capabilities of foundation models to synthesize pair\\ndata directly. This approach allows for arbitrary definition of various dimensions of the\\ndesired pair data, such as task, language, length, and difficulty within the synthesis prompts.\\nCompared to data collection from open-domain sources, foundation model-driven data'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='desired pair data, such as task, language, length, and difficulty within the synthesis prompts.\\nCompared to data collection from open-domain sources, foundation model-driven data\\nsynthesis offers greater controllability, enabling precise management of the quality and\\ndiversity of the generated data, particularly in low-resource scenarios and languages.\\n• High-Quality Synthetic Data Utilization in Supervised Fine Tuning: Due to the exceptional\\nperformance of the Qwen3 Foundation model, the synthesized data is of notably high quality.\\nTherefore, in the second stage of supervised training, selective incorporation of this high-\\nquality synthetic data further enhances the overall model performance and generalization\\ncapabilities.\\n• Model Merging: Inspired by previous work (Li et al., 2024), after completing the supervised\\nfine-tuning, we applied a model merging technique based on spherical linear interpolation'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='capabilities.\\n• Model Merging: Inspired by previous work (Li et al., 2024), after completing the supervised\\nfine-tuning, we applied a model merging technique based on spherical linear interpolation\\n(slerp). This technique involves merging multiple model checkpoints saved during the\\nfine-tuning process. This step aims to boost the model’s robustness and generalization\\nperformance across various data distributions.\\nIt is important to note that the reranking model’s training process does not include a first-stage\\nweakly supervised training phase.\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\n3.3 Synthetic Dataset\\nTo create a robust synthetic dataset for training models on various similarity tasks, we generate\\ndiverse text pairs spanning categories such as retrieval, bitext mining, classification, and semantic\\ntextual similarity (STS). The quality of these synthetic data pairs is ensured by utilizing the Qwen3-\\n32B model as the foundational model for data synthesis. We have designed a diverse prompting\\nstrategy to improve the variety and authenticity of the generated data. For instance, in the text\\nretrieval task, we synthesize data using the multilingual pre-training corpus from Qwen3. During\\nthe data synthesis process, specific roles are assigned to each document to simulate potential\\nusers querying that document. This injection of user perspectives enhances the diversity and\\nrealism of the synthetic queries. Specifically, we utilize a retrieval model to identify the top five'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='users querying that document. This injection of user perspectives enhances the diversity and\\nrealism of the synthetic queries. Specifically, we utilize a retrieval model to identify the top five\\nrole candidates for each document from a role library and present these documents along with\\ntheir role candidates to the prompt. This guides the model in outputting the most suitable role\\nconfiguration for query generation. Moreover, the prompt incorporates various dimensions such as\\nquery type (e.g., keyword, factual, summary, judgment), query length, difficulty, and language. This\\nmultidimensional approach ensures the quality and diversity of the synthetic data.\\nFinally, we create a total of approximately 150 million pairs of multi-task weak supervision training\\ndata. Our experiments reveal that the embedding model trained with these synthetic data performs\\nexceptionally well in downstream evaluations, particularly surpassing many previously supervised'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='data. Our experiments reveal that the embedding model trained with these synthetic data performs\\nexceptionally well in downstream evaluations, particularly surpassing many previously supervised\\nmodels in the MTEB Multilingual benchmarks. This motivates us to filter the synthetic data to\\nidentify high-quality pairs for inclusion in a second stage of supervised training. We employ a\\nsimple cosine similarity calculation to select data pairs, retaining those with a cosine similarity\\ngreater than 0.7 from randomly sampled data. Ultimately, approximately 12 million high-quality\\nsupervised training data pairs are selected for further training.\\nModel Size Mean(Task)Mean(Type)BitextMiningClass-ificationClus-tering Inst.RetrievalMultilabelClass. PairClass.Rerank Retrieval STS\\nSelected Open-Source Models'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Model Size Mean(Task)Mean(Type)BitextMiningClass-ificationClus-tering Inst.RetrievalMultilabelClass. PairClass.Rerank Retrieval STS\\nSelected Open-Source Models\\nNV-Embed-v2 7B 56.29 49.58 57.84 57.29 40.80 1.04 18.63 78.94 63.82 56.72 71.10GritLM-7B 7B 60.92 53.74 70.53 61.83 49.75 3.45 22.77 79.94 63.78 58.31 73.33BGE-M3 0.6B 59.56 52.18 79.11 60.35 40.88 -3.11 20.1 80.76 62.79 54.60 74.12multilingual-e5-large-instruct 0.6B63.22 55.08 80.13 64.94 50.75 -0.40 22.91 80.86 62.61 57.12 76.81gte-Qwen2-1.5B-instruct 1.5B59.45 52.69 62.51 58.32 52.05 0.74 24.02 81.58 62.58 60.78 71.61gte-Qwen2-7b-Instruct 7B 62.51 55.93 73.92 61.55 52.77 4.94 25.48 85.13 65.55 60.08 73.98\\nCommercial APIs\\ntext-embedding-3-large - 58.93 51.41 62.17 60.27 46.89 -2.68 22.03 79.17 63.89 59.27 71.68Cohere-embed-multilingual-v3.0 -61.12 53.23 70.50 62.95 46.89 -1.89 22.74 79.88 64.07 59.16 74.80Gemini Embedding - 68.37 59.59 79.28 71.82 54.59 5.18 29.16 83.63 65.58 67.71 79.40\\nQwen3 Embedding Models'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Qwen3 Embedding Models\\nQwen3-Embedding-0.6B0.6B64.33 56.00 72.22 66.83 52.33 5.09 24.59 80.83 61.41 64.64 76.17Qwen3-Embedding-4B 4B 69.45 60.86 79.36 72.33 57.15 11.56 26.77 85.05 65.08 69.60 80.86Qwen3-Embedding-8B 8B 70.58 61.69 80.89 74.00 57.65 10.06 28.66 86.40 65.63 70.88 81.08\\nTable 2: Performance on MTEB Multilingual (Enevoldsen et al., 2025). For compared models, the\\nscores are retrieved from MTEB online leaderboard on June 4th, 2025.\\n4 Evaluation\\nWe conduct comprehensive and fair evaluations across multiple benchmarks to assess the capabilities\\nof Qwen3 Embedding models.\\n4.1 Settings\\nFor the text embedding models, we utilize the Massive Multilingual Text Embedding Benchmark\\n(MMTEB) (Enevoldsen et al., 2025) for evaluation. MMTEB is a large-scale, community-driven\\nexpansion of MTEB (Muennighoff et al., 2023), covering over 500 quality-controlled evaluation tasks\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nModel Size Dim MTEB (Eng, v2) CMTEB MTEB (Code)\\nMean (Task) Mean (Type)Mean (Task) Mean (Type)\\nSelected Open-Source Models\\nNV-Embed-v2 7B 4096 69.81 65.00 63.0 62.0 -\\nGritLM-7B 7B 4096 67.07 63.22 - - 73.6α\\nmultilingual-e5-large-instruct 0.6B1024 65.53 61.21 - - 65.0α\\ngte-Qwen2-1.5b-instruct 1.5B 1536 67.20 63.26 67.12 67.79 -\\ngte-Qwen2-7b-instruct 7B 3584 70.72 65.77 71.62 72.19 56.41γ\\nCommercial APIs\\ntext-embedding-3-large - 3072 66.43 62.15 - - 58.95γ\\ncohere-embed-multilingual-v3.0 -1024 66.01 61.43 - - 51.94γ\\nGemini Embedding - 3072 73.30 67.67 - - 74.66γ\\nQwen3 Embedding Models\\nQwen3-Embedding-0.6B 0.6B 1024 70.70 64.88 66.33 67.44 75.41\\nQwen3-Embedding-4B 4B 2560 74.60 68.09 72.26 73.50 80.06\\nQwen3-Embedding-8B 8B 4096 75.22 68.70 73.83 75.00 80.68\\nTable 3: Performance on MTEB Engilish, MTEB Chinese, MTEB Code. αTaken from (Enevoldsen\\net al., 2025). γTaken from (Lee et al., 2025b). For other compared models, the scores are retrieved'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Table 3: Performance on MTEB Engilish, MTEB Chinese, MTEB Code. αTaken from (Enevoldsen\\net al., 2025). γTaken from (Lee et al., 2025b). For other compared models, the scores are retrieved\\nfrom MTEB online leaderboard on June 4th, 2025.\\nacross more than 250 languages. In addition to classic text tasks such as as a variety of retrieval,\\nclassification, and semantic textual similarity, MMTEB includes a diverse set of challenging and\\nnovel tasks, such as instruction following, long-document retrieval, and code retrieval, representing\\nthe largest multilingual collection of evaluation tasks for embedding models to date. Our MMTEB\\nevaluations encompass 216 individual evaluation tasks, consisting of 131 tasks for MTEB (Multilin-\\ngual) (Enevoldsen et al., 2025), 41 tasks for MTEB (English, v2) (Muennighoff et al., 2023), 32 tasks\\nfor CMTEB (Xiao et al., 2024), and 12 code retrieval tasks for MTEB (Code) (Enevoldsen et al., 2025).'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='for CMTEB (Xiao et al., 2024), and 12 code retrieval tasks for MTEB (Code) (Enevoldsen et al., 2025).\\nMoreover, we select a series of text retrieval tasks to assess the text reranking capabilities of our\\nmodels. We explore three types of retrieval tasks: (1) Basic Relevance Retrieval, categorized into\\nEnglish, Chinese, and Multilingual, evaluated on MTEB (Muennighoff et al., 2023), CMTEB (Xiao\\net al., 2024), MMTEB (Enevoldsen et al., 2025), and MLDR (Chen et al., 2024), respectively; (2) Code\\nRetrieval, evaluated on MTEB-Code (Enevoldsen et al., 2025), which comprises only code-related\\nretrieval data.; and (3) Complex Instruction Retrieval, evaluated on FollowIR (Weller et al., 2024).\\nCompared Methods We compare our models with the most prominent open-source text embed-\\nding models and commercial API services. The open-source models include the GTE (Li et al.,\\n2023; Zhang et al., 2024b), E5 (Wang et al., 2022), and BGE (Xiao et al., 2024) series, as well as NV-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='ding models and commercial API services. The open-source models include the GTE (Li et al.,\\n2023; Zhang et al., 2024b), E5 (Wang et al., 2022), and BGE (Xiao et al., 2024) series, as well as NV-\\nEmbed-v2 (Lee et al., 2025a), GritLM-7B Muennighoff et al. (2025). The commercial APIs evaluated\\nare text-embedding-3-large from OpenAI, Gemini-embedding from Google, and Cohere-embed-\\nmultilingual-v3.0. For reranking, we compare with the rerankers of jina1, mGTE (Zhang et al., 2024b)\\nand BGE-m3 (Chen et al., 2024).\\n4.2 Main Results\\nEmbedding In Table 2, we present the evaluation results on MMTEB (Enevoldsen et al., 2025),\\nwhich comprehensively covers a wide range of embedding tasks across multiple languages. Our\\nQwen3-Embedding-4B/8B models achieve the best performance, and our smallest model, Qwen3-\\nEmbedding-0.6B, only lags behind the best-performing baseline method (Gemini-Embedding),\\ndespite having only 0.6B parameters. In Table 3, we present the evaluation results on MTEB (English,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Embedding-0.6B, only lags behind the best-performing baseline method (Gemini-Embedding),\\ndespite having only 0.6B parameters. In Table 3, we present the evaluation results on MTEB (English,\\nv2) (Muennighoff et al., 2023), CMTEB (Xiao et al., 2024), and MTEB (Code) (Enevoldsen et al.,\\n2025). The scores reflect similar trends as MMTEB, with our Qwen3-Embedding-4B/8B models\\n1https://hf.co/jinaai/jina-reranker-v2-base-multilingual\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nBasic Relevance Retrieval\\nModel Param MTEB-R CMTEB-R MMTEB-R MLDR MTEB-Code FollowIR\\nQwen3-Embedding-0.6B 0.6B 61.82 71.02 64.64 50.26 75.41 5.09\\nJina-multilingual-reranker-v2-base 0.3B 58.22 63.37 63.73 39.66 58.98 -0.68\\ngte-multilingual-reranker-base 0.3B 59.51 74.08 59.44 66.33 54.18 -1.64\\nBGE-reranker-v2-m3 0.6B 57.03 72.16 58.36 59.51 41.38 -0.01\\nQwen3-Reranker-0.6B 0.6B 65.80 71.31 66.36 67.28 73.42 5.41\\nQwen3-Reranker-4B 4B 69.76 75.94 72.74 69.97 81.20 14.84\\nQwen3-Reranker-8B 8B 69.02 77.45 72.94 70.19 81.22 8.05\\nTable 4: Evaluation results for reranking models. We use the retrieval subsets of MTEB(eng, v2),\\nMTEB(cmn, v1) and MMTEB, which are MTEB-R, CMTEB-R and MMTEM-R. The rest are all retrieval\\ntasks. All scores are our runs based on the retrieval top-100 results from the first row.\\nModel MMTEB MTEB (Eng, v2)CMTEB MTEB (Code, v1)\\nQwen3-Embedding-0.6B w/ only synthetic data58.49 60.63 59.78 66.79\\nQwen3-Embedding-0.6B w/o synthetic data 61.21 65.59 63.37 74.58'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Model MMTEB MTEB (Eng, v2)CMTEB MTEB (Code, v1)\\nQwen3-Embedding-0.6B w/ only synthetic data58.49 60.63 59.78 66.79\\nQwen3-Embedding-0.6B w/o synthetic data 61.21 65.59 63.37 74.58\\nQwen3-Embedding-0.6B w/o model merge 62.56 68.18 64.76 74.89\\nQwen3-Embedding-0.6B 64.33 70.70 66.33 75.41\\nTable 5: Performance (mean task) on MMTEB, MTEB(eng, v2), CMTEB and MTEB(code, v1) for\\nQwen3-Embedding-0.6B model with different training setting.\\nconsistently outperforming others. Notably, the Qwen3-Embedding-0.6B model ranks just behind\\nthe Gemini-Embedding, while being competitive with the gte-Qwen2-7B-instruct.\\nReranking In Table 4, we present the evaluation results on various reranking tasks ( §4.1). We\\nutilize the Qwen3-Embedding-0.6B model to retrieve the top-100 candidates and then apply different\\nreranking models for further refinement. This approach ensures a fair evaluation of the reranking\\nmodels. Our results indicate that all three Qwen3-Reranker models enhance performance compared'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='reranking models for further refinement. This approach ensures a fair evaluation of the reranking\\nmodels. Our results indicate that all three Qwen3-Reranker models enhance performance compared\\nto the embedding model and surpass all baseline reranking methods, with Qwen3-Reranker-8B\\nachieving the highest performance across most tasks.\\n4.3 Analysis\\nTo further analyze and explore the key elements of the Qwen3 Embedding model training framework,\\nwe conduct an analysis from the following dimensions:\\nEffectiveness of Large-Scale Weakly Supervised Pre-TrainingWe first analyze the effectiveness\\nof the large-scale weak supervised training stage for the embedding models. As shown in Table 5,\\nthe Qwen3-Embedding-0.6B model trained solely on synthetic data (without subsequent training\\nstages, as indicated in the first row) achieves reasonable and strong performance compared to the\\nfinal Qwen3-Embedding-0.6B model (as shown in the last row). If we further remove the weak'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='stages, as indicated in the first row) achieves reasonable and strong performance compared to the\\nfinal Qwen3-Embedding-0.6B model (as shown in the last row). If we further remove the weak\\nsupervised training stage (i.e., without synthetic data training, as seen in the second row), the final\\nperformance shows a clear decline. This indicates that the large-scale weak supervised training\\nstage is crucial for achieving superior performance.\\nEffectiveness of Model MergingNext, we compare the performance differences arising from the\\nmodel merging stage. As shown in Table 5, the model trained without model merging techniques\\n(the third row, which uses data sampling to balance various tasks) performs considerably worse\\nthan the final Qwen3-Embedding-0.6B model (which employs model merging, as shown in the last\\nrow). This indicates that the model merging stage is also critical for developing strong models.\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\n5 Conclusion\\nIn this technical report, we present the Qwen3-Embedding series, a comprehensive suite of text\\nembedding and reranking models based on the Qwen3 foundation models. These models are\\ndesigned to excel in a wide range of text embedding and reranking tasks, including multilingual\\nretrieval, code retrieval, and complex instruction following. The Qwen3-Embedding models are\\nbuilt upon a robust multi-stage training pipeline that combines large-scale weakly supervised\\npre-training on synthetic data with supervised fine-tuning and model merging on high-quality\\ndatasets. The Qwen3 LLMs play a crucial role in synthesizing diverse training data across multiple\\nlanguages and tasks, thereby enhancing the models’ capabilities. Our comprehensive evaluations\\ndemonstrate that the Qwen3-Embedding models achieve state-of-the-art performance across various\\nbenchmarks, including MTEB, CMTEB, MMTEB, and several retrieval benchmarks. We are pleased'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='demonstrate that the Qwen3-Embedding models achieve state-of-the-art performance across various\\nbenchmarks, including MTEB, CMTEB, MMTEB, and several retrieval benchmarks. We are pleased\\nto open-source the Qwen3-Embedding and Qwen3-Reranker models (0.6B, 4B, and 8B), making\\nthem available for the community to use and build upon.\\nReferences\\nJianlyu Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. M3-embedding:\\nMulti-linguality, multi-functionality, multi-granularity text embeddings through self-knowledge\\ndistillation. In Findings of the Association for Computational Linguistics: ACL 2024, pp. 2318–2335,\\nBangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://\\naclanthology.org/2024.findings-acl.137/.\\nKenneth Enevoldsen, Isaac Chung, Imene Kerboua, M´arton Kardos, Ashwin Mathur, David Stap,\\nJay Gala, Wissam Siblini, Dominik Krzemi ´nski, Genta Indra Winata, et al. MMTEB: Massive'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, M´arton Kardos, Ashwin Mathur, David Stap,\\nJay Gala, Wissam Siblini, Dominik Krzemi ´nski, Genta Indra Winata, et al. MMTEB: Massive\\nmultilingual text embedding benchmark. In The Thirteenth International Conference on Learning\\nRepresentations, 2025. URL https://openreview.net/forum?id=zl3pfz4VCV.\\nTao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. Scaling synthetic data creation\\nwith 1,000,000,000 personas. arXiv preprint arXiv:2406.20094, 2024.\\nJui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padman-\\nabhan, Giuseppe Ottaviano, and Linjun Yang. Embedding-based retrieval in facebook search. In\\nProceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,\\npp. 2553–2561, 2020.\\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark,\\nAJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark,\\nAJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint\\narXiv:2410.21276, 2024.\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick SH Lewis, Ledell Wu, Sergey Edunov, Danqi\\nChen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In EMNLP\\n(1), pp. 6769–6781, 2020.\\nChankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro,\\nand Wei Ping. Nv-embed: Improved techniques for training llms as generalist embedding models.\\narXiv preprint arXiv:2405.17428, 2024.\\nChankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro,\\nand Wei Ping. NV-embed: Improved techniques for training LLMs as generalist embedding\\nmodels. In The Thirteenth International Conference on Learning Representations, 2025a. URL https:\\n//openreview.net/forum?id=lgsyLSsDRe.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='models. In The Thirteenth International Conference on Learning Representations, 2025a. URL https:\\n//openreview.net/forum?id=lgsyLSsDRe.\\nJinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Shanbhogue, Iftekhar Naim, Gus-\\ntavo Hern´andez ´Abrego, Zhe Li, Kaifeng Chen, Henrique Schechter Vera, et al. Gemini embedding:\\nGeneralizable embeddings from gemini. arXiv preprint arXiv:2503.07891, 2025b.\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nMingxin Li, Zhijie Nie, Yanzhao Zhang, Dingkun Long, Richong Zhang, and Pengjun Xie. Improving\\ngeneral text embedding model: Tackling task conflict and data imbalance through model merging.\\narXiv preprint arXiv:2410.15035, 2024.\\nZehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards\\ngeneral text embeddings with multi-stage contrastive learning, 2023. URL https://arxiv.org/\\nabs/2308.03281.\\nXueguang Ma, Xinyu Zhang, Ronak Pradeep, and Jimmy Lin. Zero-shot listwise document reranking\\nwith a large language model. arXiv preprint arXiv:2305.02156, 2023.\\nNiklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. MTEB: Massive text embed-\\nding benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for\\nComputational Linguistics, pp. 2014–2037, Dubrovnik, Croatia, May 2023. Association for Computa-\\ntional Linguistics. URL https://aclanthology.org/2023.eacl-main.148/.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Computational Linguistics, pp. 2014–2037, Dubrovnik, Croatia, May 2023. Association for Computa-\\ntional Linguistics. URL https://aclanthology.org/2023.eacl-main.148/.\\nNiklas Muennighoff, Hongjin SU, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and\\nDouwe Kiela. Generative representational instruction tuning. In The Thirteenth International Con-\\nference on Learning Representations, 2025. URL https://openreview.net/forum?id=BC4lIvfSzv.\\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\\ntive coding. arXiv preprint arXiv:1807.03748, 2018.\\nRonak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. Rankvicuna: Zero-shot listwise docu-\\nment reranking with open-source large language models. arXiv preprint arXiv:2309.15088, 2023.\\nNils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-\\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-\\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\\nand the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp.\\n3982–3992, Hong Kong, China, November 2019. Association for Computational Linguistics. URL\\nhttps://aclanthology.org/D19-1410/.\\nHongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih,\\nNoah A Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-finetuned text\\nembeddings. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 1102–1121,\\n2023.\\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\\nand Furu Wei. Text embeddings by weakly-supervised contrastive pre-training, 2022. URL\\nhttps://arxiv.org/abs/2212.03533.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training, 2022. URL\\nhttps://arxiv.org/abs/2212.03533.\\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Im-\\nproving text embeddings with large language models. In Proceedings of the 62nd Annual Meet-\\ning of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 11897–11916,\\nBangkok, Thailand, August 2024. Association for Computational Linguistics. URL https:\\n//aclanthology.org/2024.acl-long.642/.\\nOrion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme,\\nDawn Lawrie, and Luca Soldaini. Followir: Evaluating and teaching information retrieval models\\nto follow instructions. arXiv preprint arXiv:2403.15246, 2024.\\nShitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. C-pack:\\nPacked resources for general chinese embeddings. In Proceedings of the 47th International ACM'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. C-pack:\\nPacked resources for general chinese embeddings. In Proceedings of the 47th International ACM\\nSIGIR Conference on Research and Development in Information Retrieval, SIGIR ’24, pp. 641–649, New\\nYork, NY, USA, 2024. Association for Computing Machinery. URLhttps://doi.org/10.1145/\\n3626772.3657878.\\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\\nChengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025.\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nLonghui Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, and Min Zhang. A\\ntwo-stage adaptation of large language models for text ranking. In Findings of the Association for\\nComputational Linguistics ACL 2024, pp. 11880–11891, 2024a.\\nXin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong Tang, Huan Lin, Baosong\\nYang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min Zhang. mGTE: Generalized\\nlong-context text representation and reranking models for multilingual text retrieval. In Franck\\nDernoncourt, Daniel Preo t ¸iuc-Pietro, and Anastasia Shimorina (eds.), Proceedings of the 2024\\nConference on Empirical Methods in Natural Language Processing: Industry Track, pp. 1393–1412,\\nMiami, Florida, US, November 2024b. Association for Computational Linguistics. doi: 10.18653/\\nv1/2024.emnlp-industry.103. URL https://aclanthology.org/2024.emnlp-industry.103/.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Miami, Florida, US, November 2024b. Association for Computational Linguistics. doi: 10.18653/\\nv1/2024.emnlp-industry.103. URL https://aclanthology.org/2024.emnlp-industry.103/.\\nWayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji-Rong Wen. Dense text retrieval based on pretrained\\nlanguage models: A survey. ACM Transactions on Information Systems, 42(4):1–60, 2024.\\nXiangyu Zhao, Maolin Wang, Xinjian Zhao, Jiansheng Li, Shucheng Zhou, Dawei Yin, Qing Li,\\nJiliang Tang, and Ruocheng Guo. Embedding in recommender systems: A survey. arXiv preprint\\narXiv:2310.18608, 2023.\\nShengyao Zhuang, Honglei Zhuang, Bevan Koopman, and Guido Zuccon. A setwise approach for\\neffective and highly efficient zero-shot ranking with large language models. In Proceedings of the\\n47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp.\\n38–47, 2024.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nA Appendix\\nA.1 Synthetic Data\\nWe construct four types of synthetic data—retrieval, bitext mining, semantic textual similarity, and\\nclassification to enable the model to adapt to various similarity tasks during pre-training. To ensure\\nboth multilingual and cross-lingual diversity, the data is generated using Qwen3 32B. Below is an\\nexample of a synthetic retrieval text pair. The retrieval data is synthesized using a document-to-\\nquery approach. We collect a multilingual corpus from the pre-training corpus of the Qwen3 base\\nmodel to serve as the document source. A two-stage generation pipeline is then applied, consisting\\nof: (1) configuration and (2) query generation. In the configuration stage, we use large language\\nmodels (LLMs) to determine the “Question Type”, “Difficulty”, and “Character” for the synthetic\\nquery. The candidate characters are retrieved from Persona Hub (Ge et al., 2024), selecting the top'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='models (LLMs) to determine the “Question Type”, “Difficulty”, and “Character” for the synthetic\\nquery. The candidate characters are retrieved from Persona Hub (Ge et al., 2024), selecting the top\\nfive most relevant to the given document. This step aims to enhance the diversity of the generated\\nqueries. The template used is as follows:\\nGiven a **Passage** and **Character**, select the appropriate option from\\nthree fields: Character, Question_Type, Difficulty, and return the output\\nin JSON format.\\n,→\\n,→\\nFirst, select the Character who are likely to be interested in the Passage\\nfrom the candidates. Then select the Question_Type that the Character\\nmight ask about the Passage; Finally, choose the Difficulty of the\\npossible question based on the Passage, the Character, and the\\nQuestion_Type.\\n,→\\n,→\\n,→\\n,→\\nCharacter: Given by input **Character**\\nQuestion_Type:\\n- keywords: ...\\n- acquire_knowledge: ...\\n- summary: ...\\n- yes_or_no: ...\\n- background: ...\\nDifficulty:\\n- high_school: ...'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content=',→\\n,→\\n,→\\n,→\\nCharacter: Given by input **Character**\\nQuestion_Type:\\n- keywords: ...\\n- acquire_knowledge: ...\\n- summary: ...\\n- yes_or_no: ...\\n- background: ...\\nDifficulty:\\n- high_school: ...\\n- university: ...\\n- phd: ...\\nHere are some examples\\n<Example1> <Example2> <Example3>\\nNow, generate the **output** based on the **Passage** and **Character** from\\nuser, the **Passage** will be in {language} language and the **Character**\\nwill be in English.\\n,→\\n,→\\nEnsure to generate only the JSON output with content in English.\\n**Passage**:\\n{passage}\\n**Character**:\\n{character}\\nIn the query generation stage, we use the configuration selected in the first stage to guide the\\ngeneration of queries. Additionally, we explicitly specify the desired length and language of the\\ngenerated query. The template used is as follows:\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content=\"Technical Report\\nGiven a **Character**, **Passage**, and **Requirement**, generate a query from\\nthe **Character**'s perspective that satisfies the **Requirement** and can\\nbe used to retrieve the **Passage**. Please return the result in JSON\\nformat.\\n,→\\n,→\\n,→\\nHere is an example:\\n<example>\\nNow, generate the **output** based on the **Character**, **Passage** and\\n**Requirement** from user, the **Passage** will be in {corpus_language}\\nlanguage, the **Character** and **Requirement** will be in English.\\n,→\\n,→\\nEnsure to generate only the JSON output, with the key in English and the value\\nin {queries_language} language.,→\\n**Character**\\n{character}\\n**Passage**\\n{passage}\\n**Requirment**\\n- Type: {type};\\n- Difficulty: {difficulty};\\n- Length: the length of the generated sentences should be {length} words;\\n- Languange: the language in which the results are generated should be\\n{language} language;,→\\nStage Dataset Size\\nWeakly Supervised Pre-Training Synthetic Data ∼ 150M\\nSupervised Fine Tuning\"),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='- Languange: the language in which the results are generated should be\\n{language} language;,→\\nStage Dataset Size\\nWeakly Supervised Pre-Training Synthetic Data ∼ 150M\\nSupervised Fine Tuning\\nMS MARCO, NQ, HotpotQA, NLI,\\nDureader, T2-Ranking, SimCLUE,\\nMIRACL, MLDR, Mr.TyDi,\\nMulti-CPR, CodeSearchNet .etc\\n+ High-quality Synthetic Data\\nLabeled Data: ∼ 7M\\nSynthetic Data: ∼ 12M\\nTable 6: Statistics of training data utilized at each stage.\\nA.2 Detail Results\\nMTEB(eng, v2) Param Mean\\n(Task)\\nMean\\n(Type)\\nClass-\\nification\\nClus-\\ntering\\nPair\\nClass. Rerank Retrieval STS Summ.\\nmultilingual-e5-large-instruct 0.6B 65.53 61.21 75.54 49.89 86.24 48.74 53.47 84.72 29.89\\nNV-Embed-v2 7.8B 69.81 65.00 87.19 47.66 88.69 49.61 62.84 83.82 35.21\\nGritLM-7B 7.2B 67.07 63.22 81.25 50.82 87.29 49.59 54.95 83.03 35.65\\ngte-Qwen2-1.5B-instruct 1.5B 67.20 63.26 85.84 53.54 87.52 49.25 50.25 82.51 33.94\\nstellaen1.5Bv5 1.5B 69.43 65.32 89.38 57.06 88.02 50.19 52.42 83.27 36.91'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='gte-Qwen2-1.5B-instruct 1.5B 67.20 63.26 85.84 53.54 87.52 49.25 50.25 82.51 33.94\\nstellaen1.5Bv5 1.5B 69.43 65.32 89.38 57.06 88.02 50.19 52.42 83.27 36.91\\ngte-Qwen2-7B-instruct 7.6B 70.72 65.77 88.52 58.97 85.9 50.47 58.09 82.69 35.74\\ngemini-embedding-exp-03-07 - 73.3 67.67 90.05 59.39 87.7 48.59 64.35 85.29 38.28\\nQwen3-Embedding-0.6B 0.6B 70.70 64.88 85.76 54.05 84.37 48.18 61.83 86.57 33.43\\nQwen3-Embedding-4B 4B 74.60 68.09 89.84 57.51 87.01 50.76 68.46 88.72 34.39\\nQwen3-Embedding-8B 8B 75.22 68.70 90.43 58.57 87.52 51.56 69.44 88.58 34.83\\nTable 7: Results on MTEB(eng, v2) (Muennighoff et al., 2023). We compare models from the online\\nleaderboard.\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nMTEB(cmn, v1) Param Mean\\n(Task)\\nMean\\n(Type)\\nClass-\\nification\\nClus-\\ntering\\nPair\\nClass. Rerank Retrieval STS\\nmultilingual-e5-large-instruct 0.6B 58.08 58.24 69.80 48.23 64.52 57.45 63.65 45.81\\ngte-Qwen2-7B-instruct 7.6B 71.62 72.19 75.77 66.06 81.16 69.24 75.70 65.20\\ngte-Qwen2-1.5B-instruct 1.5B 67.12 67.79 72.53 54.61 79.5 68.21 71.86 60.05\\nQwen3-Embedding-0.6B 0.6B 66.33 67.44 71.40 68.74 76.42 62.58 71.03 54.52\\nQwen3-Embedding-4B 4B 72.26 73.50 75.46 77.89 83.34 66.05 77.03 61.26\\nQwen3-Embedding-8B 8B 73.84 75.00 76.97 80.08 84.23 66.99 78.21 63.53\\nTable 8: Results on C-MTEB (Xiao et al., 2024) (MTEB(cmn, v1).\\nMTEB(Code, v1) Avg.Apps COIR-CodeSearch-Net\\nCode-Edit-Search\\nCode-Feedback-MT\\nCode-Feedback-ST\\nCode-SearchNet-CCR\\nCode-SearchNet\\nCode-Trans-Ocean-Contest\\nCode-Trans-Ocean-DLCosQAStack-Overflow-QA\\nSynthetic-Text2SQL'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Code-Edit-Search\\nCode-Feedback-MT\\nCode-Feedback-ST\\nCode-SearchNet-CCR\\nCode-SearchNet\\nCode-Trans-Ocean-Contest\\nCode-Trans-Ocean-DLCosQAStack-Overflow-QA\\nSynthetic-Text2SQL\\nBGEmultilingual 62.0422.93 68.14 60.48 60.52 76.70 73.23 83.43 86.84 32.64 27.93 92.93 58.67NV-Embed-v2 63.7429.72 61.85 73.96 60.27 81.72 68.82 86.61 89.14 33.40 34.82 92.36 60.90gte-Qwen2-7B-instruct 62.1728.39 71.79 67.06 57.66 85.15 66.24 86.96 81.83 32.17 31.26 84.34 53.22gte-Qwen2-1.5B-instruct 61.9828.91 71.56 59.60 49.92 81.92 72.08 91.08 79.02 32.73 32.23 90.27 54.49\\nBGE-M3 (Dense) 58.2214.77 58.07 59.83 47.86 69.27 53.55 61.98 86.22 29.37 27.36 80.71 49.65Jina-v3 58.85 28.99 67.83 57.24 59.66 78.13 54.17 85.50 77.37 30.91 35.15 90.79 41.49'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='BGE-M3 (Dense) 58.2214.77 58.07 59.83 47.86 69.27 53.55 61.98 86.22 29.37 27.36 80.71 49.65Jina-v3 58.85 28.99 67.83 57.24 59.66 78.13 54.17 85.50 77.37 30.91 35.15 90.79 41.49\\nQwen3-Embedding-0.6B 75.4175.34 84.69 64.42 90.82 86.39 91.72 91.01 86.05 31.36 36.48 89.99 76.74Qwen3-Embedding-4B 80.0689.18 87.93 76.49 93.21 89.51 95.59 92.34 90.99 35.04 37.98 94.32 78.21Qwen3-Embedding-8B 80.6891.07 89.51 76.97 93.70 89.93 96.35 92.66 93.73 32.81 38.04 94.75 78.75\\nQwen3-Reranker-0.6B 73.4269.43 85.09 72.37 83.83 78.05 94.76 88.8 84.69 33.94 36.83 93.24 62.48Qwen3-Reranker-4B 81.2094.25 90.91 82.53 95.25 88.54 97.58 92.48 93.66 36.78 35.14 97.11 75.06Qwen3-Reranker-8B 81.2294.55 91.88 84.58 95.64 88.43 95.67 92.78 90.83 34.89 37.43 97.3 73.4\\nTable 9: Performance on MTEB(Code, v1) (Enevoldsen et al., 2025). We report nDCG@10 scores.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='1\\nRetrieval-Augmented Generation for Large\\nLanguage Models: A Survey\\nYunfan Gaoa, Yun Xiongb, Xinyu Gao b, Kangxiang Jia b, Jinliu Pan b, Yuxi Bic, Yi Dai a, Jiawei Sun a, Meng\\nWangc, and Haofen Wang a,c\\naShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\\nbShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\ncCollege of Design and Innovation, Tongji University\\nAbstract—Large Language Models (LLMs) showcase impres-\\nsive capabilities but encounter challenges like hallucination,\\noutdated knowledge, and non-transparent, untraceable reasoning\\nprocesses. Retrieval-Augmented Generation (RAG) has emerged\\nas a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the\\ngeneration, particularly for knowledge-intensive tasks, and allows\\nfor continuous knowledge updates and integration of domain-\\nspecific information. RAG synergistically merges LLMs’ intrin-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='generation, particularly for knowledge-intensive tasks, and allows\\nfor continuous knowledge updates and integration of domain-\\nspecific information. RAG synergistically merges LLMs’ intrin-\\nsic knowledge with the vast, dynamic repositories of external\\ndatabases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing\\nthe Naive RAG, the Advanced RAG, and the Modular RAG.\\nIt meticulously scrutinizes the tripartite foundation of RAG\\nframeworks, which includes the retrieval, the generation and the\\naugmentation techniques. The paper highlights the state-of-the-\\nart technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG\\nsystems. Furthermore, this paper introduces up-to-date evalua-\\ntion framework and benchmark. At the end, this article delineates\\nthe challenges currently faced and points out prospective avenues\\nfor research and development 1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='tion framework and benchmark. At the end, this article delineates\\nthe challenges currently faced and points out prospective avenues\\nfor research and development 1.\\nIndex Terms—Large language model, retrieval-augmented gen-\\neration, natural language processing, information retrieval\\nI. I NTRODUCTION\\nL\\nARGE language models (LLMs) have achieved remark-\\nable success, though they still face significant limitations,\\nespecially in domain-specific or knowledge-intensive tasks [1],\\nnotably producing “hallucinations” [2] when handling queries\\nbeyond their training data or requiring current information. To\\novercome challenges, Retrieval-Augmented Generation (RAG)\\nenhances LLMs by retrieving relevant document chunks from\\nexternal knowledge base through semantic similarity calcu-\\nlation. By referencing external knowledge, RAG effectively\\nreduces the problem of generating factually incorrect content.\\nIts integration into LLMs has resulted in widespread adoption,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='lation. By referencing external knowledge, RAG effectively\\nreduces the problem of generating factually incorrect content.\\nIts integration into LLMs has resulted in widespread adoption,\\nestablishing RAG as a key technology in advancing chatbots\\nand enhancing the suitability of LLMs for real-world applica-\\ntions.\\nRAG technology has rapidly developed in recent years, and\\nthe technology tree summarizing related research is shown\\nCorresponding Author.Email:haofen.wang@tongji.edu.cn\\n1Resources are available at https://github.com/Tongji-KGLLM/\\nRAG-Survey\\nin Figure 1. The development trajectory of RAG in the era\\nof large models exhibits several distinct stage characteristics.\\nInitially, RAG’s inception coincided with the rise of the\\nTransformer architecture, focusing on enhancing language\\nmodels by incorporating additional knowledge through Pre-\\nTraining Models (PTM). This early stage was characterized\\nby foundational work aimed at refining pre-training techniques'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='models by incorporating additional knowledge through Pre-\\nTraining Models (PTM). This early stage was characterized\\nby foundational work aimed at refining pre-training techniques\\n[3]–[5].The subsequent arrival of ChatGPT [6] marked a\\npivotal moment, with LLM demonstrating powerful in context\\nlearning (ICL) capabilities. RAG research shifted towards\\nproviding better information for LLMs to answer more com-\\nplex and knowledge-intensive tasks during the inference stage,\\nleading to rapid development in RAG studies. As research\\nprogressed, the enhancement of RAG was no longer limited\\nto the inference stage but began to incorporate more with LLM\\nfine-tuning techniques.\\nThe burgeoning field of RAG has experienced swift growth,\\nyet it has not been accompanied by a systematic synthesis that\\ncould clarify its broader trajectory. This survey endeavors to\\nfill this gap by mapping out the RAG process and charting\\nits evolution and anticipated future paths, with a focus on the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='could clarify its broader trajectory. This survey endeavors to\\nfill this gap by mapping out the RAG process and charting\\nits evolution and anticipated future paths, with a focus on the\\nintegration of RAG within LLMs. This paper considers both\\ntechnical paradigms and research methods, summarizing three\\nmain research paradigms from over 100 RAG studies, and\\nanalyzing key technologies in the core stages of “Retrieval,”\\n“Generation,” and “Augmentation.” On the other hand, current\\nresearch tends to focus more on methods, lacking analysis and\\nsummarization of how to evaluate RAG. This paper compre-\\nhensively reviews the downstream tasks, datasets, benchmarks,\\nand evaluation methods applicable to RAG. Overall, this\\npaper sets out to meticulously compile and categorize the\\nfoundational technical concepts, historical progression, and\\nthe spectrum of RAG methodologies and applications that\\nhave emerged post-LLMs. It is designed to equip readers and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='foundational technical concepts, historical progression, and\\nthe spectrum of RAG methodologies and applications that\\nhave emerged post-LLMs. It is designed to equip readers and\\nprofessionals with a detailed and structured understanding of\\nboth large models and RAG. It aims to illuminate the evolution\\nof retrieval augmentation techniques, assess the strengths and\\nweaknesses of various approaches in their respective contexts,\\nand speculate on upcoming trends and innovations.\\nOur contributions are as follows:\\n• In this survey, we present a thorough and systematic\\nreview of the state-of-the-art RAG methods, delineating\\nits evolution through paradigms including naive RAG,\\narXiv:2312.10997v5  [cs.CL]  27 Mar 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='2\\nFig. 1. Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs,\\nresearch on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent\\nresearch has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models\\nin the pre-training stage through retrieval-augmented techniques.\\nadvanced RAG, and modular RAG. This review contex-\\ntualizes the broader scope of RAG research within the\\nlandscape of LLMs.\\n• We identify and discuss the central technologies integral\\nto the RAG process, specifically focusing on the aspects\\nof “Retrieval”, “Generation” and “Augmentation”, and\\ndelve into their synergies, elucidating how these com-\\nponents intricately collaborate to form a cohesive and\\neffective RAG framework.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='of “Retrieval”, “Generation” and “Augmentation”, and\\ndelve into their synergies, elucidating how these com-\\nponents intricately collaborate to form a cohesive and\\neffective RAG framework.\\n• We have summarized the current assessment methods of\\nRAG, covering 26 tasks, nearly 50 datasets, outlining\\nthe evaluation objectives and metrics, as well as the\\ncurrent evaluation benchmarks and tools. Additionally,\\nwe anticipate future directions for RAG, emphasizing\\npotential enhancements to tackle current challenges.\\nThe paper unfolds as follows: Section II introduces the\\nmain concept and current paradigms of RAG. The following\\nthree sections explore core components—“Retrieval”, “Gen-\\neration” and “Augmentation”, respectively. Section III focuses\\non optimization methods in retrieval,including indexing, query\\nand embedding optimization. Section IV concentrates on post-\\nretrieval process and LLM fine-tuning in generation. Section V\\nanalyzes the three augmentation processes. Section VI focuses'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='and embedding optimization. Section IV concentrates on post-\\nretrieval process and LLM fine-tuning in generation. Section V\\nanalyzes the three augmentation processes. Section VI focuses\\non RAG’s downstream tasks and evaluation system. Sec-\\ntion VII mainly discusses the challenges that RAG currently\\nfaces and its future development directions. At last, the paper\\nconcludes in Section VIII.\\nII. O VERVIEW OF RAG\\nA typical application of RAG is illustrated in Figure 2.\\nHere, a user poses a question to ChatGPT about a recent,\\nwidely discussed news. Given ChatGPT’s reliance on pre-\\ntraining data, it initially lacks the capacity to provide up-\\ndates on recent developments. RAG bridges this information\\ngap by sourcing and incorporating knowledge from external\\ndatabases. In this case, it gathers relevant news articles related\\nto the user’s query. These articles, combined with the original\\nquestion, form a comprehensive prompt that empowers LLMs\\nto generate a well-informed answer.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='to the user’s query. These articles, combined with the original\\nquestion, form a comprehensive prompt that empowers LLMs\\nto generate a well-informed answer.\\nThe RAG research paradigm is continuously evolving, and\\nwe categorize it into three stages: Naive RAG, Advanced\\nRAG, and Modular RAG, as showed in Figure 3. Despite\\nRAG method are cost-effective and surpass the performance\\nof the native LLM, they also exhibit several limitations.\\nThe development of Advanced RAG and Modular RAG is\\na response to these specific shortcomings in Naive RAG.\\nA. Naive RAG\\nThe Naive RAG research paradigm represents the earli-\\nest methodology, which gained prominence shortly after the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='3\\nFig. 2. A representative instance of the RAG process applied to question answering. It mainly consists of 3 steps. 1) Indexing. Documents are split into chunks,\\nencoded into vectors, and stored in a vector database. 2) Retrieval. Retrieve the Top k chunks most relevant to the question based on semantic similarity. 3)\\nGeneration. Input the original question and the retrieved chunks together into LLM to generate the final answer.\\nwidespread adoption of ChatGPT. The Naive RAG follows\\na traditional process that includes indexing, retrieval, and\\ngeneration, which is also characterized as a “Retrieve-Read”\\nframework [7].\\nIndexing starts with the cleaning and extraction of raw data\\nin diverse formats like PDF, HTML, Word, and Markdown,\\nwhich is then converted into a uniform plain text format. To\\naccommodate the context limitations of language models, text\\nis segmented into smaller, digestible chunks. Chunks are then\\nencoded into vector representations using an embedding model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='accommodate the context limitations of language models, text\\nis segmented into smaller, digestible chunks. Chunks are then\\nencoded into vector representations using an embedding model\\nand stored in vector database. This step is crucial for enabling\\nefficient similarity searches in the subsequent retrieval phase.\\nRetrieval. Upon receipt of a user query, the RAG system\\nemploys the same encoding model utilized during the indexing\\nphase to transform the query into a vector representation.\\nIt then computes the similarity scores between the query\\nvector and the vector of chunks within the indexed corpus.\\nThe system prioritizes and retrieves the top K chunks that\\ndemonstrate the greatest similarity to the query. These chunks\\nare subsequently used as the expanded context in prompt.\\nGeneration. The posed query and selected documents are\\nsynthesized into a coherent prompt to which a large language\\nmodel is tasked with formulating a response. The model’s'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='Generation. The posed query and selected documents are\\nsynthesized into a coherent prompt to which a large language\\nmodel is tasked with formulating a response. The model’s\\napproach to answering may vary depending on task-specific\\ncriteria, allowing it to either draw upon its inherent parametric\\nknowledge or restrict its responses to the information con-\\ntained within the provided documents. In cases of ongoing\\ndialogues, any existing conversational history can be integrated\\ninto the prompt, enabling the model to engage in multi-turn\\ndialogue interactions effectively.\\nHowever, Naive RAG encounters notable drawbacks:\\nRetrieval Challenges . The retrieval phase often struggles\\nwith precision and recall, leading to the selection of misaligned\\nor irrelevant chunks, and the missing of crucial information.\\nGeneration Difficulties. In generating responses, the model\\nmay face the issue of hallucination, where it produces con-\\ntent not supported by the retrieved context. This phase can'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='Generation Difficulties. In generating responses, the model\\nmay face the issue of hallucination, where it produces con-\\ntent not supported by the retrieved context. This phase can\\nalso suffer from irrelevance, toxicity, or bias in the outputs,\\ndetracting from the quality and reliability of the responses.\\nAugmentation Hurdles . Integrating retrieved information\\nwith the different task can be challenging, sometimes resulting\\nin disjointed or incoherent outputs. The process may also\\nencounter redundancy when similar information is retrieved\\nfrom multiple sources, leading to repetitive responses. Deter-\\nmining the significance and relevance of various passages and\\nensuring stylistic and tonal consistency add further complexity.\\nFacing complex issues, a single retrieval based on the original\\nquery may not suffice to acquire adequate context information.\\nMoreover, there’s a concern that generation models might\\noverly rely on augmented information, leading to outputs that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='query may not suffice to acquire adequate context information.\\nMoreover, there’s a concern that generation models might\\noverly rely on augmented information, leading to outputs that\\nsimply echo retrieved content without adding insightful or\\nsynthesized information.\\nB. Advanced RAG\\nAdvanced RAG introduces specific improvements to over-\\ncome the limitations of Naive RAG. Focusing on enhancing re-\\ntrieval quality, it employs pre-retrieval and post-retrieval strate-\\ngies. To tackle the indexing issues, Advanced RAG refines\\nits indexing techniques through the use of a sliding window\\napproach, fine-grained segmentation, and the incorporation of\\nmetadata. Additionally, it incorporates several optimization\\nmethods to streamline the retrieval process [8].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='4\\nFig. 3. Comparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)\\nAdvanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a\\nchain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the\\nintroduction of multiple specific functional modules and the replacement of existing modules. The overall process is not limited to sequential retrieval and\\ngeneration; it includes methods such as iterative and adaptive retrieval.\\nPre-retrieval process. In this stage, the primary focus is\\non optimizing the indexing structure and the original query.\\nThe goal of optimizing indexing is to enhance the quality of\\nthe content being indexed. This involves strategies: enhancing\\ndata granularity, optimizing index structures, adding metadata,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='The goal of optimizing indexing is to enhance the quality of\\nthe content being indexed. This involves strategies: enhancing\\ndata granularity, optimizing index structures, adding metadata,\\nalignment optimization, and mixed retrieval. While the goal\\nof query optimization is to make the user’s original question\\nclearer and more suitable for the retrieval task. Common\\nmethods include query rewriting query transformation, query\\nexpansion and other techniques [7], [9]–[11].\\nPost-Retrieval Process. Once relevant context is retrieved,\\nit’s crucial to integrate it effectively with the query. The main\\nmethods in post-retrieval process include rerank chunks and\\ncontext compressing. Re-ranking the retrieved information to\\nrelocate the most relevant content to the edges of the prompt is\\na key strategy. This concept has been implemented in frame-\\nworks such as LlamaIndex 2, LangChain3, and HayStack [12].\\nFeeding all relevant documents directly into LLMs can lead'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='a key strategy. This concept has been implemented in frame-\\nworks such as LlamaIndex 2, LangChain3, and HayStack [12].\\nFeeding all relevant documents directly into LLMs can lead\\nto information overload, diluting the focus on key details with\\nirrelevant content.To mitigate this, post-retrieval efforts con-\\ncentrate on selecting the essential information, emphasizing\\ncritical sections, and shortening the context to be processed.\\n2https://www.llamaindex.ai\\n3https://www.langchain.com/\\nC. Modular RAG\\nThe modular RAG architecture advances beyond the for-\\nmer two RAG paradigms, offering enhanced adaptability and\\nversatility. It incorporates diverse strategies for improving its\\ncomponents, such as adding a search module for similarity\\nsearches and refining the retriever through fine-tuning. Inno-\\nvations like restructured RAG modules [13] and rearranged\\nRAG pipelines [14] have been introduced to tackle specific\\nchallenges. The shift towards a modular RAG approach is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='vations like restructured RAG modules [13] and rearranged\\nRAG pipelines [14] have been introduced to tackle specific\\nchallenges. The shift towards a modular RAG approach is\\nbecoming prevalent, supporting both sequential processing and\\nintegrated end-to-end training across its components. Despite\\nits distinctiveness, Modular RAG builds upon the foundational\\nprinciples of Advanced and Naive RAG, illustrating a progres-\\nsion and refinement within the RAG family.\\n1) New Modules: The Modular RAG framework introduces\\nadditional specialized components to enhance retrieval and\\nprocessing capabilities. The Search module adapts to spe-\\ncific scenarios, enabling direct searches across various data\\nsources like search engines, databases, and knowledge graphs,\\nusing LLM-generated code and query languages [15]. RAG-\\nFusion addresses traditional search limitations by employing\\na multi-query strategy that expands user queries into diverse'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='using LLM-generated code and query languages [15]. RAG-\\nFusion addresses traditional search limitations by employing\\na multi-query strategy that expands user queries into diverse\\nperspectives, utilizing parallel vector searches and intelligent\\nre-ranking to uncover both explicit and transformative knowl-\\nedge [16]. The Memory module leverages the LLM’s memory\\nto guide retrieval, creating an unbounded memory pool that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='5\\naligns the text more closely with data distribution through iter-\\native self-enhancement [17], [18]. Routing in the RAG system\\nnavigates through diverse data sources, selecting the optimal\\npathway for a query, whether it involves summarization,\\nspecific database searches, or merging different information\\nstreams [19]. The Predict module aims to reduce redundancy\\nand noise by generating context directly through the LLM,\\nensuring relevance and accuracy [13]. Lastly, the Task Adapter\\nmodule tailors RAG to various downstream tasks, automating\\nprompt retrieval for zero-shot inputs and creating task-specific\\nretrievers through few-shot query generation [20], [21] .This\\ncomprehensive approach not only streamlines the retrieval pro-\\ncess but also significantly improves the quality and relevance\\nof the information retrieved, catering to a wide array of tasks\\nand queries with enhanced precision and flexibility.\\n2) New Patterns: Modular RAG offers remarkable adapt-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='of the information retrieved, catering to a wide array of tasks\\nand queries with enhanced precision and flexibility.\\n2) New Patterns: Modular RAG offers remarkable adapt-\\nability by allowing module substitution or reconfiguration\\nto address specific challenges. This goes beyond the fixed\\nstructures of Naive and Advanced RAG, characterized by a\\nsimple “Retrieve” and “Read” mechanism. Moreover, Modular\\nRAG expands this flexibility by integrating new modules or\\nadjusting interaction flow among existing ones, enhancing its\\napplicability across different tasks.\\nInnovations such as the Rewrite-Retrieve-Read [7]model\\nleverage the LLM’s capabilities to refine retrieval queries\\nthrough a rewriting module and a LM-feedback mechanism\\nto update rewriting model., improving task performance.\\nSimilarly, approaches like Generate-Read [13] replace tradi-\\ntional retrieval with LLM-generated content, while Recite-\\nRead [22] emphasizes retrieval from model weights, enhanc-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='Similarly, approaches like Generate-Read [13] replace tradi-\\ntional retrieval with LLM-generated content, while Recite-\\nRead [22] emphasizes retrieval from model weights, enhanc-\\ning the model’s ability to handle knowledge-intensive tasks.\\nHybrid retrieval strategies integrate keyword, semantic, and\\nvector searches to cater to diverse queries. Additionally, em-\\nploying sub-queries and hypothetical document embeddings\\n(HyDE) [11] seeks to improve retrieval relevance by focusing\\non embedding similarities between generated answers and real\\ndocuments.\\nAdjustments in module arrangement and interaction, such\\nas the Demonstrate-Search-Predict (DSP) [23] framework\\nand the iterative Retrieve-Read-Retrieve-Read flow of ITER-\\nRETGEN [14], showcase the dynamic use of module out-\\nputs to bolster another module’s functionality, illustrating a\\nsophisticated understanding of enhancing module synergy.\\nThe flexible orchestration of Modular RAG Flow showcases'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='puts to bolster another module’s functionality, illustrating a\\nsophisticated understanding of enhancing module synergy.\\nThe flexible orchestration of Modular RAG Flow showcases\\nthe benefits of adaptive retrieval through techniques such as\\nFLARE [24] and Self-RAG [25]. This approach transcends\\nthe fixed RAG retrieval process by evaluating the necessity\\nof retrieval based on different scenarios. Another benefit of\\na flexible architecture is that the RAG system can more\\neasily integrate with other technologies (such as fine-tuning\\nor reinforcement learning) [26]. For example, this can involve\\nfine-tuning the retriever for better retrieval results, fine-tuning\\nthe generator for more personalized outputs, or engaging in\\ncollaborative fine-tuning [27].\\nD. RAG vs Fine-tuning\\nThe augmentation of LLMs has attracted considerable atten-\\ntion due to their growing prevalence. Among the optimization\\nmethods for LLMs, RAG is often compared with Fine-tuning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='D. RAG vs Fine-tuning\\nThe augmentation of LLMs has attracted considerable atten-\\ntion due to their growing prevalence. Among the optimization\\nmethods for LLMs, RAG is often compared with Fine-tuning\\n(FT) and prompt engineering. Each method has distinct charac-\\nteristics as illustrated in Figure 4. We used a quadrant chart to\\nillustrate the differences among three methods in two dimen-\\nsions: external knowledge requirements and model adaption\\nrequirements. Prompt engineering leverages a model’s inherent\\ncapabilities with minimum necessity for external knowledge\\nand model adaption. RAG can be likened to providing a model\\nwith a tailored textbook for information retrieval, ideal for pre-\\ncise information retrieval tasks. In contrast, FT is comparable\\nto a student internalizing knowledge over time, suitable for\\nscenarios requiring replication of specific structures, styles, or\\nformats.\\nRAG excels in dynamic environments by offering real-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='to a student internalizing knowledge over time, suitable for\\nscenarios requiring replication of specific structures, styles, or\\nformats.\\nRAG excels in dynamic environments by offering real-\\ntime knowledge updates and effective utilization of external\\nknowledge sources with high interpretability. However, it\\ncomes with higher latency and ethical considerations regarding\\ndata retrieval. On the other hand, FT is more static, requiring\\nretraining for updates but enabling deep customization of the\\nmodel’s behavior and style. It demands significant compu-\\ntational resources for dataset preparation and training, and\\nwhile it can reduce hallucinations, it may face challenges with\\nunfamiliar data.\\nIn multiple evaluations of their performance on various\\nknowledge-intensive tasks across different topics, [28] re-\\nvealed that while unsupervised fine-tuning shows some im-\\nprovement, RAG consistently outperforms it, for both exist-\\ning knowledge encountered during training and entirely new'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='vealed that while unsupervised fine-tuning shows some im-\\nprovement, RAG consistently outperforms it, for both exist-\\ning knowledge encountered during training and entirely new\\nknowledge. Additionally, it was found that LLMs struggle\\nto learn new factual information through unsupervised fine-\\ntuning. The choice between RAG and FT depends on the\\nspecific needs for data dynamics, customization, and com-\\nputational capabilities in the application context. RAG and\\nFT are not mutually exclusive and can complement each\\nother, enhancing a model’s capabilities at different levels.\\nIn some instances, their combined use may lead to optimal\\nperformance. The optimization process involving RAG and FT\\nmay require multiple iterations to achieve satisfactory results.\\nIII. R ETRIEVAL\\nIn the context of RAG, it is crucial to efficiently retrieve\\nrelevant documents from the data source. There are several\\nkey issues involved, such as the retrieval source, retrieval'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='III. R ETRIEVAL\\nIn the context of RAG, it is crucial to efficiently retrieve\\nrelevant documents from the data source. There are several\\nkey issues involved, such as the retrieval source, retrieval\\ngranularity, pre-processing of the retrieval, and selection of\\nthe corresponding embedding model.\\nA. Retrieval Source\\nRAG relies on external knowledge to enhance LLMs, while\\nthe type of retrieval source and the granularity of retrieval\\nunits both affect the final generation results.\\n1) Data Structure: Initially, text is s the mainstream source\\nof retrieval. Subsequently, the retrieval source expanded to in-\\nclude semi-structured data (PDF) and structured data (Knowl-\\nedge Graph, KG) for enhancement. In addition to retrieving\\nfrom original external sources, there is also a growing trend in\\nrecent researches towards utilizing content generated by LLMs\\nthemselves for retrieval and enhancement purposes.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='6\\nTABLE I\\nSUMMARY OF RAG METHODS\\nMethod Retrieval Source Retrieval\\nData Type\\nRetrieval\\nGranularity\\nAugmentation\\nStage\\nRetrieval\\nprocess\\nCoG [29] Wikipedia Text Phrase Pre-training Iterative\\nDenseX [30] FactoidWiki Text Proposition Inference Once\\nEAR [31] Dataset-base Text Sentence Tuning Once\\nUPRISE [20] Dataset-base Text Sentence Tuning Once\\nRAST [32] Dataset-base Text Sentence Tuning Once\\nSelf-Mem [17] Dataset-base Text Sentence Tuning Iterative\\nFLARE [24] Search Engine,Wikipedia Text Sentence Tuning Adaptive\\nPGRA [33] Wikipedia Text Sentence Inference Once\\nFILCO [34] Wikipedia Text Sentence Inference Once\\nRADA [35] Dataset-base Text Sentence Inference Once\\nFilter-rerank [36] Synthesized dataset Text Sentence Inference Once\\nR-GQA [37] Dataset-base Text Sentence Pair Tuning Once\\nLLM-R [38] Dataset-base Text Sentence Pair Inference Iterative\\nTIGER [39] Dataset-base Text Item-base Pre-training Once\\nLM-Indexer [40] Dataset-base Text Item-base Tuning Once'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='LLM-R [38] Dataset-base Text Sentence Pair Inference Iterative\\nTIGER [39] Dataset-base Text Item-base Pre-training Once\\nLM-Indexer [40] Dataset-base Text Item-base Tuning Once\\nBEQUE [9] Dataset-base Text Item-base Tuning Once\\nCT-RAG [41] Synthesized dataset Text Item-base Tuning Once\\nAtlas [42] Wikipedia, Common Crawl Text Chunk Pre-training Iterative\\nRA VEN [43] Wikipedia Text Chunk Pre-training Once\\nRETRO++ [44] Pre-training Corpus Text Chunk Pre-training Iterative\\nINSTRUCTRETRO [45] Pre-training corpus Text Chunk Pre-training Iterative\\nRRR [7] Search Engine Text Chunk Tuning Once\\nRA-e2e [46] Dataset-base Text Chunk Tuning Once\\nPROMPTAGATOR [21] BEIR Text Chunk Tuning Once\\nAAR [47] MSMARCO,Wikipedia Text Chunk Tuning Once\\nRA-DIT [27] Common Crawl,Wikipedia Text Chunk Tuning Once\\nRAG-Robust [48] Wikipedia Text Chunk Tuning Once\\nRA-Long-Form [49] Dataset-base Text Chunk Tuning Once\\nCoN [50] Wikipedia Text Chunk Tuning Once\\nSelf-RAG [25] Wikipedia Text Chunk Tuning Adaptive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='RAG-Robust [48] Wikipedia Text Chunk Tuning Once\\nRA-Long-Form [49] Dataset-base Text Chunk Tuning Once\\nCoN [50] Wikipedia Text Chunk Tuning Once\\nSelf-RAG [25] Wikipedia Text Chunk Tuning Adaptive\\nBGM [26] Wikipedia Text Chunk Inference Once\\nCoQ [51] Wikipedia Text Chunk Inference Iterative\\nToken-Elimination [52] Wikipedia Text Chunk Inference Once\\nPaperQA [53] Arxiv,Online Database,PubMed Text Chunk Inference Iterative\\nNoiseRAG [54] FactoidWiki Text Chunk Inference Once\\nIAG [55] Search Engine,Wikipedia Text Chunk Inference Once\\nNoMIRACL [56] Wikipedia Text Chunk Inference Once\\nToC [57] Search Engine,Wikipedia Text Chunk Inference Recursive\\nSKR [58] Dataset-base,Wikipedia Text Chunk Inference Adaptive\\nITRG [59] Wikipedia Text Chunk Inference Iterative\\nRAG-LongContext [60] Dataset-base Text Chunk Inference Once\\nITER-RETGEN [14] Wikipedia Text Chunk Inference Iterative\\nIRCoT [61] Wikipedia Text Chunk Inference Recursive\\nLLM-Knowledge-Boundary [62] Wikipedia Text Chunk Inference Once'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='ITER-RETGEN [14] Wikipedia Text Chunk Inference Iterative\\nIRCoT [61] Wikipedia Text Chunk Inference Recursive\\nLLM-Knowledge-Boundary [62] Wikipedia Text Chunk Inference Once\\nRAPTOR [63] Dataset-base Text Chunk Inference Recursive\\nRECITE [22] LLMs Text Chunk Inference Once\\nICRALM [64] Pile,Wikipedia Text Chunk Inference Iterative\\nRetrieve-and-Sample [65] Dataset-base Text Doc Tuning Once\\nZemi [66] C4 Text Doc Tuning Once\\nCRAG [67] Arxiv Text Doc Inference Once\\n1-PAGER [68] Wikipedia Text Doc Inference Iterative\\nPRCA [69] Dataset-base Text Doc Inference Once\\nQLM-Doc-ranking [70] Dataset-base Text Doc Inference Once\\nRecomp [71] Wikipedia Text Doc Inference Once\\nDSP [23] Wikipedia Text Doc Inference Iterative\\nRePLUG [72] Pile Text Doc Inference Once\\nARM-RAG [73] Dataset-base Text Doc Inference Iterative\\nGenRead [13] LLMs Text Doc Inference Iterative\\nUniMS-RAG [74] Dataset-base Text Multi Tuning Once\\nCREA-ICL [19] Dataset-base Crosslingual,Text Sentence Inference Once'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='GenRead [13] LLMs Text Doc Inference Iterative\\nUniMS-RAG [74] Dataset-base Text Multi Tuning Once\\nCREA-ICL [19] Dataset-base Crosslingual,Text Sentence Inference Once\\nPKG [75] LLM Tabular,Text Chunk Inference Once\\nSANTA [76] Dataset-base Code,Text Item Pre-training Once\\nSURGE [77] Freebase KG Sub-Graph Tuning Once\\nMK-ToD [78] Dataset-base KG Entity Tuning Once\\nDual-Feedback-ToD [79] Dataset-base KG Entity Sequence Tuning Once\\nKnowledGPT [15] Dataset-base KG Triplet Inference Muti-time\\nFABULA [80] Dataset-base,Graph KG Entity Inference Once\\nHyKGE [81] CMeKG KG Entity Inference Once\\nKALMV [82] Wikipedia KG Triplet Inference Iterative\\nRoG [83] Freebase KG Triplet Inference Iterative\\nG-Retriever [84] Dataset-base TextGraph Sub-Graph Inference Once'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='7\\nFig. 4. RAG compared with other model optimization methods in the aspects of “External Knowledge Required” and “Model Adaption Required”. Prompt\\nEngineering requires low modifications to the model and external knowledge, focusing on harnessing the capabilities of LLMs themselves. Fine-tuning, on\\nthe other hand, involves further training the model. In the early stages of RAG (Naive RAG), there is a low demand for model modifications. As research\\nprogresses, Modular RAG has become more integrated with fine-tuning techniques.\\nUnstructured Data , such as text, is the most widely used\\nretrieval source, which are mainly gathered from corpus. For\\nopen-domain question-answering (ODQA) tasks, the primary\\nretrieval sources are Wikipedia Dump with the current major\\nversions including HotpotQA 4 (1st October , 2017), DPR5 (20\\nDecember, 2018). In addition to encyclopedic data, common\\nunstructured data includes cross-lingual text [19] and domain-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='versions including HotpotQA 4 (1st October , 2017), DPR5 (20\\nDecember, 2018). In addition to encyclopedic data, common\\nunstructured data includes cross-lingual text [19] and domain-\\nspecific data (such as medical [67]and legal domains [29]).\\nSemi-structured data. typically refers to data that contains a\\ncombination of text and table information, such as PDF. Han-\\ndling semi-structured data poses challenges for conventional\\nRAG systems due to two main reasons. Firstly, text splitting\\nprocesses may inadvertently separate tables, leading to data\\ncorruption during retrieval. Secondly, incorporating tables into\\nthe data can complicate semantic similarity searches. When\\ndealing with semi-structured data, one approach involves lever-\\naging the code capabilities of LLMs to execute Text-2-SQL\\nqueries on tables within databases, such as TableGPT [85].\\nAlternatively, tables can be transformed into text format for\\nfurther analysis using text-based methods [75]. However, both'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='queries on tables within databases, such as TableGPT [85].\\nAlternatively, tables can be transformed into text format for\\nfurther analysis using text-based methods [75]. However, both\\nof these methods are not optimal solutions, indicating substan-\\ntial research opportunities in this area.\\nStructured data , such as knowledge graphs (KGs) [86] ,\\nwhich are typically verified and can provide more precise in-\\nformation. KnowledGPT [15] generates KB search queries and\\nstores knowledge in a personalized base, enhancing the RAG\\nmodel’s knowledge richness. In response to the limitations of\\nLLMs in understanding and answering questions about textual\\ngraphs, G-Retriever [84] integrates Graph Neural Networks\\n4https://hotpotqa.github.io/wiki-readme.html\\n5https://github.com/facebookresearch/DPR\\n(GNNs), LLMs and RAG, enhancing graph comprehension\\nand question-answering capabilities through soft prompting\\nof the LLM, and employs the Prize-Collecting Steiner Tree'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='(GNNs), LLMs and RAG, enhancing graph comprehension\\nand question-answering capabilities through soft prompting\\nof the LLM, and employs the Prize-Collecting Steiner Tree\\n(PCST) optimization problem for targeted graph retrieval. On\\nthe contrary, it requires additional effort to build, validate,\\nand maintain structured databases. On the contrary, it requires\\nadditional effort to build, validate, and maintain structured\\ndatabases.\\nLLMs-Generated Content. Addressing the limitations of\\nexternal auxiliary information in RAG, some research has\\nfocused on exploiting LLMs’ internal knowledge. SKR [58]\\nclassifies questions as known or unknown, applying retrieval\\nenhancement selectively. GenRead [13] replaces the retriever\\nwith an LLM generator, finding that LLM-generated contexts\\noften contain more accurate answers due to better alignment\\nwith the pre-training objectives of causal language modeling.\\nSelfmem [17] iteratively creates an unbounded memory pool'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='often contain more accurate answers due to better alignment\\nwith the pre-training objectives of causal language modeling.\\nSelfmem [17] iteratively creates an unbounded memory pool\\nwith a retrieval-enhanced generator, using a memory selec-\\ntor to choose outputs that serve as dual problems to the\\noriginal question, thus self-enhancing the generative model.\\nThese methodologies underscore the breadth of innovative\\ndata source utilization in RAG, striving to improve model\\nperformance and task effectiveness.\\n2) Retrieval Granularity: Another important factor besides\\nthe data format of the retrieval source is the granularity of\\nthe retrieved data. Coarse-grained retrieval units theoretically\\ncan provide more relevant information for the problem, but\\nthey may also contain redundant content, which could distract\\nthe retriever and language models in downstream tasks [50],\\n[87]. On the other hand, fine-grained retrieval unit granularity'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='they may also contain redundant content, which could distract\\nthe retriever and language models in downstream tasks [50],\\n[87]. On the other hand, fine-grained retrieval unit granularity\\nincreases the burden of retrieval and does not guarantee seman-\\ntic integrity and meeting the required knowledge. Choosing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='8\\nthe appropriate retrieval granularity during inference can be\\na simple and effective strategy to improve the retrieval and\\ndownstream task performance of dense retrievers.\\nIn text, retrieval granularity ranges from fine to coarse,\\nincluding Token, Phrase, Sentence, Proposition, Chunks, Doc-\\nument. Among them, DenseX [30]proposed the concept of\\nusing propositions as retrieval units. Propositions are defined\\nas atomic expressions in the text, each encapsulating a unique\\nfactual segment and presented in a concise, self-contained nat-\\nural language format. This approach aims to enhance retrieval\\nprecision and relevance. On the Knowledge Graph (KG),\\nretrieval granularity includes Entity, Triplet, and sub-Graph.\\nThe granularity of retrieval can also be adapted to downstream\\ntasks, such as retrieving Item IDs [40]in recommendation tasks\\nand Sentence pairs [38]. Detailed information is illustrated in\\nTable I.\\nB. Indexing Optimization\\nIn the Indexing phase, documents will be processed, seg-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='and Sentence pairs [38]. Detailed information is illustrated in\\nTable I.\\nB. Indexing Optimization\\nIn the Indexing phase, documents will be processed, seg-\\nmented, and transformed into Embeddings to be stored in a\\nvector database. The quality of index construction determines\\nwhether the correct context can be obtained in the retrieval\\nphase.\\n1) Chunking Strategy: The most common method is to split\\nthe document into chunks on a fixed number of tokens (e.g.,\\n100, 256, 512) [88]. Larger chunks can capture more context,\\nbut they also generate more noise, requiring longer processing\\ntime and higher costs. While smaller chunks may not fully\\nconvey the necessary context, they do have less noise. How-\\never, chunks leads to truncation within sentences, prompting\\nthe optimization of a recursive splits and sliding window meth-\\nods, enabling layered retrieval by merging globally related\\ninformation across multiple retrieval processes [89]. Never-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='the optimization of a recursive splits and sliding window meth-\\nods, enabling layered retrieval by merging globally related\\ninformation across multiple retrieval processes [89]. Never-\\ntheless, these approaches still cannot strike a balance between\\nsemantic completeness and context length. Therefore, methods\\nlike Small2Big have been proposed, where sentences (small)\\nare used as the retrieval unit, and the preceding and following\\nsentences are provided as (big) context to LLMs [90].\\n2) Metadata Attachments: Chunks can be enriched with\\nmetadata information such as page number, file name, au-\\nthor,category timestamp. Subsequently, retrieval can be filtered\\nbased on this metadata, limiting the scope of the retrieval.\\nAssigning different weights to document timestamps during\\nretrieval can achieve time-aware RAG, ensuring the freshness\\nof knowledge and avoiding outdated information.\\nIn addition to extracting metadata from the original doc-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='retrieval can achieve time-aware RAG, ensuring the freshness\\nof knowledge and avoiding outdated information.\\nIn addition to extracting metadata from the original doc-\\numents, metadata can also be artificially constructed. For\\nexample, adding summaries of paragraph, as well as intro-\\nducing hypothetical questions. This method is also known as\\nReverse HyDE. Specifically, using LLM to generate questions\\nthat can be answered by the document, then calculating the\\nsimilarity between the original question and the hypothetical\\nquestion during retrieval to reduce the semantic gap between\\nthe question and the answer.\\n3) Structural Index: One effective method for enhancing\\ninformation retrieval is to establish a hierarchical structure for\\nthe documents. By constructing In structure, RAG system can\\nexpedite the retrieval and processing of pertinent data.\\nHierarchical index structure . File are arranged in parent-\\nchild relationships, with chunks linked to them. Data sum-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='expedite the retrieval and processing of pertinent data.\\nHierarchical index structure . File are arranged in parent-\\nchild relationships, with chunks linked to them. Data sum-\\nmaries are stored at each node, aiding in the swift traversal\\nof data and assisting the RAG system in determining which\\nchunks to extract. This approach can also mitigate the illusion\\ncaused by block extraction issues.\\nKnowledge Graph index . Utilize KG in constructing the\\nhierarchical structure of documents contributes to maintaining\\nconsistency. It delineates the connections between different\\nconcepts and entities, markedly reducing the potential for\\nillusions. Another advantage is the transformation of the\\ninformation retrieval process into instructions that LLM can\\ncomprehend, thereby enhancing the accuracy of knowledge\\nretrieval and enabling LLM to generate contextually coherent\\nresponses, thus improving the overall efficiency of the RAG\\nsystem. To capture the logical relationship between document'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='retrieval and enabling LLM to generate contextually coherent\\nresponses, thus improving the overall efficiency of the RAG\\nsystem. To capture the logical relationship between document\\ncontent and structure, KGP [91] proposed a method of building\\nan index between multiple documents using KG. This KG\\nconsists of nodes (representing paragraphs or structures in the\\ndocuments, such as pages and tables) and edges (indicating\\nsemantic/lexical similarity between paragraphs or relationships\\nwithin the document structure), effectively addressing knowl-\\nedge retrieval and reasoning problems in a multi-document\\nenvironment.\\nC. Query Optimization\\nOne of the primary challenges with Naive RAG is its\\ndirect reliance on the user’s original query as the basis for\\nretrieval. Formulating a precise and clear question is difficult,\\nand imprudent queries result in subpar retrieval effectiveness.\\nSometimes, the question itself is complex, and the language'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='retrieval. Formulating a precise and clear question is difficult,\\nand imprudent queries result in subpar retrieval effectiveness.\\nSometimes, the question itself is complex, and the language\\nis not well-organized. Another difficulty lies in language\\ncomplexity ambiguity. Language models often struggle when\\ndealing with specialized vocabulary or ambiguous abbrevi-\\nations with multiple meanings. For instance, they may not\\ndiscern whether “LLM” refers to large language model or a\\nMaster of Laws in a legal context.\\n1) Query Expansion: Expanding a single query into mul-\\ntiple queries enriches the content of the query, providing\\nfurther context to address any lack of specific nuances, thereby\\nensuring the optimal relevance of the generated answers.\\nMulti-Query. By employing prompt engineering to expand\\nqueries via LLMs, these queries can then be executed in\\nparallel. The expansion of queries is not random, but rather\\nmeticulously designed.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='Multi-Query. By employing prompt engineering to expand\\nqueries via LLMs, these queries can then be executed in\\nparallel. The expansion of queries is not random, but rather\\nmeticulously designed.\\nSub-Query. The process of sub-question planning represents\\nthe generation of the necessary sub-questions to contextualize\\nand fully answer the original question when combined. This\\nprocess of adding relevant context is, in principle, similar\\nto query expansion. Specifically, a complex question can be\\ndecomposed into a series of simpler sub-questions using the\\nleast-to-most prompting method [92].\\nChain-of-Verification(CoVe). The expanded queries undergo\\nvalidation by LLM to achieve the effect of reducing halluci-\\nnations. Validated expanded queries typically exhibit higher\\nreliability [93].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='9\\n2) Query Transformation: The core concept is to retrieve\\nchunks based on a transformed query instead of the user’s\\noriginal query.\\nQuery Rewrite.The original queries are not always optimal\\nfor LLM retrieval, especially in real-world scenarios. There-\\nfore, we can prompt LLM to rewrite the queries. In addition to\\nusing LLM for query rewriting, specialized smaller language\\nmodels, such as RRR (Rewrite-retrieve-read) [7]. The imple-\\nmentation of the query rewrite method in the Taobao, known\\nas BEQUE [9] has notably enhanced recall effectiveness for\\nlong-tail queries, resulting in a rise in GMV .\\nAnother query transformation method is to use prompt\\nengineering to let LLM generate a query based on the original\\nquery for subsequent retrieval. HyDE [11] construct hypothet-\\nical documents (assumed answers to the original query). It\\nfocuses on embedding similarity from answer to answer rather\\nthan seeking embedding similarity for the problem or query.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='ical documents (assumed answers to the original query). It\\nfocuses on embedding similarity from answer to answer rather\\nthan seeking embedding similarity for the problem or query.\\nUsing the Step-back Prompting method [10], the original\\nquery is abstracted to generate a high-level concept question\\n(step-back question). In the RAG system, both the step-back\\nquestion and the original query are used for retrieval, and both\\nthe results are utilized as the basis for language model answer\\ngeneration.\\n3) Query Routing: Based on varying queries, routing to\\ndistinct RAG pipeline,which is suitable for a versatile RAG\\nsystem designed to accommodate diverse scenarios.\\nMetadata Router/ Filter . The first step involves extracting\\nkeywords (entity) from the query, followed by filtering based\\non the keywords and metadata within the chunks to narrow\\ndown the search scope.\\nSemantic Router is another method of routing involves\\nleveraging the semantic information of the query. Specific'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='on the keywords and metadata within the chunks to narrow\\ndown the search scope.\\nSemantic Router is another method of routing involves\\nleveraging the semantic information of the query. Specific\\napprach see Semantic Router 6. Certainly, a hybrid routing\\napproach can also be employed, combining both semantic and\\nmetadata-based methods for enhanced query routing.\\nD. Embedding\\nIn RAG, retrieval is achieved by calculating the similarity\\n(e.g. cosine similarity) between the embeddings of the ques-\\ntion and document chunks, where the semantic representation\\ncapability of embedding models plays a key role. This mainly\\nincludes a sparse encoder (BM25) and a dense retriever (BERT\\narchitecture Pre-training language models). Recent research\\nhas introduced prominent embedding models such as AngIE,\\nV oyage, BGE,etc [94]–[96], which are benefit from multi-task\\ninstruct tuning. Hugging Face’s MTEB leaderboard 7 evaluates\\nembedding models across 8 tasks, covering 58 datasests. Ad-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='V oyage, BGE,etc [94]–[96], which are benefit from multi-task\\ninstruct tuning. Hugging Face’s MTEB leaderboard 7 evaluates\\nembedding models across 8 tasks, covering 58 datasests. Ad-\\nditionally, C-MTEB focuses on Chinese capability, covering\\n6 tasks and 35 datasets. There is no one-size-fits-all answer\\nto “which embedding model to use.” However, some specific\\nmodels are better suited for particular use cases.\\n1) Mix/hybrid Retrieval : Sparse and dense embedding\\napproaches capture different relevance features and can ben-\\nefit from each other by leveraging complementary relevance\\ninformation. For instance, sparse retrieval models can be used\\n6https://github.com/aurelio-labs/semantic-router\\n7https://huggingface.co/spaces/mteb/leaderboard\\nto provide initial search results for training dense retrieval\\nmodels. Additionally, pre-training language models (PLMs)\\ncan be utilized to learn term weights to enhance sparse\\nretrieval. Specifically, it also demonstrates that sparse retrieval'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='models. Additionally, pre-training language models (PLMs)\\ncan be utilized to learn term weights to enhance sparse\\nretrieval. Specifically, it also demonstrates that sparse retrieval\\nmodels can enhance the zero-shot retrieval capability of dense\\nretrieval models and assist dense retrievers in handling queries\\ncontaining rare entities, thereby improving robustness.\\n2) Fine-tuning Embedding Model: In instances where the\\ncontext significantly deviates from pre-training corpus, partic-\\nularly within highly specialized disciplines such as healthcare,\\nlegal practice, and other sectors replete with proprietary jargon,\\nfine-tuning the embedding model on your own domain dataset\\nbecomes essential to mitigate such discrepancies.\\nIn addition to supplementing domain knowledge, another\\npurpose of fine-tuning is to align the retriever and generator,\\nfor example, using the results of LLM as the supervision signal\\nfor fine-tuning, known as LSR (LM-supervised Retriever).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='purpose of fine-tuning is to align the retriever and generator,\\nfor example, using the results of LLM as the supervision signal\\nfor fine-tuning, known as LSR (LM-supervised Retriever).\\nPROMPTAGATOR [21] utilizes the LLM as a few-shot query\\ngenerator to create task-specific retrievers, addressing chal-\\nlenges in supervised fine-tuning, particularly in data-scarce\\ndomains. Another approach, LLM-Embedder [97], exploits\\nLLMs to generate reward signals across multiple downstream\\ntasks. The retriever is fine-tuned with two types of supervised\\nsignals: hard labels for the dataset and soft rewards from\\nthe LLMs. This dual-signal approach fosters a more effective\\nfine-tuning process, tailoring the embedding model to diverse\\ndownstream applications. REPLUG [72] utilizes a retriever\\nand an LLM to calculate the probability distributions of the\\nretrieved documents and then performs supervised training\\nby computing the KL divergence. This straightforward and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='and an LLM to calculate the probability distributions of the\\nretrieved documents and then performs supervised training\\nby computing the KL divergence. This straightforward and\\neffective training method enhances the performance of the\\nretrieval model by using an LM as the supervisory signal,\\neliminating the need for specific cross-attention mechanisms.\\nMoreover, inspired by RLHF (Reinforcement Learning from\\nHuman Feedback), utilizing LM-based feedback to reinforce\\nthe retriever through reinforcement learning.\\nE. Adapter\\nFine-tuning models may present challenges, such as in-\\ntegrating functionality through an API or addressing con-\\nstraints arising from limited local computational resources.\\nConsequently, some approaches opt to incorporate an external\\nadapter to aid in alignment.\\nTo optimize the multi-task capabilities of LLM, UP-\\nRISE [20] trained a lightweight prompt retriever that can\\nautomatically retrieve prompts from a pre-built prompt pool'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='adapter to aid in alignment.\\nTo optimize the multi-task capabilities of LLM, UP-\\nRISE [20] trained a lightweight prompt retriever that can\\nautomatically retrieve prompts from a pre-built prompt pool\\nthat are suitable for a given zero-shot task input. AAR\\n(Augmentation-Adapted Retriver) [47] introduces a universal\\nadapter designed to accommodate multiple downstream tasks.\\nWhile PRCA [69] add a pluggable reward-driven contextual\\nadapter to enhance performance on specific tasks. BGM [26]\\nkeeps the retriever and LLM fixed,and trains a bridge Seq2Seq\\nmodel in between. The bridge model aims to transform the\\nretrieved information into a format that LLMs can work with\\neffectively, allowing it to not only rerank but also dynami-\\ncally select passages for each query, and potentially employ\\nmore advanced strategies like repetition. Furthermore, PKG'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='10\\nintroduces an innovative method for integrating knowledge\\ninto white-box models via directive fine-tuning [75]. In this\\napproach, the retriever module is directly substituted to gen-\\nerate relevant documents according to a query. This method\\nassists in addressing the difficulties encountered during the\\nfine-tuning process and enhances model performance.\\nIV. G ENERATION\\nAfter retrieval, it is not a good practice to directly input all\\nthe retrieved information to the LLM for answering questions.\\nFollowing will introduce adjustments from two perspectives:\\nadjusting the retrieved content and adjusting the LLM.\\nA. Context Curation\\nRedundant information can interfere with the final gener-\\nation of LLM, and overly long contexts can also lead LLM\\nto the “Lost in the middle” problem [98]. Like humans, LLM\\ntends to only focus on the beginning and end of long texts,\\nwhile forgetting the middle portion. Therefore, in the RAG\\nsystem, we typically need to further process the retrieved\\ncontent.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='tends to only focus on the beginning and end of long texts,\\nwhile forgetting the middle portion. Therefore, in the RAG\\nsystem, we typically need to further process the retrieved\\ncontent.\\n1) Reranking: Reranking fundamentally reorders document\\nchunks to highlight the most pertinent results first, effectively\\nreducing the overall document pool, severing a dual purpose\\nin information retrieval, acting as both an enhancer and a\\nfilter, delivering refined inputs for more precise language\\nmodel processing [70]. Reranking can be performed using\\nrule-based methods that depend on predefined metrics like\\nDiversity, Relevance, and MRR, or model-based approaches\\nlike Encoder-Decoder models from the BERT series (e.g.,\\nSpanBERT), specialized reranking models such as Cohere\\nrerank or bge-raranker-large, and general large language mod-\\nels like GPT [12], [99].\\n2) Context Selection/Compression: A common misconcep-\\ntion in the RAG process is the belief that retrieving as many'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='els like GPT [12], [99].\\n2) Context Selection/Compression: A common misconcep-\\ntion in the RAG process is the belief that retrieving as many\\nrelevant documents as possible and concatenating them to form\\na lengthy retrieval prompt is beneficial. However, excessive\\ncontext can introduce more noise, diminishing the LLM’s\\nperception of key information .\\n(Long) LLMLingua [100], [101] utilize small language\\nmodels (SLMs) such as GPT-2 Small or LLaMA-7B, to\\ndetect and remove unimportant tokens, transforming it into\\na form that is challenging for humans to comprehend but\\nwell understood by LLMs. This approach presents a direct\\nand practical method for prompt compression, eliminating the\\nneed for additional training of LLMs while balancing language\\nintegrity and compression ratio. PRCA tackled this issue by\\ntraining an information extractor [69]. Similarly, RECOMP\\nadopts a comparable approach by training an information\\ncondenser using contrastive learning [71]. Each training data'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='training an information extractor [69]. Similarly, RECOMP\\nadopts a comparable approach by training an information\\ncondenser using contrastive learning [71]. Each training data\\npoint consists of one positive sample and five negative sam-\\nples, and the encoder undergoes training using contrastive loss\\nthroughout this process [102] .\\nIn addition to compressing the context, reducing the num-\\nber of documents aslo helps improve the accuracy of the\\nmodel’s answers. Ma et al. [103] propose the “Filter-Reranker”\\nparadigm, which combines the strengths of LLMs and SLMs.\\nIn this paradigm, SLMs serve as filters, while LLMs function\\nas reordering agents. The research shows that instructing\\nLLMs to rearrange challenging samples identified by SLMs\\nleads to significant improvements in various Information\\nExtraction (IE) tasks. Another straightforward and effective\\napproach involves having the LLM evaluate the retrieved\\ncontent before generating the final answer. This allows the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='Extraction (IE) tasks. Another straightforward and effective\\napproach involves having the LLM evaluate the retrieved\\ncontent before generating the final answer. This allows the\\nLLM to filter out documents with poor relevance through LLM\\ncritique. For instance, in Chatlaw [104], the LLM is prompted\\nto self-suggestion on the referenced legal provisions to assess\\ntheir relevance.\\nB. LLM Fine-tuning\\nTargeted fine-tuning based on the scenario and data char-\\nacteristics on LLMs can yield better results. This is also one\\nof the greatest advantages of using on-premise LLMs. When\\nLLMs lack data in a specific domain, additional knowledge can\\nbe provided to the LLM through fine-tuning. Huggingface’s\\nfine-tuning data can also be used as an initial step.\\nAnother benefit of fine-tuning is the ability to adjust the\\nmodel’s input and output. For example, it can enable LLM to\\nadapt to specific data formats and generate responses in a par-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='Another benefit of fine-tuning is the ability to adjust the\\nmodel’s input and output. For example, it can enable LLM to\\nadapt to specific data formats and generate responses in a par-\\nticular style as instructed [37]. For retrieval tasks that engage\\nwith structured data, the SANTA framework [76] implements\\na tripartite training regimen to effectively encapsulate both\\nstructural and semantic nuances. The initial phase focuses on\\nthe retriever, where contrastive learning is harnessed to refine\\nthe query and document embeddings.\\nAligning LLM outputs with human or retriever preferences\\nthrough reinforcement learning is a potential approach. For\\ninstance, manually annotating the final generated answers\\nand then providing feedback through reinforcement learning.\\nIn addition to aligning with human preferences, it is also\\npossible to align with the preferences of fine-tuned models\\nand retrievers [79]. When circumstances prevent access to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='In addition to aligning with human preferences, it is also\\npossible to align with the preferences of fine-tuned models\\nand retrievers [79]. When circumstances prevent access to\\npowerful proprietary models or larger parameter open-source\\nmodels, a simple and effective method is to distill the more\\npowerful models(e.g. GPT-4). Fine-tuning of LLM can also\\nbe coordinated with fine-tuning of the retriever to align pref-\\nerences. A typical approach, such as RA-DIT [27], aligns the\\nscoring functions between Retriever and Generator using KL\\ndivergence.\\nV. A UGMENTATION PROCESS IN RAG\\nIn the domain of RAG, the standard practice often involves\\na singular (once) retrieval step followed by generation, which\\ncan lead to inefficiencies and sometimes is typically insuffi-\\ncient for complex problems demanding multi-step reasoning,\\nas it provides a limited scope of information [105]. Many\\nstudies have optimized the retrieval process in response to this\\nissue, and we have summarised them in Figure 5.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='as it provides a limited scope of information [105]. Many\\nstudies have optimized the retrieval process in response to this\\nissue, and we have summarised them in Figure 5.\\nA. Iterative Retrieval\\nIterative retrieval is a process where the knowledge base\\nis repeatedly searched based on the initial query and the text\\ngenerated so far, providing a more comprehensive knowledge'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='11\\nFig. 5. In addition to the most common once retrieval, RAG also includes three types of retrieval augmentation processes. (left) Iterative retrieval involves\\nalternating between retrieval and generation, allowing for richer and more targeted context from the knowledge base at each step. (Middle) Recursive retrieval\\ninvolves gradually refining the user query and breaking down the problem into sub-problems, then continuously solving complex problems through retrieval\\nand generation. (Right) Adaptive retrieval focuses on enabling the RAG system to autonomously determine whether external knowledge retrieval is necessary\\nand when to stop retrieval and generation, often utilizing LLM-generated special tokens for control.\\nbase for LLMs. This approach has been shown to enhance\\nthe robustness of subsequent answer generation by offering\\nadditional contextual references through multiple retrieval\\niterations. However, it may be affected by semantic discon-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='the robustness of subsequent answer generation by offering\\nadditional contextual references through multiple retrieval\\niterations. However, it may be affected by semantic discon-\\ntinuity and the accumulation of irrelevant information. ITER-\\nRETGEN [14] employs a synergistic approach that lever-\\nages “retrieval-enhanced generation” alongside “generation-\\nenhanced retrieval” for tasks that necessitate the reproduction\\nof specific information. The model harnesses the content\\nrequired to address the input task as a contextual basis for\\nretrieving pertinent knowledge, which in turn facilitates the\\ngeneration of improved responses in subsequent iterations.\\nB. Recursive Retrieval\\nRecursive retrieval is often used in information retrieval and\\nNLP to improve the depth and relevance of search results.\\nThe process involves iteratively refining search queries based\\non the results obtained from previous searches. Recursive\\nRetrieval aims to enhance the search experience by gradu-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='The process involves iteratively refining search queries based\\non the results obtained from previous searches. Recursive\\nRetrieval aims to enhance the search experience by gradu-\\nally converging on the most pertinent information through a\\nfeedback loop. IRCoT [61] uses chain-of-thought to guide\\nthe retrieval process and refines the CoT with the obtained\\nretrieval results. ToC [57] creates a clarification tree that\\nsystematically optimizes the ambiguous parts in the Query. It\\ncan be particularly useful in complex search scenarios where\\nthe user’s needs are not entirely clear from the outset or where\\nthe information sought is highly specialized or nuanced. The\\nrecursive nature of the process allows for continuous learning\\nand adaptation to the user’s requirements, often resulting in\\nimproved satisfaction with the search outcomes.\\nTo address specific data scenarios, recursive retrieval and\\nmulti-hop retrieval techniques are utilized together. Recursive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='improved satisfaction with the search outcomes.\\nTo address specific data scenarios, recursive retrieval and\\nmulti-hop retrieval techniques are utilized together. Recursive\\nretrieval involves a structured index to process and retrieve\\ndata in a hierarchical manner, which may include summarizing\\nsections of a document or lengthy PDF before performing a\\nretrieval based on this summary. Subsequently, a secondary\\nretrieval within the document refines the search, embodying\\nthe recursive nature of the process. In contrast, multi-hop\\nretrieval is designed to delve deeper into graph-structured data\\nsources, extracting interconnected information [106].\\nC. Adaptive Retrieval\\nAdaptive retrieval methods, exemplified by Flare [24] and\\nSelf-RAG [25], refine the RAG framework by enabling LLMs\\nto actively determine the optimal moments and content for\\nretrieval, thus enhancing the efficiency and relevance of the\\ninformation sourced.\\nThese methods are part of a broader trend wherein'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='to actively determine the optimal moments and content for\\nretrieval, thus enhancing the efficiency and relevance of the\\ninformation sourced.\\nThese methods are part of a broader trend wherein\\nLLMs employ active judgment in their operations, as seen\\nin model agents like AutoGPT, Toolformer, and Graph-\\nToolformer [107]–[109]. Graph-Toolformer, for instance, di-\\nvides its retrieval process into distinct steps where LLMs\\nproactively use retrievers, apply Self-Ask techniques, and em-\\nploy few-shot prompts to initiate search queries. This proactive\\nstance allows LLMs to decide when to search for necessary\\ninformation, akin to how an agent utilizes tools.\\nWebGPT [110] integrates a reinforcement learning frame-\\nwork to train the GPT-3 model in autonomously using a\\nsearch engine during text generation. It navigates this process\\nusing special tokens that facilitate actions such as search\\nengine queries, browsing results, and citing references, thereby'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='search engine during text generation. It navigates this process\\nusing special tokens that facilitate actions such as search\\nengine queries, browsing results, and citing references, thereby\\nexpanding GPT-3’s capabilities through the use of external\\nsearch engines. Flare automates timing retrieval by monitoring\\nthe confidence of the generation process, as indicated by the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='12\\nprobability of generated terms [24]. When the probability falls\\nbelow a certain threshold would activates the retrieval system\\nto collect relevant information, thus optimizing the retrieval\\ncycle. Self-RAG [25] introduces “reflection tokens” that allow\\nthe model to introspect its outputs. These tokens come in\\ntwo varieties: “retrieve” and “critic”. The model autonomously\\ndecides when to activate retrieval, or alternatively, a predefined\\nthreshold may trigger the process. During retrieval, the gen-\\nerator conducts a fragment-level beam search across multiple\\nparagraphs to derive the most coherent sequence. Critic scores\\nare used to update the subdivision scores, with the flexibility\\nto adjust these weights during inference, tailoring the model’s\\nbehavior. Self-RAG’s design obviates the need for additional\\nclassifiers or reliance on Natural Language Inference (NLI)\\nmodels, thus streamlining the decision-making process for\\nwhen to engage retrieval mechanisms and improving the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='classifiers or reliance on Natural Language Inference (NLI)\\nmodels, thus streamlining the decision-making process for\\nwhen to engage retrieval mechanisms and improving the\\nmodel’s autonomous judgment capabilities in generating ac-\\ncurate responses.\\nVI. T ASK AND EVALUATION\\nThe rapid advancement and growing adoption of RAG\\nin the field of NLP have propelled the evaluation of RAG\\nmodels to the forefront of research in the LLMs community.\\nThe primary objective of this evaluation is to comprehend\\nand optimize the performance of RAG models across diverse\\napplication scenarios.This chapter will mainly introduce the\\nmain downstream tasks of RAG, datasets, and how to evaluate\\nRAG systems.\\nA. Downstream Task\\nThe core task of RAG remains Question Answering (QA),\\nincluding traditional single-hop/multi-hop QA, multiple-\\nchoice, domain-specific QA as well as long-form scenarios\\nsuitable for RAG. In addition to QA, RAG is continuously\\nbeing expanded into multiple downstream tasks, such as Infor-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='choice, domain-specific QA as well as long-form scenarios\\nsuitable for RAG. In addition to QA, RAG is continuously\\nbeing expanded into multiple downstream tasks, such as Infor-\\nmation Extraction (IE), dialogue generation, code search, etc.\\nThe main downstream tasks of RAG and their corresponding\\ndatasets are summarized in Table II.\\nB. Evaluation Target\\nHistorically, RAG models assessments have centered on\\ntheir execution in specific downstream tasks. These evaluations\\nemploy established metrics suitable to the tasks at hand. For\\ninstance, question answering evaluations might rely on EM\\nand F1 scores [7], [45], [59], [72], whereas fact-checking\\ntasks often hinge on Accuracy as the primary metric [4],\\n[14], [42]. BLEU and ROUGE metrics are also commonly\\nused to evaluate answer quality [26], [32], [52], [78]. Tools\\nlike RALLE, designed for the automatic evaluation of RAG\\napplications, similarly base their assessments on these task-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='used to evaluate answer quality [26], [32], [52], [78]. Tools\\nlike RALLE, designed for the automatic evaluation of RAG\\napplications, similarly base their assessments on these task-\\nspecific metrics [160]. Despite this, there is a notable paucity\\nof research dedicated to evaluating the distinct characteristics\\nof RAG models.The main evaluation objectives include:\\nRetrieval Quality. Evaluating the retrieval quality is crucial\\nfor determining the effectiveness of the context sourced by\\nthe retriever component. Standard metrics from the domains\\nof search engines, recommendation systems, and information\\nretrieval systems are employed to measure the performance of\\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\\nNDCG are commonly utilized for this purpose [161], [162].\\nGeneration Quality . The assessment of generation quality\\ncenters on the generator’s capacity to synthesize coherent and\\nrelevant answers from the retrieved context. This evaluation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='Generation Quality . The assessment of generation quality\\ncenters on the generator’s capacity to synthesize coherent and\\nrelevant answers from the retrieved context. This evaluation\\ncan be categorized based on the content’s objectives: unlabeled\\nand labeled content. For unlabeled content, the evaluation\\nencompasses the faithfulness, relevance, and non-harmfulness\\nof the generated answers. In contrast, for labeled content,\\nthe focus is on the accuracy of the information produced by\\nthe model [161]. Additionally, both retrieval and generation\\nquality assessments can be conducted through manual or\\nautomatic evaluation methods [29], [161], [163].\\nC. Evaluation Aspects\\nContemporary evaluation practices of RAG models empha-\\nsize three primary quality scores and four essential abilities,\\nwhich collectively inform the evaluation of the two principal\\ntargets of the RAG model: retrieval and generation.\\n1) Quality Scores: Quality scores include context rele-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='which collectively inform the evaluation of the two principal\\ntargets of the RAG model: retrieval and generation.\\n1) Quality Scores: Quality scores include context rele-\\nvance, answer faithfulness, and answer relevance. These qual-\\nity scores evaluate the efficiency of the RAG model from\\ndifferent perspectives in the process of information retrieval\\nand generation [164]–[166].\\nContext Relevance evaluates the precision and specificity\\nof the retrieved context, ensuring relevance and minimizing\\nprocessing costs associated with extraneous content.\\nAnswer Faithfulness ensures that the generated answers\\nremain true to the retrieved context, maintaining consistency\\nand avoiding contradictions.\\nAnswer Relevance requires that the generated answers are\\ndirectly pertinent to the posed questions, effectively addressing\\nthe core inquiry.\\n2) Required Abilities: RAG evaluation also encompasses\\nfour abilities indicative of its adaptability and efficiency:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='the core inquiry.\\n2) Required Abilities: RAG evaluation also encompasses\\nfour abilities indicative of its adaptability and efficiency:\\nnoise robustness, negative rejection, information integration,\\nand counterfactual robustness [167], [168]. These abilities are\\ncritical for the model’s performance under various challenges\\nand complex scenarios, impacting the quality scores.\\nNoise Robustness appraises the model’s capability to man-\\nage noise documents that are question-related but lack sub-\\nstantive information.\\nNegative Rejection assesses the model’s discernment in\\nrefraining from responding when the retrieved documents do\\nnot contain the necessary knowledge to answer a question.\\nInformation Integration evaluates the model’s proficiency in\\nsynthesizing information from multiple documents to address\\ncomplex questions.\\nCounterfactual Robustness tests the model’s ability to rec-\\nognize and disregard known inaccuracies within documents,\\neven when instructed about potential misinformation.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='complex questions.\\nCounterfactual Robustness tests the model’s ability to rec-\\nognize and disregard known inaccuracies within documents,\\neven when instructed about potential misinformation.\\nContext relevance and noise robustness are important for\\nevaluating the quality of retrieval, while answer faithfulness,\\nanswer relevance, negative rejection, information integration,\\nand counterfactual robustness are important for evaluating the\\nquality of generation.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='13\\nTABLE II\\nDOWNSTREAM TASKS AND DATASETS OF RAG\\nTask Sub Task Dataset Method\\nQA Single-hop Natural Qustion(NQ) [111]\\n[26], [30], [34], [42], [45], [50], [52], [59], [64], [82]\\n[3], [4], [22], [27], [40], [43], [54], [62], [71], [112]\\n[20], [44], [72]\\nTriviaQA(TQA) [113]\\n[13], [30], [34], [45], [50], [64]\\n[4], [27], [59], [62], [112]\\n[22], [25], [43], [44], [71], [72]\\nSQuAD [114] [20], [23], [30], [32], [45], [69], [112]\\nWeb Questions(WebQ) [115] [3], [4], [13], [30], [50], [68]\\nPopQA [116] [7], [25], [67]\\nMS MARCO [117] [4], [40], [52]\\nMulti-hop HotpotQA [118] [23], [26], [31], [34], [47], [51], [61], [82]\\n[7], [14], [22], [27], [59], [62], [69], [71], [91]\\n2WikiMultiHopQA [119] [14], [24], [48], [59], [61], [91]\\nMuSiQue [120] [14], [51], [61], [91]\\nLong-form QA ELI5 [121] [27], [34], [43], [49], [51]\\nNarrativeQA(NQA) [122] [45], [60], [63], [123]\\nASQA [124] [24], [57]\\nQMSum(QM) [125] [60], [123]\\nDomain QA Qasper [126] [60], [63]\\nCOVID-QA [127] [35], [46]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='NarrativeQA(NQA) [122] [45], [60], [63], [123]\\nASQA [124] [24], [57]\\nQMSum(QM) [125] [60], [123]\\nDomain QA Qasper [126] [60], [63]\\nCOVID-QA [127] [35], [46]\\nCMB [128],MMCU Medical [129] [81]\\nMulti-Choice QA QuALITY [130] [60], [63]\\nARC [131] [25], [67]\\nCommonsenseQA [132] [58], [66]\\nGraph QA GraphQA [84] [84]\\nDialog Dialog Generation Wizard of Wikipedia (WoW) [133] [13], [27], [34], [42]\\nPersonal Dialog KBP [134] [74], [135]\\nDuleMon [136] [74]\\nTask-oriented Dialog CamRest [137] [78], [79]\\nRecommendation Amazon(Toys,Sport,Beauty) [138] [39], [40]\\nIE Event Argument Extraction WikiEvent [139] [13], [27], [37], [42]\\nRAMS [140] [36], [37]\\nRelation Extraction T-REx [141],ZsRE [142] [27], [51]\\nReasoning Commonsense Reasoning HellaSwag [143] [20], [66]\\nCoT Reasoning CoT Reasoning [144] [27]\\nComplex Reasoning CSQA [145] [55]\\nOthers Language Understanding MMLU [146] [7], [27], [28], [42], [43], [47], [72]\\nLanguage Modeling WikiText-103 [147] [5], [29], [64], [71]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='Complex Reasoning CSQA [145] [55]\\nOthers Language Understanding MMLU [146] [7], [27], [28], [42], [43], [47], [72]\\nLanguage Modeling WikiText-103 [147] [5], [29], [64], [71]\\nStrategyQA [148] [14], [24], [48], [51], [55], [58]\\nFact Checking/Verification FEVER [149] [4], [13], [27], [34], [42], [50]\\nPubHealth [150] [25], [67]\\nText Generation Biography [151] [67]\\nText Summarization WikiASP [152] [24]\\nXSum [153] [17]\\nText Classification VioLens [154] [19]\\nTREC [155] [33]\\nSentiment SST-2 [156] [20], [33], [38]\\nCode Search CodeSearchNet [157] [76]\\nRobustness Evaluation NoMIRACL [56] [56]\\nMath GSM8K [158] [73]\\nMachine Translation JRC-Acquis [159] [17]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='14\\nTABLE III\\nSUMMARY OF METRICS APPLICABLE FOR EVALUATION ASPECTS OF RAG\\nContext\\nRelevance Faithfulness Answer\\nRelevance\\nNoise\\nRobustness\\nNegative\\nRejection\\nInformation\\nIntegration\\nCounterfactual\\nRobustness\\nAccuracy ✓ ✓ ✓ ✓ ✓ ✓ ✓\\nEM ✓\\nRecall ✓\\nPrecision ✓ ✓\\nR-Rate ✓\\nCosine Similarity ✓\\nHit Rate ✓\\nMRR ✓\\nNDCG ✓\\nBLEU ✓ ✓ ✓\\nROUGE/ROUGE-L ✓ ✓ ✓\\nThe specific metrics for each evaluation aspect are sum-\\nmarized in Table III. It is essential to recognize that these\\nmetrics, derived from related work, are traditional measures\\nand do not yet represent a mature or standardized approach for\\nquantifying RAG evaluation aspects. Custom metrics tailored\\nto the nuances of RAG models, though not included here, have\\nalso been developed in some evaluation studies.\\nD. Evaluation Benchmarks and Tools\\nA series of benchmark tests and tools have been proposed\\nto facilitate the evaluation of RAG.These instruments furnish\\nquantitative metrics that not only gauge RAG model perfor-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='A series of benchmark tests and tools have been proposed\\nto facilitate the evaluation of RAG.These instruments furnish\\nquantitative metrics that not only gauge RAG model perfor-\\nmance but also enhance comprehension of the model’s capabil-\\nities across various evaluation aspects. Prominent benchmarks\\nsuch as RGB, RECALL and CRUD [167]–[169] focus on\\nappraising the essential abilities of RAG models. Concur-\\nrently, state-of-the-art automated tools like RAGAS [164],\\nARES [165], and TruLens 8 employ LLMs to adjudicate the\\nquality scores. These tools and benchmarks collectively form\\na robust framework for the systematic evaluation of RAG\\nmodels, as summarized in Table IV.\\nVII. D ISCUSSION AND FUTURE PROSPECTS\\nDespite the considerable progress in RAG technology, sev-\\neral challenges persist that warrant in-depth research.This\\nchapter will mainly introduce the current challenges and future\\nresearch directions faced by RAG.\\nA. RAG vs Long Context'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='eral challenges persist that warrant in-depth research.This\\nchapter will mainly introduce the current challenges and future\\nresearch directions faced by RAG.\\nA. RAG vs Long Context\\nWith the deepening of related research, the context of LLMs\\nis continuously expanding [170]–[172]. Presently, LLMs can\\neffortlessly manage contexts exceeding 200,000 tokens 9. This\\ncapability signifies that long-document question answering,\\npreviously reliant on RAG, can now incorporate the entire\\ndocument directly into the prompt. This has also sparked\\ndiscussions on whether RAG is still necessary when LLMs\\n8https://www.trulens.org/trulens eval/core concepts rag triad/\\n9https://kimi.moonshot.cn\\nare not constrained by context. In fact, RAG still plays an\\nirreplaceable role. On one hand, providing LLMs with a\\nlarge amount of context at once will significantly impact its\\ninference speed, while chunked retrieval and on-demand input\\ncan significantly improve operational efficiency. On the other'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='large amount of context at once will significantly impact its\\ninference speed, while chunked retrieval and on-demand input\\ncan significantly improve operational efficiency. On the other\\nhand, RAG-based generation can quickly locate the original\\nreferences for LLMs to help users verify the generated an-\\nswers. The entire retrieval and reasoning process is observable,\\nwhile generation solely relying on long context remains a\\nblack box. Conversely, the expansion of context provides new\\nopportunities for the development of RAG, enabling it to\\naddress more complex problems and integrative or summary\\nquestions that require reading a large amount of material to\\nanswer [49]. Developing new RAG methods in the context of\\nsuper-long contexts is one of the future research trends.\\nB. RAG Robustness\\nThe presence of noise or contradictory information during\\nretrieval can detrimentally affect RAG’s output quality. This\\nsituation is figuratively referred to as “Misinformation can'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='B. RAG Robustness\\nThe presence of noise or contradictory information during\\nretrieval can detrimentally affect RAG’s output quality. This\\nsituation is figuratively referred to as “Misinformation can\\nbe worse than no information at all”. Improving RAG’s\\nresistance to such adversarial or counterfactual inputs is gain-\\ning research momentum and has become a key performance\\nmetric [48], [50], [82]. Cuconasu et al. [54] analyze which\\ntype of documents should be retrieved, evaluate the relevance\\nof the documents to the prompt, their position, and the\\nnumber included in the context. The research findings reveal\\nthat including irrelevant documents can unexpectedly increase\\naccuracy by over 30%, contradicting the initial assumption\\nof reduced quality. These results underscore the importance\\nof developing specialized strategies to integrate retrieval with\\nlanguage generation models, highlighting the need for further\\nresearch and exploration into the robustness of RAG.\\nC. Hybrid Approaches'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='language generation models, highlighting the need for further\\nresearch and exploration into the robustness of RAG.\\nC. Hybrid Approaches\\nCombining RAG with fine-tuning is emerging as a leading\\nstrategy. Determining the optimal integration of RAG and\\nfine-tuning whether sequential, alternating, or through end-to-\\nend joint training—and how to harness both parameterized'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='15\\nTABLE IV\\nSUMMARY OF EVALUATION FRAMEWORKS\\nEvaluation Framework Evaluation Targets Evaluation Aspects Quantitative Metrics\\nRGB† Retrieval Quality\\nGeneration Quality\\nNoise Robustness\\nNegative Rejection\\nInformation Integration\\nCounterfactual Robustness\\nAccuracy\\nEM\\nAccuracy\\nAccuracy\\nRECALL† Generation Quality Counterfactual Robustness R-Rate (Reappearance Rate)\\nRAGAS‡ Retrieval Quality\\nGeneration Quality\\nContext Relevance\\nFaithfulness\\nAnswer Relevance\\n*\\n*\\nCosine Similarity\\nARES‡ Retrieval Quality\\nGeneration Quality\\nContext Relevance\\nFaithfulness\\nAnswer Relevance\\nAccuracy\\nAccuracy\\nAccuracy\\nTruLens‡ Retrieval Quality\\nGeneration Quality\\nContext Relevance\\nFaithfulness\\nAnswer Relevance\\n*\\n*\\n*\\nCRUD† Retrieval Quality\\nGeneration Quality\\nCreative Generation\\nKnowledge-intensive QA\\nError Correction\\nSummarization\\nBLEU\\nROUGE-L\\nBertScore\\nRAGQuestEval\\n† represents a benchmark, and ‡ represents a tool. * denotes customized quantitative metrics, which deviate from traditional'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='Error Correction\\nSummarization\\nBLEU\\nROUGE-L\\nBertScore\\nRAGQuestEval\\n† represents a benchmark, and ‡ represents a tool. * denotes customized quantitative metrics, which deviate from traditional\\nmetrics. Readers are encouraged to consult pertinent literature for the specific quantification formulas associated with these\\nmetrics, as required.\\nand non-parameterized advantages are areas ripe for explo-\\nration [27]. Another trend is to introduce SLMs with specific\\nfunctionalities into RAG and fine-tuned by the results of RAG\\nsystem. For example, CRAG [67] trains a lightweight retrieval\\nevaluator to assess the overall quality of the retrieved docu-\\nments for a query and triggers different knowledge retrieval\\nactions based on confidence levels.\\nD. Scaling laws of RAG\\nEnd-to-end RAG models and pre-trained models based\\non RAG are still one of the focuses of current re-\\nsearchers [173].The parameters of these models are one of\\nthe key factors.While scaling laws [174] are established for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='on RAG are still one of the focuses of current re-\\nsearchers [173].The parameters of these models are one of\\nthe key factors.While scaling laws [174] are established for\\nLLMs, their applicability to RAG remains uncertain. Initial\\nstudies like RETRO++ [44] have begun to address this, yet the\\nparameter count in RAG models still lags behind that of LLMs.\\nThe possibility of an Inverse Scaling Law 10, where smaller\\nmodels outperform larger ones, is particularly intriguing and\\nmerits further investigation.\\nE. Production-Ready RAG\\nRAG’s practicality and alignment with engineering require-\\nments have facilitated its adoption. However, enhancing re-\\ntrieval efficiency, improving document recall in large knowl-\\nedge bases, and ensuring data security—such as preventing\\n10https://github.com/inverse-scaling/prize\\ninadvertent disclosure of document sources or metadata by\\nLLMs—are critical engineering challenges that remain to be\\naddressed [175].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='10https://github.com/inverse-scaling/prize\\ninadvertent disclosure of document sources or metadata by\\nLLMs—are critical engineering challenges that remain to be\\naddressed [175].\\nThe development of the RAG ecosystem is greatly impacted\\nby the progression of its technical stack. Key tools like\\nLangChain and LLamaIndex have quickly gained popularity\\nwith the emergence of ChatGPT, providing extensive RAG-\\nrelated APIs and becoming essential in the realm of LLMs.The\\nemerging technology stack, while not as rich in features as\\nLangChain and LLamaIndex, stands out through its specialized\\nproducts. For example, Flowise AI prioritizes a low-code\\napproach, allowing users to deploy AI applications, including\\nRAG, through a user-friendly drag-and-drop interface. Other\\ntechnologies like HayStack, Meltano, and Cohere Coral are\\nalso gaining attention for their unique contributions to the field.\\nIn addition to AI-focused vendors, traditional software and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='technologies like HayStack, Meltano, and Cohere Coral are\\nalso gaining attention for their unique contributions to the field.\\nIn addition to AI-focused vendors, traditional software and\\ncloud service providers are expanding their offerings to include\\nRAG-centric services. Weaviate’s Verba 11 is designed for\\npersonal assistant applications, while Amazon’s Kendra 12\\noffers intelligent enterprise search services, enabling users to\\nbrowse various content repositories using built-in connectors.\\nIn the development of RAG technology, there is a clear\\ntrend towards different specialization directions, such as: 1)\\nCustomization - tailoring RAG to meet specific requirements.\\n2) Simplification - making RAG easier to use to reduce the\\n11https://github.com/weaviate/Verba\\n12https://aws.amazon.com/cn/kendra/'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='16\\nFig. 6. Summary of RAG ecosystem\\ninitial learning curve. 3) Specialization - optimizing RAG to\\nbetter serve production environments.\\nThe mutual growth of RAG models and their technology\\nstacks is evident; technological advancements continuously\\nestablish new standards for existing infrastructure. In turn,\\nenhancements to the technology stack drive the development\\nof RAG capabilities. RAG toolkits are converging into a\\nfoundational technology stack, laying the groundwork for\\nadvanced enterprise applications. However, a fully integrated,\\ncomprehensive platform concept is still in the future, requiring\\nfurther innovation and development.\\nF . Multi-modal RAG\\nRAG has transcended its initial text-based question-\\nanswering confines, embracing a diverse array of modal data.\\nThis expansion has spawned innovative multimodal models\\nthat integrate RAG concepts across various domains:\\nImage. RA-CM3 [176] stands as a pioneering multimodal\\nmodel of both retrieving and generating text and images.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='that integrate RAG concepts across various domains:\\nImage. RA-CM3 [176] stands as a pioneering multimodal\\nmodel of both retrieving and generating text and images.\\nBLIP-2 [177] leverages frozen image encoders alongside\\nLLMs for efficient visual language pre-training, enabling zero-\\nshot image-to-text conversions. The “Visualize Before You\\nWrite” method [178] employs image generation to steer the\\nLM’s text generation, showing promise in open-ended text\\ngeneration tasks.\\nAudio and Video . The GSS method retrieves and stitches\\ntogether audio clips to convert machine-translated data into\\nspeech-translated data [179]. UEOP marks a significant ad-\\nvancement in end-to-end automatic speech recognition by\\nincorporating external, offline strategies for voice-to-text con-\\nversion [180]. Additionally, KNN-based attention fusion lever-\\nages audio embeddings and semantically related text embed-\\ndings to refine ASR, thereby accelerating domain adaptation.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='version [180]. Additionally, KNN-based attention fusion lever-\\nages audio embeddings and semantically related text embed-\\ndings to refine ASR, thereby accelerating domain adaptation.\\nVid2Seq augments language models with specialized temporal\\nmarkers, facilitating the prediction of event boundaries and\\ntextual descriptions within a unified output sequence [181].\\nCode. RBPS [182] excels in small-scale learning tasks by\\nretrieving code examples that align with developers’ objectives\\nthrough encoding and frequency analysis. This approach has\\ndemonstrated efficacy in tasks such as test assertion genera-\\ntion and program repair. For structured knowledge, the CoK\\nmethod [106] first extracts facts pertinent to the input query\\nfrom a knowledge graph, then integrates these facts as hints\\nwithin the input, enhancing performance in knowledge graph\\nquestion-answering tasks.\\nVIII. C ONCLUSION\\nThe summary of this paper, as depicted in Figure 6, empha-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='within the input, enhancing performance in knowledge graph\\nquestion-answering tasks.\\nVIII. C ONCLUSION\\nThe summary of this paper, as depicted in Figure 6, empha-\\nsizes RAG’s significant advancement in enhancing the capa-\\nbilities of LLMs by integrating parameterized knowledge from\\nlanguage models with extensive non-parameterized data from\\nexternal knowledge bases. The survey showcases the evolution\\nof RAG technologies and their application on many different\\ntasks. The analysis outlines three developmental paradigms\\nwithin the RAG framework: Naive, Advanced, and Modu-\\nlar RAG, each representing a progressive enhancement over\\nits predecessors. RAG’s technical integration with other AI\\nmethodologies, such as fine-tuning and reinforcement learning,\\nhas further expanded its capabilities. Despite the progress in\\nRAG technology, there are research opportunities to improve\\nits robustness and its ability to handle extended contexts.\\nRAG’s application scope is expanding into multimodal do-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='RAG technology, there are research opportunities to improve\\nits robustness and its ability to handle extended contexts.\\nRAG’s application scope is expanding into multimodal do-\\nmains, adapting its principles to interpret and process diverse\\ndata forms like images, videos, and code. This expansion high-\\nlights RAG’s significant practical implications for AI deploy-\\nment, attracting interest from academic and industrial sectors.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='17\\nThe growing ecosystem of RAG is evidenced by the rise in\\nRAG-centric AI applications and the continuous development\\nof supportive tools. As RAG’s application landscape broadens,\\nthere is a need to refine evaluation methodologies to keep\\npace with its evolution. Ensuring accurate and representative\\nperformance assessments is crucial for fully capturing RAG’s\\ncontributions to the AI research and development community.\\nREFERENCES\\n[1] N. Kandpal, H. Deng, A. Roberts, E. Wallace, and C. Raffel, “Large\\nlanguage models struggle to learn long-tail knowledge,” in Interna-\\ntional Conference on Machine Learning . PMLR, 2023, pp. 15 696–\\n15 707.\\n[2] Y . Zhang, Y . Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\\nY . Zhang, Y . Chenet al., “Siren’s song in the ai ocean: A survey on hal-\\nlucination in large language models,” arXiv preprint arXiv:2309.01219,\\n2023.\\n[3] D. Arora, A. Kini, S. R. Chowdhury, N. Natarajan, G. Sinha, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='lucination in large language models,” arXiv preprint arXiv:2309.01219,\\n2023.\\n[3] D. Arora, A. Kini, S. R. Chowdhury, N. Natarajan, G. Sinha, and\\nA. Sharma, “Gar-meets-rag paradigm for zero-shot information re-\\ntrieval,” arXiv preprint arXiv:2310.20158 , 2023.\\n[4] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,\\nH. K ¨uttler, M. Lewis, W.-t. Yih, T. Rockt ¨aschel et al. , “Retrieval-\\naugmented generation for knowledge-intensive nlp tasks,” Advances in\\nNeural Information Processing Systems, vol. 33, pp. 9459–9474, 2020.\\n[5] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli-\\ncan, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clarket al.,\\n“Improving language models by retrieving from trillions of tokens,”\\nin International conference on machine learning . PMLR, 2022, pp.\\n2206–2240.\\n[6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al. , “Training language'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='2206–2240.\\n[6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al. , “Training language\\nmodels to follow instructions with human feedback,” Advances in\\nneural information processing systems , vol. 35, pp. 27 730–27 744,\\n2022.\\n[7] X. Ma, Y . Gong, P. He, H. Zhao, and N. Duan, “Query rewrit-\\ning for retrieval-augmented large language models,” arXiv preprint\\narXiv:2305.14283, 2023.\\n[8] I. ILIN, “Advanced rag techniques: an il-\\nlustrated overview,” https://pub.towardsai.net/\\nadvanced-rag-techniques-an-illustrated-overview-04d193d8fec6,\\n2023.\\n[9] W. Peng, G. Li, Y . Jiang, Z. Wang, D. Ou, X. Zeng, E. Chen et al. ,\\n“Large language model based long-tail query rewriting in taobao\\nsearch,” arXiv preprint arXiv:2311.03758 , 2023.\\n[10] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V . Le,\\nand D. Zhou, “Take a step back: Evoking reasoning via abstraction in\\nlarge language models,” arXiv preprint arXiv:2310.06117 , 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='and D. Zhou, “Take a step back: Evoking reasoning via abstraction in\\nlarge language models,” arXiv preprint arXiv:2310.06117 , 2023.\\n[11] L. Gao, X. Ma, J. Lin, and J. Callan, “Precise zero-shot dense retrieval\\nwithout relevance labels,” arXiv preprint arXiv:2212.10496 , 2022.\\n[12] V . Blagojevi, “Enhancing rag pipelines in haystack: Introducing diver-\\nsityranker and lostinthemiddleranker,” https://towardsdatascience.com/\\nenhancing-rag-pipelines-in-haystack-45f14e2bc9f5, 2023.\\n[13] W. Yu, D. Iter, S. Wang, Y . Xu, M. Ju, S. Sanyal, C. Zhu, M. Zeng,\\nand M. Jiang, “Generate rather than retrieve: Large language models\\nare strong context generators,” arXiv preprint arXiv:2209.10063, 2022.\\n[14] Z. Shao, Y . Gong, Y . Shen, M. Huang, N. Duan, and W. Chen,\\n“Enhancing retrieval-augmented large language models with iterative\\nretrieval-generation synergy,” arXiv preprint arXiv:2305.15294 , 2023.\\n[15] X. Wang, Q. Yang, Y . Qiu, J. Liang, Q. He, Z. Gu, Y . Xiao,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='retrieval-generation synergy,” arXiv preprint arXiv:2305.15294 , 2023.\\n[15] X. Wang, Q. Yang, Y . Qiu, J. Liang, Q. He, Z. Gu, Y . Xiao,\\nand W. Wang, “Knowledgpt: Enhancing large language models with\\nretrieval and storage access on knowledge bases,” arXiv preprint\\narXiv:2308.11761, 2023.\\n[16] A. H. Raudaschl, “Forget rag, the future\\nis rag-fusion,” https://towardsdatascience.com/\\nforget-rag-the-future-is-rag-fusion-1147298d8ad1, 2023.\\n[17] X. Cheng, D. Luo, X. Chen, L. Liu, D. Zhao, and R. Yan, “Lift\\nyourself up: Retrieval-augmented text generation with self memory,”\\narXiv preprint arXiv:2305.02437 , 2023.\\n[18] S. Wang, Y . Xu, Y . Fang, Y . Liu, S. Sun, R. Xu, C. Zhu, and\\nM. Zeng, “Training data is more valuable than you think: A simple\\nand effective method by retrieving from training data,” arXiv preprint\\narXiv:2203.08773, 2022.\\n[19] X. Li, E. Nie, and S. Liang, “From classification to generation:\\nInsights into crosslingual retrieval augmented icl,” arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='arXiv:2203.08773, 2022.\\n[19] X. Li, E. Nie, and S. Liang, “From classification to generation:\\nInsights into crosslingual retrieval augmented icl,” arXiv preprint\\narXiv:2311.06595, 2023.\\n[20] D. Cheng, S. Huang, J. Bi, Y . Zhan, J. Liu, Y . Wang, H. Sun,\\nF. Wei, D. Deng, and Q. Zhang, “Uprise: Universal prompt retrieval\\nfor improving zero-shot evaluation,” arXiv preprint arXiv:2303.08518,\\n2023.\\n[21] Z. Dai, V . Y . Zhao, J. Ma, Y . Luan, J. Ni, J. Lu, A. Bakalov, K. Guu,\\nK. B. Hall, and M.-W. Chang, “Promptagator: Few-shot dense retrieval\\nfrom 8 examples,” arXiv preprint arXiv:2209.11755 , 2022.\\n[22] Z. Sun, X. Wang, Y . Tay, Y . Yang, and D. Zhou, “Recitation-augmented\\nlanguage models,” arXiv preprint arXiv:2210.01296 , 2022.\\n[23] O. Khattab, K. Santhanam, X. L. Li, D. Hall, P. Liang, C. Potts,\\nand M. Zaharia, “Demonstrate-search-predict: Composing retrieval\\nand language models for knowledge-intensive nlp,” arXiv preprint\\narXiv:2212.14024, 2022.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='and M. Zaharia, “Demonstrate-search-predict: Composing retrieval\\nand language models for knowledge-intensive nlp,” arXiv preprint\\narXiv:2212.14024, 2022.\\n[24] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y . Yang,\\nJ. Callan, and G. Neubig, “Active retrieval augmented generation,”\\narXiv preprint arXiv:2305.06983 , 2023.\\n[25] A. Asai, Z. Wu, Y . Wang, A. Sil, and H. Hajishirzi, “Self-rag:\\nLearning to retrieve, generate, and critique through self-reflection,”\\narXiv preprint arXiv:2310.11511 , 2023.\\n[26] Z. Ke, W. Kong, C. Li, M. Zhang, Q. Mei, and M. Bendersky,\\n“Bridging the preference gap between retrievers and llms,” arXiv\\npreprint arXiv:2401.06954, 2024.\\n[27] X. V . Lin, X. Chen, M. Chen, W. Shi, M. Lomeli, R. James, P. Ro-\\ndriguez, J. Kahn, G. Szilvasy, M. Lewis et al. , “Ra-dit: Retrieval-\\naugmented dual instruction tuning,” arXiv preprint arXiv:2310.01352 ,\\n2023.\\n[28] O. Ovadia, M. Brief, M. Mishaeli, and O. Elisha, “Fine-tuning or'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='augmented dual instruction tuning,” arXiv preprint arXiv:2310.01352 ,\\n2023.\\n[28] O. Ovadia, M. Brief, M. Mishaeli, and O. Elisha, “Fine-tuning or\\nretrieval? comparing knowledge injection in llms,” arXiv preprint\\narXiv:2312.05934, 2023.\\n[29] T. Lan, D. Cai, Y . Wang, H. Huang, and X.-L. Mao, “Copy is all\\nyou need,” in The Eleventh International Conference on Learning\\nRepresentations, 2022.\\n[30] T. Chen, H. Wang, S. Chen, W. Yu, K. Ma, X. Zhao, D. Yu, and\\nH. Zhang, “Dense x retrieval: What retrieval granularity should we\\nuse?” arXiv preprint arXiv:2312.06648 , 2023.\\n[31] F. Luo and M. Surdeanu, “Divide & conquer for entailment-aware\\nmulti-hop evidence retrieval,” arXiv preprint arXiv:2311.02616 , 2023.\\n[32] Q. Gou, Z. Xia, B. Yu, H. Yu, F. Huang, Y . Li, and N. Cam-Tu,\\n“Diversify question generation with retrieval-augmented style transfer,”\\narXiv preprint arXiv:2310.14503 , 2023.\\n[33] Z. Guo, S. Cheng, Y . Wang, P. Li, and Y . Liu, “Prompt-guided re-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='“Diversify question generation with retrieval-augmented style transfer,”\\narXiv preprint arXiv:2310.14503 , 2023.\\n[33] Z. Guo, S. Cheng, Y . Wang, P. Li, and Y . Liu, “Prompt-guided re-\\ntrieval augmentation for non-knowledge-intensive tasks,”arXiv preprint\\narXiv:2305.17653, 2023.\\n[34] Z. Wang, J. Araki, Z. Jiang, M. R. Parvez, and G. Neubig, “Learning\\nto filter context for retrieval-augmented generation,” arXiv preprint\\narXiv:2311.08377, 2023.\\n[35] M. Seo, J. Baek, J. Thorne, and S. J. Hwang, “Retrieval-augmented\\ndata augmentation for low-resource domain tasks,” arXiv preprint\\narXiv:2402.13482, 2024.\\n[36] Y . Ma, Y . Cao, Y . Hong, and A. Sun, “Large language model is not\\na good few-shot information extractor, but a good reranker for hard\\nsamples!” arXiv preprint arXiv:2303.08559 , 2023.\\n[37] X. Du and H. Ji, “Retrieval-augmented generative question answering\\nfor event argument extraction,” arXiv preprint arXiv:2211.07067, 2022.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='samples!” arXiv preprint arXiv:2303.08559 , 2023.\\n[37] X. Du and H. Ji, “Retrieval-augmented generative question answering\\nfor event argument extraction,” arXiv preprint arXiv:2211.07067, 2022.\\n[38] L. Wang, N. Yang, and F. Wei, “Learning to retrieve in-context\\nexamples for large language models,”arXiv preprint arXiv:2307.07164,\\n2023.\\n[39] S. Rajput, N. Mehta, A. Singh, R. H. Keshavan, T. Vu, L. Heldt,\\nL. Hong, Y . Tay, V . Q. Tran, J. Samostet al., “Recommender systems\\nwith generative retrieval,” arXiv preprint arXiv:2305.05065 , 2023.\\n[40] B. Jin, H. Zeng, G. Wang, X. Chen, T. Wei, R. Li, Z. Wang, Z. Li,\\nY . Li, H. Lu et al. , “Language models as semantic indexers,” arXiv\\npreprint arXiv:2310.07815, 2023.\\n[41] R. Anantha, T. Bethi, D. V odianik, and S. Chappidi, “Context tuning\\nfor retrieval augmented generation,” arXiv preprint arXiv:2312.05708 ,\\n2023.\\n[42] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='for retrieval augmented generation,” arXiv preprint arXiv:2312.05708 ,\\n2023.\\n[42] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\\nJ. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, “Few-shot\\nlearning with retrieval augmented language models,” arXiv preprint\\narXiv:2208.03299, 2022.\\n[43] J. Huang, W. Ping, P. Xu, M. Shoeybi, K. C.-C. Chang, and B. Catan-\\nzaro, “Raven: In-context learning with retrieval augmented encoder-\\ndecoder language models,” arXiv preprint arXiv:2308.07922 , 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='18\\n[44] B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi, Y . Dong,\\nO. Kuchaiev, B. Li, C. Xiao et al. , “Shall we pretrain autoregressive\\nlanguage models with retrieval? a comprehensive study,”arXiv preprint\\narXiv:2304.06762, 2023.\\n[45] B. Wang, W. Ping, L. McAfee, P. Xu, B. Li, M. Shoeybi, and B. Catan-\\nzaro, “Instructretro: Instruction tuning post retrieval-augmented pre-\\ntraining,” arXiv preprint arXiv:2310.07713 , 2023.\\n[46] S. Siriwardhana, R. Weerasekera, E. Wen, T. Kaluarachchi, R. Rana,\\nand S. Nanayakkara, “Improving the domain adaptation of retrieval\\naugmented generation (rag) models for open domain question answer-\\ning,” Transactions of the Association for Computational Linguistics ,\\nvol. 11, pp. 1–17, 2023.\\n[47] Z. Yu, C. Xiong, S. Yu, and Z. Liu, “Augmentation-adapted retriever\\nimproves generalization of language models as generic plug-in,” arXiv\\npreprint arXiv:2305.17331, 2023.\\n[48] O. Yoran, T. Wolfson, O. Ram, and J. Berant, “Making retrieval-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='improves generalization of language models as generic plug-in,” arXiv\\npreprint arXiv:2305.17331, 2023.\\n[48] O. Yoran, T. Wolfson, O. Ram, and J. Berant, “Making retrieval-\\naugmented language models robust to irrelevant context,” arXiv\\npreprint arXiv:2310.01558, 2023.\\n[49] H.-T. Chen, F. Xu, S. A. Arora, and E. Choi, “Understanding re-\\ntrieval augmentation for long-form question answering,” arXiv preprint\\narXiv:2310.12150, 2023.\\n[50] W. Yu, H. Zhang, X. Pan, K. Ma, H. Wang, and D. Yu, “Chain-of-note:\\nEnhancing robustness in retrieval-augmented language models,” arXiv\\npreprint arXiv:2311.09210, 2023.\\n[51] S. Xu, L. Pang, H. Shen, X. Cheng, and T.-S. Chua, “Search-in-the-\\nchain: Towards accurate, credible and traceable large language models\\nfor knowledgeintensive tasks,” CoRR, vol. abs/2304.14732 , 2023.\\n[52] M. Berchansky, P. Izsak, A. Caciularu, I. Dagan, and M. Wasserblat,\\n“Optimizing retrieval-augmented reader models via token elimination,”\\narXiv preprint arXiv:2310.13682 , 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='[52] M. Berchansky, P. Izsak, A. Caciularu, I. Dagan, and M. Wasserblat,\\n“Optimizing retrieval-augmented reader models via token elimination,”\\narXiv preprint arXiv:2310.13682 , 2023.\\n[53] J. L ´ala, O. O’Donoghue, A. Shtedritski, S. Cox, S. G. Rodriques,\\nand A. D. White, “Paperqa: Retrieval-augmented generative agent for\\nscientific research,” arXiv preprint arXiv:2312.07559 , 2023.\\n[54] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano,\\nY . Maarek, N. Tonellotto, and F. Silvestri, “The power of noise:\\nRedefining retrieval for rag systems,” arXiv preprint arXiv:2401.14887,\\n2024.\\n[55] Z. Zhang, X. Zhang, Y . Ren, S. Shi, M. Han, Y . Wu, R. Lai, and\\nZ. Cao, “Iag: Induction-augmented generation framework for answer-\\ning reasoning questions,” in Proceedings of the 2023 Conference on\\nEmpirical Methods in Natural Language Processing , 2023, pp. 1–14.\\n[56] N. Thakur, L. Bonifacio, X. Zhang, O. Ogundepo, E. Kamalloo,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='ing reasoning questions,” in Proceedings of the 2023 Conference on\\nEmpirical Methods in Natural Language Processing , 2023, pp. 1–14.\\n[56] N. Thakur, L. Bonifacio, X. Zhang, O. Ogundepo, E. Kamalloo,\\nD. Alfonso-Hermelo, X. Li, Q. Liu, B. Chen, M. Rezagholizadeh et al.,\\n“Nomiracl: Knowing when you don’t know for robust multilingual\\nretrieval-augmented generation,” arXiv preprint arXiv:2312.11361 ,\\n2023.\\n[57] G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, “Tree of clarifica-\\ntions: Answering ambiguous questions with retrieval-augmented large\\nlanguage models,” arXiv preprint arXiv:2310.14696 , 2023.\\n[58] Y . Wang, P. Li, M. Sun, and Y . Liu, “Self-knowledge guided\\nretrieval augmentation for large language models,” arXiv preprint\\narXiv:2310.05002, 2023.\\n[59] Z. Feng, X. Feng, D. Zhao, M. Yang, and B. Qin, “Retrieval-\\ngeneration synergy augmented large language models,” arXiv preprint\\narXiv:2310.05149, 2023.\\n[60] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='generation synergy augmented large language models,” arXiv preprint\\narXiv:2310.05149, 2023.\\n[60] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian,\\nE. Bakhturina, M. Shoeybi, and B. Catanzaro, “Retrieval meets long\\ncontext large language models,” arXiv preprint arXiv:2310.03025 ,\\n2023.\\n[61] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Interleav-\\ning retrieval with chain-of-thought reasoning for knowledge-intensive\\nmulti-step questions,” arXiv preprint arXiv:2212.10509 , 2022.\\n[62] R. Ren, Y . Wang, Y . Qu, W. X. Zhao, J. Liu, H. Tian, H. Wu, J.-\\nR. Wen, and H. Wang, “Investigating the factual knowledge boundary\\nof large language models with retrieval augmentation,” arXiv preprint\\narXiv:2307.11019, 2023.\\n[63] P. Sarthi, S. Abdullah, A. Tuli, S. Khanna, A. Goldie, and C. D.\\nManning, “Raptor: Recursive abstractive processing for tree-organized\\nretrieval,” arXiv preprint arXiv:2401.18059 , 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='[63] P. Sarthi, S. Abdullah, A. Tuli, S. Khanna, A. Goldie, and C. D.\\nManning, “Raptor: Recursive abstractive processing for tree-organized\\nretrieval,” arXiv preprint arXiv:2401.18059 , 2024.\\n[64] O. Ram, Y . Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-\\nBrown, and Y . Shoham, “In-context retrieval-augmented language\\nmodels,” arXiv preprint arXiv:2302.00083 , 2023.\\n[65] Y . Ren, Y . Cao, P. Guo, F. Fang, W. Ma, and Z. Lin, “Retrieve-and-\\nsample: Document-level event argument extraction via hybrid retrieval\\naugmentation,” in Proceedings of the 61st Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers) ,\\n2023, pp. 293–306.\\n[66] Z. Wang, X. Pan, D. Yu, D. Yu, J. Chen, and H. Ji, “Zemi: Learning\\nzero-shot semi-parametric language models from multiple tasks,” arXiv\\npreprint arXiv:2210.00185, 2022.\\n[67] S.-Q. Yan, J.-C. Gu, Y . Zhu, and Z.-H. Ling, “Corrective retrieval\\naugmented generation,” arXiv preprint arXiv:2401.15884 , 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='preprint arXiv:2210.00185, 2022.\\n[67] S.-Q. Yan, J.-C. Gu, Y . Zhu, and Z.-H. Ling, “Corrective retrieval\\naugmented generation,” arXiv preprint arXiv:2401.15884 , 2024.\\n[68] P. Jain, L. B. Soares, and T. Kwiatkowski, “1-pager: One pass answer\\ngeneration and evidence retrieval,” arXiv preprint arXiv:2310.16568 ,\\n2023.\\n[69] H. Yang, Z. Li, Y . Zhang, J. Wang, N. Cheng, M. Li, and J. Xiao, “Prca:\\nFitting black-box large language models for retrieval question answer-\\ning via pluggable reward-driven contextual adapter,” arXiv preprint\\narXiv:2310.18347, 2023.\\n[70] S. Zhuang, B. Liu, B. Koopman, and G. Zuccon, “Open-source large\\nlanguage models are strong zero-shot query likelihood models for\\ndocument ranking,” arXiv preprint arXiv:2310.13243 , 2023.\\n[71] F. Xu, W. Shi, and E. Choi, “Recomp: Improving retrieval-augmented\\nlms with compression and selective augmentation,” arXiv preprint\\narXiv:2310.04408, 2023.\\n[72] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='lms with compression and selective augmentation,” arXiv preprint\\narXiv:2310.04408, 2023.\\n[72] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-\\nmoyer, and W.-t. Yih, “Replug: Retrieval-augmented black-box lan-\\nguage models,” arXiv preprint arXiv:2301.12652 , 2023.\\n[73] E. Melz, “Enhancing llm intelligence with arm-rag: Auxiliary ra-\\ntionale memory for retrieval augmented generation,” arXiv preprint\\narXiv:2311.04177, 2023.\\n[74] H. Wang, W. Huang, Y . Deng, R. Wang, Z. Wang, Y . Wang, F. Mi,\\nJ. Z. Pan, and K.-F. Wong, “Unims-rag: A unified multi-source\\nretrieval-augmented generation for personalized dialogue systems,”\\narXiv preprint arXiv:2401.13256 , 2024.\\n[75] Z. Luo, C. Xu, P. Zhao, X. Geng, C. Tao, J. Ma, Q. Lin, and D. Jiang,\\n“Augmented large language models with parametric knowledge guid-\\ning,” arXiv preprint arXiv:2305.04757 , 2023.\\n[76] X. Li, Z. Liu, C. Xiong, S. Yu, Y . Gu, Z. Liu, and G. Yu, “Structure-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='“Augmented large language models with parametric knowledge guid-\\ning,” arXiv preprint arXiv:2305.04757 , 2023.\\n[76] X. Li, Z. Liu, C. Xiong, S. Yu, Y . Gu, Z. Liu, and G. Yu, “Structure-\\naware language model pretraining improves dense retrieval on struc-\\ntured data,” arXiv preprint arXiv:2305.19912 , 2023.\\n[77] M. Kang, J. M. Kwak, J. Baek, and S. J. Hwang, “Knowledge\\ngraph-augmented language models for knowledge-grounded dialogue\\ngeneration,” arXiv preprint arXiv:2305.18846 , 2023.\\n[78] W. Shen, Y . Gao, C. Huang, F. Wan, X. Quan, and W. Bi, “Retrieval-\\ngeneration alignment for end-to-end task-oriented dialogue system,”\\narXiv preprint arXiv:2310.08877 , 2023.\\n[79] T. Shi, L. Li, Z. Lin, T. Yang, X. Quan, and Q. Wang, “Dual-feedback\\nknowledge retrieval for task-oriented dialogue systems,” arXiv preprint\\narXiv:2310.14528, 2023.\\n[80] P. Ranade and A. Joshi, “Fabula: Intelligence report generation\\nusing retrieval-augmented narrative construction,” arXiv preprint\\narXiv:2310.13848, 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='arXiv:2310.14528, 2023.\\n[80] P. Ranade and A. Joshi, “Fabula: Intelligence report generation\\nusing retrieval-augmented narrative construction,” arXiv preprint\\narXiv:2310.13848, 2023.\\n[81] X. Jiang, R. Zhang, Y . Xu, R. Qiu, Y . Fang, Z. Wang, J. Tang,\\nH. Ding, X. Chu, J. Zhao et al. , “Think and retrieval: A hypothesis\\nknowledge graph enhanced medical large language models,” arXiv\\npreprint arXiv:2312.15883, 2023.\\n[82] J. Baek, S. Jeong, M. Kang, J. C. Park, and S. J. Hwang,\\n“Knowledge-augmented language model verification,” arXiv preprint\\narXiv:2310.12836, 2023.\\n[83] L. Luo, Y .-F. Li, G. Haffari, and S. Pan, “Reasoning on graphs: Faithful\\nand interpretable large language model reasoning,” arXiv preprint\\narXiv:2310.01061, 2023.\\n[84] X. He, Y . Tian, Y . Sun, N. V . Chawla, T. Laurent, Y . LeCun,\\nX. Bresson, and B. Hooi, “G-retriever: Retrieval-augmented generation\\nfor textual graph understanding and question answering,”arXiv preprint\\narXiv:2402.07630, 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='X. Bresson, and B. Hooi, “G-retriever: Retrieval-augmented generation\\nfor textual graph understanding and question answering,”arXiv preprint\\narXiv:2402.07630, 2024.\\n[85] L. Zha, J. Zhou, L. Li, R. Wang, Q. Huang, S. Yang, J. Yuan, C. Su,\\nX. Li, A. Su et al., “Tablegpt: Towards unifying tables, nature language\\nand commands into one gpt,” arXiv preprint arXiv:2307.08674 , 2023.\\n[86] M. Gaur, K. Gunaratna, V . Srinivasan, and H. Jin, “Iseeq: Information\\nseeking question generation using dynamic meta-information retrieval\\nand knowledge graphs,” in Proceedings of the AAAI Conference on\\nArtificial Intelligence, vol. 36, no. 10, 2022, pp. 10 672–10 680.\\n[87] F. Shi, X. Chen, K. Misra, N. Scales, D. Dohan, E. H. Chi, N. Sch ¨arli,\\nand D. Zhou, “Large language models can be easily distracted by\\nirrelevant context,” in International Conference on Machine Learning .\\nPMLR, 2023, pp. 31 210–31 227.\\n[88] R. Teja, “Evaluating the ideal chunk size for a rag'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='irrelevant context,” in International Conference on Machine Learning .\\nPMLR, 2023, pp. 31 210–31 227.\\n[88] R. Teja, “Evaluating the ideal chunk size for a rag\\nsystem using llamaindex,” https://www.llamaindex.ai/blog/\\nevaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5,\\n2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='19\\n[89] Langchain, “Recursively split by character,” https://python.langchain.\\ncom/docs/modules/data connection/document transformers/recursive\\ntext splitter, 2023.\\n[90] S. Yang, “Advanced rag 01: Small-to-\\nbig retrieval,” https://towardsdatascience.com/\\nadvanced-rag-01-small-to-big-retrieval-172181b396d4, 2023.\\n[91] Y . Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and T. Derr,\\n“Knowledge graph prompting for multi-document question answering,”\\narXiv preprint arXiv:2308.11730 , 2023.\\n[92] D. Zhou, N. Sch ¨arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schu-\\nurmans, C. Cui, O. Bousquet, Q. Le et al., “Least-to-most prompting\\nenables complex reasoning in large language models,” arXiv preprint\\narXiv:2205.10625, 2022.\\n[93] S. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz,\\nand J. Weston, “Chain-of-verification reduces hallucination in large\\nlanguage models,” arXiv preprint arXiv:2309.11495 , 2023.\\n[94] X. Li and J. Li, “Angle-optimized text embeddings,” arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='and J. Weston, “Chain-of-verification reduces hallucination in large\\nlanguage models,” arXiv preprint arXiv:2309.11495 , 2023.\\n[94] X. Li and J. Li, “Angle-optimized text embeddings,” arXiv preprint\\narXiv:2309.12871, 2023.\\n[95] V oyageAI, “V oyage’s embedding models,” https://docs.voyageai.com/\\nembeddings/, 2023.\\n[96] BAAI, “Flagembedding,” https://github.com/FlagOpen/\\nFlagEmbedding, 2023.\\n[97] P. Zhang, S. Xiao, Z. Liu, Z. Dou, and J.-Y . Nie, “Retrieve anything\\nto augment large language models,” arXiv preprint arXiv:2310.07554 ,\\n2023.\\n[98] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni,\\nand P. Liang, “Lost in the middle: How language models use long\\ncontexts,” arXiv preprint arXiv:2307.03172 , 2023.\\n[99] Y . Gao, T. Sheng, Y . Xiang, Y . Xiong, H. Wang, and J. Zhang, “Chat-\\nrec: Towards interactive and explainable llms-augmented recommender\\nsystem,” arXiv preprint arXiv:2303.14524 , 2023.\\n[100] N. Anderson, C. Wilson, and S. D. Richardson, “Lingua: Addressing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='rec: Towards interactive and explainable llms-augmented recommender\\nsystem,” arXiv preprint arXiv:2303.14524 , 2023.\\n[100] N. Anderson, C. Wilson, and S. D. Richardson, “Lingua: Addressing\\nscenarios for live interpretation and automatic dubbing,” inProceedings\\nof the 15th Biennial Conference of the Association for Machine\\nTranslation in the Americas (Volume 2: Users and Providers Track\\nand Government Track) , J. Campbell, S. Larocca, J. Marciano,\\nK. Savenkov, and A. Yanishevsky, Eds. Orlando, USA: Association\\nfor Machine Translation in the Americas, Sep. 2022, pp. 202–209.\\n[Online]. Available: https://aclanthology.org/2022.amta-upg.14\\n[101] H. Jiang, Q. Wu, X. Luo, D. Li, C.-Y . Lin, Y . Yang, and L. Qiu,\\n“Longllmlingua: Accelerating and enhancing llms in long context\\nscenarios via prompt compression,” arXiv preprint arXiv:2310.06839 ,\\n2023.\\n[102] V . Karpukhin, B. O ˘guz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen,\\nand W.-t. Yih, “Dense passage retrieval for open-domain question'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='2023.\\n[102] V . Karpukhin, B. O ˘guz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen,\\nand W.-t. Yih, “Dense passage retrieval for open-domain question\\nanswering,” arXiv preprint arXiv:2004.04906 , 2020.\\n[103] Y . Ma, Y . Cao, Y . Hong, and A. Sun, “Large language model is\\nnot a good few-shot information extractor, but a good reranker for\\nhard samples!” ArXiv, vol. abs/2303.08559, 2023. [Online]. Available:\\nhttps://api.semanticscholar.org/CorpusID:257532405\\n[104] J. Cui, Z. Li, Y . Yan, B. Chen, and L. Yuan, “Chatlaw: Open-source\\nlegal large language model with integrated external knowledge bases,”\\narXiv preprint arXiv:2306.16092 , 2023.\\n[105] O. Yoran, T. Wolfson, O. Ram, and J. Berant, “Making retrieval-\\naugmented language models robust to irrelevant context,” arXiv\\npreprint arXiv:2310.01558, 2023.\\n[106] X. Li, R. Zhao, Y . K. Chia, B. Ding, L. Bing, S. Joty, and S. Poria,\\n“Chain of knowledge: A framework for grounding large language mod-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='preprint arXiv:2310.01558, 2023.\\n[106] X. Li, R. Zhao, Y . K. Chia, B. Ding, L. Bing, S. Joty, and S. Poria,\\n“Chain of knowledge: A framework for grounding large language mod-\\nels with structured knowledge bases,”arXiv preprint arXiv:2305.13269,\\n2023.\\n[107] H. Yang, S. Yue, and Y . He, “Auto-gpt for online decision\\nmaking: Benchmarks and additional opinions,” arXiv preprint\\narXiv:2306.02224, 2023.\\n[108] T. Schick, J. Dwivedi-Yu, R. Dess `ı, R. Raileanu, M. Lomeli, L. Zettle-\\nmoyer, N. Cancedda, and T. Scialom, “Toolformer: Language models\\ncan teach themselves to use tools,” arXiv preprint arXiv:2302.04761 ,\\n2023.\\n[109] J. Zhang, “Graph-toolformer: To empower llms with graph rea-\\nsoning ability via prompt augmented by chatgpt,” arXiv preprint\\narXiv:2304.11116, 2023.\\n[110] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\\nC. Hesse, S. Jain, V . Kosaraju, W. Saunders et al., “Webgpt: Browser-\\nassisted question-answering with human feedback,” arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='C. Hesse, S. Jain, V . Kosaraju, W. Saunders et al., “Webgpt: Browser-\\nassisted question-answering with human feedback,” arXiv preprint\\narXiv:2112.09332, 2021.\\n[111] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh,\\nC. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee et al., “Natural\\nquestions: a benchmark for question answering research,” Transactions\\nof the Association for Computational Linguistics , vol. 7, pp. 453–466,\\n2019.\\n[112] Y . Liu, S. Yavuz, R. Meng, M. Moorthy, S. Joty, C. Xiong, and Y . Zhou,\\n“Exploring the integration strategies of retriever and large language\\nmodels,” arXiv preprint arXiv:2308.12574 , 2023.\\n[113] M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer, “Triviaqa: A large\\nscale distantly supervised challenge dataset for reading comprehen-\\nsion,” arXiv preprint arXiv:1705.03551 , 2017.\\n[114] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad: 100,000+\\nquestions for machine comprehension of text,” arXiv preprint\\narXiv:1606.05250, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='[114] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad: 100,000+\\nquestions for machine comprehension of text,” arXiv preprint\\narXiv:1606.05250, 2016.\\n[115] J. Berant, A. Chou, R. Frostig, and P. Liang, “Semantic parsing on\\nfreebase from question-answer pairs,” in Proceedings of the 2013\\nconference on empirical methods in natural language processing, 2013,\\npp. 1533–1544.\\n[116] A. Mallen, A. Asai, V . Zhong, R. Das, H. Hajishirzi, and D. Khashabi,\\n“When not to trust language models: Investigating effectiveness and\\nlimitations of parametric and non-parametric memories,” arXiv preprint\\narXiv:2212.10511, 2022.\\n[117] T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder,\\nand L. Deng, “Ms marco: A human-generated machine reading com-\\nprehension dataset,” 2016.\\n[118] Z. Yang, P. Qi, S. Zhang, Y . Bengio, W. W. Cohen, R. Salakhutdi-\\nnov, and C. D. Manning, “Hotpotqa: A dataset for diverse, explain-\\nable multi-hop question answering,” arXiv preprint arXiv:1809.09600,\\n2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='nov, and C. D. Manning, “Hotpotqa: A dataset for diverse, explain-\\nable multi-hop question answering,” arXiv preprint arXiv:1809.09600,\\n2018.\\n[119] X. Ho, A.-K. D. Nguyen, S. Sugawara, and A. Aizawa, “Constructing a\\nmulti-hop qa dataset for comprehensive evaluation of reasoning steps,”\\narXiv preprint arXiv:2011.01060 , 2020.\\n[120] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Musique:\\nMultihop questions via single-hop question composition,” Transactions\\nof the Association for Computational Linguistics , vol. 10, pp. 539–554,\\n2022.\\n[121] A. Fan, Y . Jernite, E. Perez, D. Grangier, J. Weston, and M. Auli, “Eli5:\\nLong form question answering,” arXiv preprint arXiv:1907.09190 ,\\n2019.\\n[122] T. Ko ˇcisk`y, J. Schwarz, P. Blunsom, C. Dyer, K. M. Hermann, G. Melis,\\nand E. Grefenstette, “The narrativeqa reading comprehension chal-\\nlenge,” Transactions of the Association for Computational Linguistics ,\\nvol. 6, pp. 317–328, 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='and E. Grefenstette, “The narrativeqa reading comprehension chal-\\nlenge,” Transactions of the Association for Computational Linguistics ,\\nvol. 6, pp. 317–328, 2018.\\n[123] K.-H. Lee, X. Chen, H. Furuta, J. Canny, and I. Fischer, “A human-\\ninspired reading agent with gist memory of very long contexts,” arXiv\\npreprint arXiv:2402.09727, 2024.\\n[124] I. Stelmakh, Y . Luan, B. Dhingra, and M.-W. Chang, “Asqa: Factoid\\nquestions meet long-form answers,” arXiv preprint arXiv:2204.06092 ,\\n2022.\\n[125] M. Zhong, D. Yin, T. Yu, A. Zaidi, M. Mutuma, R. Jha, A. H.\\nAwadallah, A. Celikyilmaz, Y . Liu, X. Qiu et al. , “Qmsum: A new\\nbenchmark for query-based multi-domain meeting summarization,”\\narXiv preprint arXiv:2104.05938 , 2021.\\n[126] P. Dasigi, K. Lo, I. Beltagy, A. Cohan, N. A. Smith, and M. Gardner,\\n“A dataset of information-seeking questions and answers anchored in\\nresearch papers,” arXiv preprint arXiv:2105.03011 , 2021.\\n[127] T. M ¨oller, A. Reina, R. Jayakumar, and M. Pietsch, “Covid-qa: A'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='“A dataset of information-seeking questions and answers anchored in\\nresearch papers,” arXiv preprint arXiv:2105.03011 , 2021.\\n[127] T. M ¨oller, A. Reina, R. Jayakumar, and M. Pietsch, “Covid-qa: A\\nquestion answering dataset for covid-19,” in ACL 2020 Workshop on\\nNatural Language Processing for COVID-19 (NLP-COVID) , 2020.\\n[128] X. Wang, G. H. Chen, D. Song, Z. Zhang, Z. Chen, Q. Xiao, F. Jiang,\\nJ. Li, X. Wan, B. Wang et al. , “Cmb: A comprehensive medical\\nbenchmark in chinese,” arXiv preprint arXiv:2308.08833 , 2023.\\n[129] H. Zeng, “Measuring massive multitask chinese understanding,” arXiv\\npreprint arXiv:2304.12986, 2023.\\n[130] R. Y . Pang, A. Parrish, N. Joshi, N. Nangia, J. Phang, A. Chen, V . Pad-\\nmakumar, J. Ma, J. Thompson, H. He et al. , “Quality: Question an-\\nswering with long input texts, yes!” arXiv preprint arXiv:2112.08608 ,\\n2021.\\n[131] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\\nand O. Tafjord, “Think you have solved question answering? try arc,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='2021.\\n[131] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\\nand O. Tafjord, “Think you have solved question answering? try arc,\\nthe ai2 reasoning challenge,” arXiv preprint arXiv:1803.05457 , 2018.\\n[132] A. Talmor, J. Herzig, N. Lourie, and J. Berant, “Commonsenseqa:\\nA question answering challenge targeting commonsense knowledge,”\\narXiv preprint arXiv:1811.00937 , 2018.\\n[133] E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, and J. Weston,\\n“Wizard of wikipedia: Knowledge-powered conversational agents,”\\narXiv preprint arXiv:1811.01241 , 2018.\\n[134] H. Wang, M. Hu, Y . Deng, R. Wang, F. Mi, W. Wang, Y . Wang, W.-\\nC. Kwan, I. King, and K.-F. Wong, “Large language models as source'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='20\\nplanner for personalized knowledge-grounded dialogue,” arXiv preprint\\narXiv:2310.08840, 2023.\\n[135] ——, “Large language models as source planner for personal-\\nized knowledge-grounded dialogue,” arXiv preprint arXiv:2310.08840,\\n2023.\\n[136] X. Xu, Z. Gou, W. Wu, Z.-Y . Niu, H. Wu, H. Wang, and S. Wang,\\n“Long time no see! open-domain conversation with long-term persona\\nmemory,” arXiv preprint arXiv:2203.05797 , 2022.\\n[137] T.-H. Wen, M. Gasic, N. Mrksic, L. M. Rojas-Barahona, P.-H.\\nSu, S. Ultes, D. Vandyke, and S. Young, “Conditional generation\\nand snapshot learning in neural dialogue systems,” arXiv preprint\\narXiv:1606.03352, 2016.\\n[138] R. He and J. McAuley, “Ups and downs: Modeling the visual evolution\\nof fashion trends with one-class collaborative filtering,” in proceedings\\nof the 25th international conference on world wide web , 2016, pp.\\n507–517.\\n[139] S. Li, H. Ji, and J. Han, “Document-level event argument extraction'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='of the 25th international conference on world wide web , 2016, pp.\\n507–517.\\n[139] S. Li, H. Ji, and J. Han, “Document-level event argument extraction\\nby conditional generation,” arXiv preprint arXiv:2104.05919 , 2021.\\n[140] S. Ebner, P. Xia, R. Culkin, K. Rawlins, and B. Van Durme, “Multi-\\nsentence argument linking,” arXiv preprint arXiv:1911.03766 , 2019.\\n[141] H. Elsahar, P. V ougiouklis, A. Remaci, C. Gravier, J. Hare, F. Laforest,\\nand E. Simperl, “T-rex: A large scale alignment of natural language\\nwith knowledge base triples,” in Proceedings of the Eleventh Inter-\\nnational Conference on Language Resources and Evaluation (LREC\\n2018), 2018.\\n[142] O. Levy, M. Seo, E. Choi, and L. Zettlemoyer, “Zero-shot relation ex-\\ntraction via reading comprehension,” arXiv preprint arXiv:1706.04115,\\n2017.\\n[143] R. Zellers, A. Holtzman, Y . Bisk, A. Farhadi, and Y . Choi, “Hel-\\nlaswag: Can a machine really finish your sentence?” arXiv preprint\\narXiv:1905.07830, 2019.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='2017.\\n[143] R. Zellers, A. Holtzman, Y . Bisk, A. Farhadi, and Y . Choi, “Hel-\\nlaswag: Can a machine really finish your sentence?” arXiv preprint\\narXiv:1905.07830, 2019.\\n[144] S. Kim, S. J. Joo, D. Kim, J. Jang, S. Ye, J. Shin, and M. Seo,\\n“The cot collection: Improving zero-shot and few-shot learning of\\nlanguage models via chain-of-thought fine-tuning,” arXiv preprint\\narXiv:2305.14045, 2023.\\n[145] A. Saha, V . Pahuja, M. Khapra, K. Sankaranarayanan, and S. Chandar,\\n“Complex sequential question answering: Towards learning to converse\\nover linked question answer pairs with a knowledge graph,” inProceed-\\nings of the AAAI conference on artificial intelligence , vol. 32, no. 1,\\n2018.\\n[146] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\\nJ. Steinhardt, “Measuring massive multitask language understanding,”\\narXiv preprint arXiv:2009.03300 , 2020.\\n[147] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer sentinel'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='J. Steinhardt, “Measuring massive multitask language understanding,”\\narXiv preprint arXiv:2009.03300 , 2020.\\n[147] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer sentinel\\nmixture models,” arXiv preprint arXiv:1609.07843 , 2016.\\n[148] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant,\\n“Did aristotle use a laptop? a question answering benchmark with\\nimplicit reasoning strategies,” Transactions of the Association for\\nComputational Linguistics, vol. 9, pp. 346–361, 2021.\\n[149] J. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal, “Fever: a\\nlarge-scale dataset for fact extraction and verification,” arXiv preprint\\narXiv:1803.05355, 2018.\\n[150] N. Kotonya and F. Toni, “Explainable automated fact-checking for\\npublic health claims,” arXiv preprint arXiv:2010.09926 , 2020.\\n[151] R. Lebret, D. Grangier, and M. Auli, “Neural text generation from\\nstructured data with application to the biography domain,” arXiv\\npreprint arXiv:1603.07771, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='[151] R. Lebret, D. Grangier, and M. Auli, “Neural text generation from\\nstructured data with application to the biography domain,” arXiv\\npreprint arXiv:1603.07771, 2016.\\n[152] H. Hayashi, P. Budania, P. Wang, C. Ackerson, R. Neervannan,\\nand G. Neubig, “Wikiasp: A dataset for multi-domain aspect-based\\nsummarization,” Transactions of the Association for Computational\\nLinguistics, vol. 9, pp. 211–225, 2021.\\n[153] S. Narayan, S. B. Cohen, and M. Lapata, “Don’t give me the details,\\njust the summary! topic-aware convolutional neural networks for ex-\\ntreme summarization,” arXiv preprint arXiv:1808.08745 , 2018.\\n[154] S. Saha, J. A. Junaed, M. Saleki, A. S. Sharma, M. R. Rifat, M. Rahouti,\\nS. I. Ahmed, N. Mohammed, and M. R. Amin, “Vio-lens: A novel\\ndataset of annotated social network posts leading to different forms\\nof communal violence and its evaluation,” in Proceedings of the First\\nWorkshop on Bangla Language Processing (BLP-2023), 2023, pp. 72–\\n84.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='of communal violence and its evaluation,” in Proceedings of the First\\nWorkshop on Bangla Language Processing (BLP-2023), 2023, pp. 72–\\n84.\\n[155] X. Li and D. Roth, “Learning question classifiers,” in COLING 2002:\\nThe 19th International Conference on Computational Linguistics, 2002.\\n[156] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y . Ng,\\nand C. Potts, “Recursive deep models for semantic compositionality\\nover a sentiment treebank,” in Proceedings of the 2013 conference on\\nempirical methods in natural language processing , 2013, pp. 1631–\\n1642.\\n[157] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,\\n“Codesearchnet challenge: Evaluating the state of semantic code\\nsearch,” arXiv preprint arXiv:1909.09436 , 2019.\\n[158] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\\nM. Plappert, J. Tworek, J. Hilton, R. Nakano et al., “Training verifiers\\nto solve math word problems,” arXiv preprint arXiv:2110.14168, 2021.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='M. Plappert, J. Tworek, J. Hilton, R. Nakano et al., “Training verifiers\\nto solve math word problems,” arXiv preprint arXiv:2110.14168, 2021.\\n[159] R. Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Erjavec, D. Tufis,\\nand D. Varga, “The jrc-acquis: A multilingual aligned parallel corpus\\nwith 20+ languages,” arXiv preprint cs/0609058 , 2006.\\n[160] Y . Hoshi, D. Miyashita, Y . Ng, K. Tatsuno, Y . Morioka, O. Torii,\\nand J. Deguchi, “Ralle: A framework for developing and eval-\\nuating retrieval-augmented large language models,” arXiv preprint\\narXiv:2308.10633, 2023.\\n[161] J. Liu, “Building production-ready rag applications,” https://www.ai.\\nengineer/summit/schedule/building-production-ready-rag-applications,\\n2023.\\n[162] I. Nguyen, “Evaluating rag part i: How to evaluate document retrieval,”\\nhttps://www.deepset.ai/blog/rag-evaluation-retrieval, 2023.\\n[163] Q. Leng, K. Uhlenhuth, and A. Polyzotis, “Best practices for\\nllm evaluation of rag applications,” https://www.databricks.com/blog/'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='https://www.deepset.ai/blog/rag-evaluation-retrieval, 2023.\\n[163] Q. Leng, K. Uhlenhuth, and A. Polyzotis, “Best practices for\\nllm evaluation of rag applications,” https://www.databricks.com/blog/\\nLLM-auto-eval-best-practices-RAG, 2023.\\n[164] S. Es, J. James, L. Espinosa-Anke, and S. Schockaert, “Ragas: Au-\\ntomated evaluation of retrieval augmented generation,” arXiv preprint\\narXiv:2309.15217, 2023.\\n[165] J. Saad-Falcon, O. Khattab, C. Potts, and M. Zaharia, “Ares: An\\nautomated evaluation framework for retrieval-augmented generation\\nsystems,” arXiv preprint arXiv:2311.09476 , 2023.\\n[166] C. Jarvis and J. Allard, “A survey of techniques for\\nmaximizing llm performance,” https://community.openai.\\ncom/t/openai-dev-day-2023-breakout-sessions/505213#\\na-survey-of-techniques-for-maximizing-llm-performance-2, 2023.\\n[167] J. Chen, H. Lin, X. Han, and L. Sun, “Benchmarking large lan-\\nguage models in retrieval-augmented generation,” arXiv preprint\\narXiv:2309.01431, 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='[167] J. Chen, H. Lin, X. Han, and L. Sun, “Benchmarking large lan-\\nguage models in retrieval-augmented generation,” arXiv preprint\\narXiv:2309.01431, 2023.\\n[168] Y . Liu, L. Huang, S. Li, S. Chen, H. Zhou, F. Meng, J. Zhou, and\\nX. Sun, “Recall: A benchmark for llms robustness against external\\ncounterfactual knowledge,” arXiv preprint arXiv:2311.08147 , 2023.\\n[169] Y . Lyu, Z. Li, S. Niu, F. Xiong, B. Tang, W. Wang, H. Wu, H. Liu,\\nT. Xu, and E. Chen, “Crud-rag: A comprehensive chinese benchmark\\nfor retrieval-augmented generation of large language models,” arXiv\\npreprint arXiv:2401.17043, 2024.\\n[170] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian,\\nE. Bakhturina, M. Shoeybi, and B. Catanzaro, “Retrieval meets long\\ncontext large language models,” arXiv preprint arXiv:2310.03025 ,\\n2023.\\n[171] C. Packer, V . Fang, S. G. Patil, K. Lin, S. Wooders, and J. E. Gon-\\nzalez, “Memgpt: Towards llms as operating systems,” arXiv preprint\\narXiv:2310.08560, 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='2023.\\n[171] C. Packer, V . Fang, S. G. Patil, K. Lin, S. Wooders, and J. E. Gon-\\nzalez, “Memgpt: Towards llms as operating systems,” arXiv preprint\\narXiv:2310.08560, 2023.\\n[172] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis, “Efficient\\nstreaming language models with attention sinks,” arXiv preprint\\narXiv:2309.17453, 2023.\\n[173] T. Zhang, S. G. Patil, N. Jain, S. Shen, M. Zaharia, I. Stoica, and J. E.\\nGonzalez, “Raft: Adapting language model to domain specific rag,”\\narXiv preprint arXiv:2403.10131 , 2024.\\n[174] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,\\nR. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws\\nfor neural language models,” arXiv preprint arXiv:2001.08361 , 2020.\\n[175] U. Alon, F. Xu, J. He, S. Sengupta, D. Roth, and G. Neubig, “Neuro-\\nsymbolic language modeling with automaton-augmented retrieval,” in\\nInternational Conference on Machine Learning . PMLR, 2022, pp.\\n468–485.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='symbolic language modeling with automaton-augmented retrieval,” in\\nInternational Conference on Machine Learning . PMLR, 2022, pp.\\n468–485.\\n[176] M. Yasunaga, A. Aghajanyan, W. Shi, R. James, J. Leskovec, P. Liang,\\nM. Lewis, L. Zettlemoyer, and W.-t. Yih, “Retrieval-augmented multi-\\nmodal language modeling,” arXiv preprint arXiv:2211.12561 , 2022.\\n[177] J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language-\\nimage pre-training with frozen image encoders and large language\\nmodels,” arXiv preprint arXiv:2301.12597 , 2023.\\n[178] W. Zhu, A. Yan, Y . Lu, W. Xu, X. E. Wang, M. Eckstein, and W. Y .\\nWang, “Visualize before you write: Imagination-guided open-ended\\ntext generation,” arXiv preprint arXiv:2210.03765 , 2022.\\n[179] J. Zhao, G. Haffar, and E. Shareghi, “Generating synthetic speech from\\nspokenvocab for speech translation,” arXiv preprint arXiv:2210.08174,\\n2022.\\n[180] D. M. Chan, S. Ghosh, A. Rastrow, and B. Hoffmeister, “Using external'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='spokenvocab for speech translation,” arXiv preprint arXiv:2210.08174,\\n2022.\\n[180] D. M. Chan, S. Ghosh, A. Rastrow, and B. Hoffmeister, “Using external\\noff-policy speech-to-text mappings in contextual end-to-end automated\\nspeech recognition,” arXiv preprint arXiv:2301.02736 , 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='21\\n[181] A. Yang, A. Nagrani, P. H. Seo, A. Miech, J. Pont-Tuset, I. Laptev,\\nJ. Sivic, and C. Schmid, “Vid2seq: Large-scale pretraining of a visual\\nlanguage model for dense video captioning,” in Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\\n2023, pp. 10 714–10 726.\\n[182] N. Nashid, M. Sintaha, and A. Mesbah, “Retrieval-based prompt\\nselection for code-related few-shot learning,” in 2023 IEEE/ACM 45th\\nInternational Conference on Software Engineering (ICSE) , 2023, pp.\\n2450–2462.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8611f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1c1687d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x25f38ca4590>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager=EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec57fba",
   "metadata": {},
   "source": [
    "VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9753cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x25f3a5a41a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "113e661a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='entirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='transduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='efﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\\nof continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\\nsequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='around each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\\nthe matrix of outputs as:\\nAttention(Q,K,V ) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='of 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = ∑dk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\\nwhere headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈Rdmodel×dk , WK\\ni ∈Rdmodel×dk , WV\\ni ∈Rdmodel×dv\\nand WO ∈Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='position in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='of the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0,xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Similarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 ·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel )\\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='PE(pos,2i) = sin(pos/100002i/dmodel )\\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='during training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='computational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 15], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+ n·d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='target vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate= d−0.5\\nmodel ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_stepstraining steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps= 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\nResidual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\n7'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0 ·1020\\nGNMT + RL [31] 24.6 39.92 2.3 ·1019 1.4 ·1020\\nConvS2S [8] 25.16 40.46 9.6 ·1018 1.5 ·1020\\nMoE [26] 26.03 40.56 2.0 ·1019 1.2 ·1020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0 ·1020\\nGNMT + RL Ensemble [31] 26.30 41.16 1.8 ·1020 1.1 ·1021\\nConvS2S Ensemble [8] 26.36 41.29 7.7 ·1019 1.2 ·1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.0 2.3 ·1019\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='dropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α= 0.6 [31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='single-precision ﬂoating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='multi-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='tensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='machine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='Recognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '..\\\\data\\\\pdf\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'NIPS-2017-attention-is-all-you-need-Paper.pdf', 'file_type': 'pdf'}, page_content='networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='IEEE REVIEWS IN BIOMEDICAL ENGINEERING 1\\nNatural Language Processing for Smart Healthcare\\nBinggui Zhou, Guanghua Yang \\x00 , Zheng Shi, and Shaodan Ma \\x00\\nAbstract—Smart healthcare has achieved signiﬁcant progress\\nin recent years. Emerging artiﬁcial intelligence (AI) technologies\\nenable various smart applications across various healthcare\\nscenarios. As an essential technology powered by AI, natural\\nlanguage processing (NLP) plays a key role in smart healthcare\\ndue to its capability of analysing and understanding human\\nlanguage. In this work, we review existing studies that concern\\nNLP for smart healthcare from the perspectives of technique and\\napplication. We ﬁrst elaborate on different NLP approaches and\\nthe NLP pipeline for smart healthcare from the technical point\\nof view. Then, in the context of smart healthcare applications\\nemploying NLP techniques, we introduce representative smart\\nhealthcare scenarios, including clinical practice, hospital man-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='of view. Then, in the context of smart healthcare applications\\nemploying NLP techniques, we introduce representative smart\\nhealthcare scenarios, including clinical practice, hospital man-\\nagement, personal care, public health, and drug development. We\\nfurther discuss two speciﬁc medical issues, i.e., the coronavirus\\ndisease 2019 (COVID-19) pandemic and mental health, in which\\nNLP-driven smart healthcare plays an important role. Finally,\\nwe discuss the limitations of current works and identify the\\ndirections for future works.\\nIndex Terms—Natural Language Processing, Smart Health-\\ncare, Artiﬁcial Intelligence, NLP Techniques, Healthcare Appli-\\ncations\\nI. I NTRODUCTION\\nS\\nMART healthcare is a healthcare system that exploits\\nemerging technologies, such as artiﬁcial intelligence (AI),\\nblockchain, big data, cloud/edge computing, and the internet\\nof things (IOT), for realizing various intelligent systems to\\nconnect healthcare participants and promote the quality of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='blockchain, big data, cloud/edge computing, and the internet\\nof things (IOT), for realizing various intelligent systems to\\nconnect healthcare participants and promote the quality of\\nhealthcare [1]. Major participants in smart healthcare can\\nbe classiﬁed into three categories, i.e., the public, health-\\ncare service providers, and third-party healthcare participants.\\nRelated to the participants, representative smart healthcare\\nscenarios include smart homes, smart hospitals, intelligent\\nresearch and development for life science, health management,\\npublic health, rehabilitation therapy, and etc. Fig. 1 shows the\\nmajor participants, emerging technologies, and representative\\nscenarios of smart healthcare.\\nNatural language processing (NLP) is a subﬁeld of com-\\nputer science and artiﬁcial intelligence that is concerned with\\nBinggui Zhou is with the School of Intelligent Systems Science and\\nEngineering, Jinan University, Zhuhai 519070, China; and also with the State'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='Binggui Zhou is with the School of Intelligent Systems Science and\\nEngineering, Jinan University, Zhuhai 519070, China; and also with the State\\nKey Laboratory of Internet of Things for Smart City and the Department of\\nElectrical and Computer Engineering, University of Macau, Macao 999078,\\nChina.\\nGuanghua Yang is with the School of Intelligent Systems Science and\\nEngineering, Jinan University, Zhuhai 519070, China.\\nZheng Shi is with the School of Intelligent Systems Science and Engineer-\\ning, Jinan University, Zhuhai 519070, China; and also with the State Key\\nLaboratory of Internet of Things for Smart City, University of Macau, Macao\\n999078, China.\\nShaodan Ma is with the State Key Laboratory of Internet of Things for\\nSmart City and the Department of Electrical and Computer Engineering,\\nUniversity of Macau, Macao 999078, China.\\n\\x00 Corresponding authors: Guanghua Yang (ghyang@jnu.edu.cn), Shaodan\\nMa (shaodanma@um.edu.mo).\\nthe automatic analysis, representation and understanding of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='University of Macau, Macao 999078, China.\\n\\x00 Corresponding authors: Guanghua Yang (ghyang@jnu.edu.cn), Shaodan\\nMa (shaodanma@um.edu.mo).\\nthe automatic analysis, representation and understanding of\\nhuman language [2]. NLP has become a hot research area\\nand has attracted widespread attention from many research\\ncommunities in the past several years. As human language\\nis a general form of data entry for intelligent systems, NLP\\nenables machines to understand human language and interact\\nwith humans, making it essential to smart healthcare.\\nThe main manifestations of natural language are text and\\nspeech, where text encompasses text records, articles, book\\nchapters, dictionaries, and so forth, while speech occurs in\\nhuman-human and human-machine dialogues. NLP has been\\ndeveloped for several decades following the early origin of\\nartiﬁcial intelligence in the 1950s. Approaches to conduct\\nNLP are generally divided into three categories: rule-based'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='developed for several decades following the early origin of\\nartiﬁcial intelligence in the 1950s. Approaches to conduct\\nNLP are generally divided into three categories: rule-based\\napproaches, statistical approaches, and deep learning-based\\napproaches. From the 1950s to 1980s, NLP research mainly\\nfocused on rule-based approaches, which required expertise in\\nboth computer science and linguistics to design rules that ﬁt\\nhuman language. However, even well-designed rules are quite\\nlimited for covering human language due to its ﬂexibility and\\ncomplex patterns. Since the 1980s, statistical NLP systems\\nhave been designed by extracting features from corpora using\\nstatistical and machine learning algorithms and have gradually\\nreplaced rule-based NLP systems due to their superiority in\\nperformance and robustness. With the early application of\\nthe neural probabilistic language model [3] and the rapid\\ndevelopment of deep learning since 2013, neural NLP, by'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='performance and robustness. With the early application of\\nthe neural probabilistic language model [3] and the rapid\\ndevelopment of deep learning since 2013, neural NLP, by\\nusing neural networks and large corpora for automated feature\\nlearning, has dominated current research and achieved SOTA\\nperformance of many NLP tasks.\\nIn smart healthcare, NLP is applied to process text data and\\nis associated with human-machine/human-human communica-\\ntion. The text data can be classiﬁed into 2 categories: clinical\\ntext and other text data. Clinical text comes from all clinical\\nscenarios and mainly comprises of unstructured text records\\nfrom electronic health record (EHR) systems, including med-\\nical notes, diagnostic reports, electronic prescriptions, and\\netc. Other text data include all text that appears within other\\nhealthcare scenarios, e.g., surveys in population screening and\\narticles for evidence-based reference. Communication is com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='etc. Other text data include all text that appears within other\\nhealthcare scenarios, e.g., surveys in population screening and\\narticles for evidence-based reference. Communication is com-\\nmon in all smart healthcare scenarios, such as patient-provider\\ncommunication in clinical inquiry and human-robot interaction\\nin rehabilitation therapy, accompanied by applications such\\nas machine translations and user interfaces for rehabilitation\\nrobots.\\nAs well recognized, research on and applications of NLP\\nfor smart healthcare have received intensive attention in recent\\nyears. However, no study has offered a well-organized sum-\\nmary of existing works in a systematic way. In this paper, we\\nﬁrst provide a systematic review of NLP for smart healthcare\\n© 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 0, 'page_label': '1', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='© 2022 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media,\\nincluding reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers\\nor lists, or reuse of any copyrighted component of this work in other works.\\narXiv:2110.15803v3  [cs.CL]  26 Sep 2022'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 1, 'page_label': '2', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='2 IEEE REVIEWS IN BIOMEDICAL ENGINEERING\\nMajor Participants\\nThird Parties\\ngovernment\\n medical insurance\\nService Providers\\nhealthcare\\ninstitutions\\nlife science\\ncompanies\\nSmart\\nHealthcare\\nPublic\\nhealthy people patients\\nEmerging Technologies\\nAI\\n cloud computing\\nIoT\\n blockchain\\nRepresentative Scenarios\\nsmart hospitals\\nintelligent R&D\\nfor life science\\npublic health\\npromotion\\nhealth monitoringa\\nb\\nc\\nFig. 1. Smart healthcare. a, major participants in smart healthcare include the public, healthcare service providers, and third-party healthcare participants.\\nb, example emerging technologies enable smart healthcare applications include artiﬁcial intelligence, blockchain, cloud computing, the internet of things, and\\netc. c, representative smart healthcare scenarios include intelligent research and development for life science, public health promotion, smart hospitals, health\\nmonitoring, and etc.\\nfrom both technical and application perspectives. After that,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 1, 'page_label': '2', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='monitoring, and etc.\\nfrom both technical and application perspectives. After that,\\nwe discuss two speciﬁc medical issues, i.e., coronavirus dis-\\nease 2019 (COVID-19) pandemic and mental health, in which\\nNLP-driven smart healthcare plays an important role. Finally\\nwe discuss the limitations of existing works, identify the future\\ndirections of applying NLP to smart healthcare, and close the\\nreview with some conclusions.\\nII. NLP FOR SMART HEALTHCARE FROM TECHNICAL\\nPERSPECTIVE\\nNLP has been undergoing continuous development since the\\n1950s. Studies on NLP for smart healthcare have also been\\nconducted for decades and have attracted increased attention\\nin recent years with the advancement of artiﬁcial intelligence\\nand general NLP. To connect existing works from technical\\nperspective, in this section, we ﬁrst introduce the three kinds of\\nNLP approaches and their representative algorithms, and then\\nintroduce the NLP pipeline for smart healthcare to show how'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 1, 'page_label': '2', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='perspective, in this section, we ﬁrst introduce the three kinds of\\nNLP approaches and their representative algorithms, and then\\nintroduce the NLP pipeline for smart healthcare to show how\\nNLP techniques are used in real smart healthcare applications.\\nA. Comparisons of different NLP approaches\\nThe mainstream NLP approaches can be classiﬁed into three\\ncategories, i.e., rule-based NLP, statistical NLP and neural\\nNLP, which have different characteristics. Below, we discuss\\nthe advantage and disadvantages of the three categories and\\nintroduce the representative algorithms of them.\\nRule-based NLP approaches, e.g., pattern matching [4]\\nand parsing [5], could be quite accurate in speciﬁc cases if\\ndedicated studies by experts are conducted. In addition, rule-\\nbased NLP approaches are easy to interpret and understand.\\nHowever, rules are normally too limited to cover all cases\\nconsidering the ﬂexibility and complex patterns of human lan-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 1, 'page_label': '2', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='based NLP approaches are easy to interpret and understand.\\nHowever, rules are normally too limited to cover all cases\\nconsidering the ﬂexibility and complex patterns of human lan-\\nguage. In addition, rule-based NLP requires expertise in both\\ncomputer science and linguistics to design appropriate rules to\\nﬁt human language, hindering it from large-scale applications.\\nCurrently, rule-based approaches have been widely considered\\nobsolete by academia [6], and are occasionally used for better\\npreprocessing nowadays [7].\\nIn general, statistical NLP is superior to rule-based NLP in\\nperformance and robustness. However, it also requires domain\\nexpertise to create handcrafted features, and is therefore lim-\\nited to taking full advantage of available data and providing\\nenough accuracy in complex applications. Although statistical\\nNLP requires intensive feature engineering, it is this direct\\nfeature design that makes it transparent and interpretable as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 1, 'page_label': '2', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='enough accuracy in complex applications. Although statistical\\nNLP requires intensive feature engineering, it is this direct\\nfeature design that makes it transparent and interpretable as\\nrule-based NLP. In addition, statistical NLP does not rely on\\nlarge-scale datasets or large amounts of computational power,\\nand thus is much more efﬁcient than neural NLP. Furthermore,\\nrepresentative statistical NLP models, such as bag-of-words\\n[8], TF-IDF [9], [10], and n-gram [11]–[13], have different\\ncharacteristics. Bag-of-words is easy to implement, but it\\nonly considers the frequencies of words in a sentence, which\\nneglects the importance and sequential order of these words.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ZHOU et al.: NATURAL LANGUAGE PROCESSING FOR SMART HEALTHCARE 3\\nThrough the inverse document frequency, TF-IDF improves\\nthe measurement of a word’s importance, but still does not\\ntake sequential order information into consideration. N-gram\\nconsiders n − 1 words before a word, which makes it more\\naccurate than bag-of-words but with higher computational\\ncomplexity (increases exponentially with n). It is worth men-\\ntioning that despite the dominance of deep learning in recent\\nyears, statistical NLP is still active in many healthcare studies\\nand applications.\\nRecent years have witnessed the success of neural NLP, who\\nhas shown better performance than both rule-based NLP and\\nstatistical NLP in applications with abundant available data.\\nHowever, neural NLP is often blamed for low interpretability\\nand dependence on expensive computing platforms. It is\\nalso worth noting that, compared with rule-based NLP and\\nstatistical NLP, neural NLP usually fails to achieve satisfac-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='and dependence on expensive computing platforms. It is\\nalso worth noting that, compared with rule-based NLP and\\nstatistical NLP, neural NLP usually fails to achieve satisfac-\\ntory performance if limited data is available. Among neural\\nNLP models, recurrent neural network (RNN)-based models,\\nespecially long short-term memory (LSTM) [14]–[16]-based\\nmodels and gated recurrent unit (GRU)-based models [15],\\n[17], are more natural for processing sequential data such\\nas text and speech. They have the ability to remember his-\\ntorical information of the inputs, but suffer from gradient\\nvanishing/explosion, training issues and short-term memories.\\nConvolutional neural networks (CNN)-based models [18],\\n[19], combining with word embeddings, also show good\\nperformance in some tasks due to their ability in learning\\nlocal features and high computational efﬁciency which enables\\ndeep network architectures. Recently, graph neural network\\n(GNN)-based models have been applied to NLP-driven smart'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='local features and high computational efﬁciency which enables\\ndeep network architectures. Recently, graph neural network\\n(GNN)-based models have been applied to NLP-driven smart\\nhealthcare by incorporating knowledge from graph-structured\\nontology/entities [20]–[22]. When graphs are large in scale\\nor complex, GNN-based models are difﬁcult and costly to\\nimplement and train. Generally speaking, RNNs, CNNs, and\\nGNNs are all limited in tackling long-term dependencies in se-\\nquences. Through the self-attention mechanism, Transformer-\\nbased models [23], [24] are very efﬁcient in processing\\nlong sequences and support parallel training, but are lack\\nof ability in learning local features and position information.\\nWe have witnessed many combinations of the aforementioned\\nmodels for better feature extraction performance, including\\nCNN-LSTM networks [25], RNN-Attention networks [26],\\n[27], memory networks (MM) [28], [29], graph convolutional\\nnetworks (GCN) [30], CNN-LSTM-Attention networks [31],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='CNN-LSTM networks [25], RNN-Attention networks [26],\\n[27], memory networks (MM) [28], [29], graph convolutional\\nnetworks (GCN) [30], CNN-LSTM-Attention networks [31],\\n[32], graph convolutional attention networks (GCAN) [33],\\n[34], etc. In addition, to further leverage large unlabelled\\ncorpora, pretraining, a very effective method, has been widely\\nexploited to obtain non-contextual or contextual embeddings\\n[35]. Word2vec [36]–[39], and GloVe (Global Vectors) [36],\\n[40], as representative algorithms of non-contextual embed-\\ndings, provide distributed dense vectors as word embeddings,\\nand outperform statistical algorithms such as bag-of-words and\\nn-gram. The non-contextual embedding for a word is static\\nand does not dynamically change as its context changes [35].\\nBased on the Transformer architecture, contextual embed-\\ndings, e.g., ELMo (Embeddings from Language Models) [41],\\nBERT (Bidirectional Encoder Representations from Trans-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='Based on the Transformer architecture, contextual embed-\\ndings, e.g., ELMo (Embeddings from Language Models) [41],\\nBERT (Bidirectional Encoder Representations from Trans-\\nformers) [42]–[45], and GPT (Generative Pre-Training) [46],\\nare developed to embed dynamic contextual information into\\nword embeddings, achieving outstanding performance than\\nother word embedding algorithms. It should be noted that these\\nmodels are typically huge and expensive to pre-train, which\\nsomehow constraints their broad application in healthcare.\\nThe comparisons of different NLP approaches and repre-\\nsentative algorithms are shown in Table I.\\nB. NLP pipeline for smart healthcare\\nAs shown in Fig. 2, there are three parts in an NLP pipeline\\nfor smart healthcare, i.e., preprocessing, feature extraction, and\\nmodelling. An NLP pipeline takes text or speech as illustrated\\nbefore as the input. After that, preprocessing is conducted\\nconsidering various inputs and their qualities to facilitate'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='modelling. An NLP pipeline takes text or speech as illustrated\\nbefore as the input. After that, preprocessing is conducted\\nconsidering various inputs and their qualities to facilitate\\nfeature extraction and modelling. As the most important step,\\nfeature extraction is essential to NLP, which undoubtedly\\nexplains the attention it has received from researchers. Finally,\\nmodels for speciﬁc NLP tasks are built with the extracted\\nfeatures to yield the outputs accordingly.\\n1) Preprocessing: Preprocessing, including the procedures\\nof tokenization, stemming, lemmatization, stopword removal,\\nand etc., makes natural language normalized, machine-\\nreadable, and easy for postprocessing. Text preprocessing\\nmostly paves the way for feature extraction and modelling,\\nsince many NLP tasks require normalized text input to guar-\\nantee accuracy and efﬁciency due to signiﬁcant challenges\\ncoming from the ﬂexibility of natural languages and the\\nwide variety of morphological variants of medical terms in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='antee accuracy and efﬁciency due to signiﬁcant challenges\\ncoming from the ﬂexibility of natural languages and the\\nwide variety of morphological variants of medical terms in\\nmedical text [47]–[49]. However, with the development of\\nneural NLP, some text preprocessing procedures have become\\nunnecessary and may even cause problems. For example,\\nremoving stopwords may lead to the loss of informative\\ncontext information when using the BERT pre-trained model\\n[50]. As the preprocessing of speech, such as denoising, is\\ntypically regarded as a problem in signal processing, we do\\nnot discuss it in detail here.\\n2) Feature extraction: Apart from the increase in accessible\\ndigital data and the advances in computing platforms such\\nas graphics processing units, the development of NLP is\\nlargely attributed to the improvement in feature design or\\nfeature extraction methods. Both rule-based approaches and\\nstatistical approaches require expertise for rule design [4],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='largely attributed to the improvement in feature design or\\nfeature extraction methods. Both rule-based approaches and\\nstatistical approaches require expertise for rule design [4],\\n[5] or feature engineering [8]–[13]. For neural NLP, auto-\\nmated feature extraction via varieties of neural networks [14]–\\n[34] have greatly improved the efﬁciency of data utilization\\nand feature extraction. Automated feature engineering can be\\nconducted directly according to the downstream tasks using\\nsupervised learning, unsupervised learning or reinforcement\\nlearning. In addition, pretraining is also widely used in NLP\\nto automatically extract features from large unlabelled corpora\\nvia self-supervised learning in a generative, contrastive or\\ngenerative-contrastive manner [51] before the downstream\\ntasks begin. The extracted features, known as contextual or\\nnon-contextual embeddings, may encompass features such as\\nlexical meanings, syntactic features, semantic features, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 2, 'page_label': '3', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='tasks begin. The extracted features, known as contextual or\\nnon-contextual embeddings, may encompass features such as\\nlexical meanings, syntactic features, semantic features, and\\neven pragmatics, which contribute to downstream tasks [35].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 3, 'page_label': '4', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='4 IEEE REVIEWS IN BIOMEDICAL ENGINEERING\\nTABLE I\\nCOMPARISONS OF DIFFERENT NLP APPROACHES AND REPRESENTATIVE ALGORITHMS .\\nNLP Approach Feature Extrac-\\ntion Method\\nAdvantages and Disadvantages Representative Algorithms\\nRule-based NLP rule design\\nadvantages :\\n- could be quite accurate in speciﬁc\\ncases;\\n- easy to interpret and understand\\ndisadvantages :\\n- rules are too limited to cover all\\ncases considering the ﬂexibility and\\ncomplex patterns of human language;\\n- require expertise in both computer\\nand linguistics to ﬁt human language\\npattern matching [4] and parsing [5]\\nStatistical NLP hand-crafted fea-\\nture engineering\\nadvantages :\\n- superior to rule-based NLP in\\nperformance and robustness;\\n- good interpretability\\ndisadvantages :\\n- require domain expertise to create\\nhandcrafted features;\\n- limited to taking full advantage of\\navailable data and providing enough\\naccuracy in complex applications\\nbag-of-words [8]:\\n- easy to implement;\\n- neglects the importance and sequential order of words'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 3, 'page_label': '4', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='available data and providing enough\\naccuracy in complex applications\\nbag-of-words [8]:\\n- easy to implement;\\n- neglects the importance and sequential order of words\\nTF-IDF [9], [10]:\\n- improves the measurement of a word’s importance;\\n- does not take sequential order information into consideration\\nn-gram [11]–[13]:\\n- more accurate than bag-of-words;\\n- high computational complexity (increasing exponentially with n)\\nNeural NLP automated\\nfeature extraction\\nadvantages :\\n- better performance than both\\nrule-based NLP and statistical NLP\\nin applications with abundant\\navailable data\\ndisadvantages :\\n- low interpretability;\\n- dependence on expensive computing\\nplatforms;\\n- usually fail to achieve satisfactory\\nperformance if limited data is\\navailable\\n1) RNN-based models (e.g., LSTM [14]–[16] and GRUs [15], [17]):\\n- more natural for processing text and speech input;\\n- capable of remembering historical information of the inputs;\\n- suffer from gradient vanishing/explosion, training issues and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 3, 'page_label': '4', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='- more natural for processing text and speech input;\\n- capable of remembering historical information of the inputs;\\n- suffer from gradient vanishing/explosion, training issues and\\nshort-term memories\\n2) CNN-based models [18], [19]:\\n- able to learn local features;\\n- high computational efﬁciency;\\n- limited in tackling long-term dependencies in sequences\\n3) GNN-based models [20]–[22]:\\n- efﬁcient in incorporating knowledge from graph-structured\\nontology/entities;\\n- limited in tackling long-term dependencies in sequences;\\n- difﬁcult and costly to implement and train with large-scale\\nor very complex graphs\\n4) Transformer-based models [23], [24]:\\n- efﬁcient in processing long sequences and parallel training;\\n- lack of ability in learning local features and position information\\n5) combinations: CNN-LSTM [25], RNN-Attention [26], [27],\\nMN [28], [29], GCN [30], CNN-LSTM-Attention [31], [32],\\nand GCAN [33], [34], etc.\\n6) non-contextual embedding-oriented pre-trained models'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 3, 'page_label': '4', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='5) combinations: CNN-LSTM [25], RNN-Attention [26], [27],\\nMN [28], [29], GCN [30], CNN-LSTM-Attention [31], [32],\\nand GCAN [33], [34], etc.\\n6) non-contextual embedding-oriented pre-trained models\\n(word2vec [36]–[39], GloVe [36], [40]):\\n- outperform statistical algorithms;\\n- the non-contextual embedding for a word is static and will not\\ndynamically change as its context change\\n7) contextual embedding-oriented pre-trained models (ELMo [41],\\nBERT [42]–[45], GPT [46]):\\n- able to embed dynamic contextual information into word embeddings;\\n- outstanding performance than other word embedding algorithms;\\n- typically huge and expensive to pre-train'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ZHOU et al.: NATURAL LANGUAGE PROCESSING FOR SMART HEALTHCARE 5\\nInputs OutputsFeature ExtractionPreprocessing\\n• Text Classification\\n• Information Extraction\\n• Machine Translation\\n• Text Generation\\n• Information Retrieval\\n• Question Answering and Dialogue\\n• Knowledge Engineering\\n• Natural Language Understanding\\n• Causal Inference\\n• Speech Recognition\\n• Speech Synthesis\\n• …\\nModelling\\nFig. 2. The NLP pipeline for smart healthcare . There are three parts in an NLP pipeline for smart healthcare, i.e., preprocessing, feature extraction, and\\nmodelling. NLP takes text or speech as the input, followed by preprocessing to facilitate feature extraction and modelling. Features can be extracted with\\nvarious methods and models. Models for speciﬁc NLP tasks are ﬁnally built with the extracted features to yield the outputs.\\n3) Modelling: For various smart healthcare applications,\\ndifferent models should be built to accomplish various NLP\\ntasks, such as text classiﬁcation, information extraction, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='3) Modelling: For various smart healthcare applications,\\ndifferent models should be built to accomplish various NLP\\ntasks, such as text classiﬁcation, information extraction, and\\nnatural language understanding. The extracted feature can be\\ndirectly processed by classiﬁers and regressors to yield outputs\\nfor simple tasks, e.g., medical text classiﬁcation [18], [52],\\nwhile further steps are required to complete complex tasks. In\\nthe following subsections, we ﬁrst introduce several text input-\\nbased NLP tasks according to their complexity. At the end of\\nthis section, we will introduce two speech-speciﬁc tasks, i.e.,\\nspeech recognition and speech synthesis.\\nInformation extraction. Information extraction (IE), a.k.a.\\ntext mining, enables harvesting information from text inputs,\\nand plays an important role in text analysis. Works related to\\ninformation extraction in smart healthcare focus on the ex-\\ntraction of diseases, drugs, events (mainly including temporal'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='and plays an important role in text analysis. Works related to\\ninformation extraction in smart healthcare focus on the ex-\\ntraction of diseases, drugs, events (mainly including temporal\\nexpressions, spatial expressions and participant information)\\nthrough name entity recognition [53], [54], relation extraction\\n[54]–[56], and event extraction [57] from medical text, includ-\\ning unstructured text in EHRs, articles, etc.\\nMachine translation . Machine translation (MT) aims to\\nautomatically translate text from one language to another\\n[58]. Currently, healthcare resources in various languages are\\nbecoming easily accessible as technologies evolve, and they\\nare all of great value in modern medical practice. Machine\\ntranslation therefore has drawn growing attention for building\\nbetter (multilingual) translation systems and further leveraging\\nmultilingual healthcare resources for other applications, either\\nto provide more accurate translations [59], [60] or to require'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='better (multilingual) translation systems and further leveraging\\nmultilingual healthcare resources for other applications, either\\nto provide more accurate translations [59], [60] or to require\\nless time [60] than human translations.\\nText generation. Text generation (TG) automatically gener-\\nates text with given inputs while pursuing the goal of appearing\\nindistinguishable from human-written text. Speciﬁcally, there\\nare 3 kinds of inputs and corresponding subtasks in smart\\nhealthcare: text inputs (e.g., routine reports) associated with\\ntext summarization [61]–[63], question generation [64]–[66],\\ndialogue generation [67]–[69], and etc.; data inputs (e.g.,\\nneonatal intensive care data) connected with data-to-text [70];\\nand image inputs (e.g., medical images) related to image cap-\\ntioning [71], [72], visual question answering (VQA) [73]–[75],\\nand etc. Note that for data-to-text and image-to-text generation,\\na combination of NLP with data analysis or computer vision'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='tioning [71], [72], visual question answering (VQA) [73]–[75],\\nand etc. Note that for data-to-text and image-to-text generation,\\na combination of NLP with data analysis or computer vision\\nis generally required, respectively.\\nInformation retrieval . Information retrieval (IR) obtains\\nmaterials that meet the query requirements from numerous\\ndocuments, and is a core of search engines for all applications.\\nTo ease the retrieval process [76], [77], improve the relevance\\nand diversity of the retrieval [78]–[80] or reduce the query\\ntime [81], current works aim to develop fast and efﬁcient\\ninformation retrieval methods to obtain useful retrieval from a\\nlarge collection of data sources, ranging from internal health\\ninformation system (HIS) systems and other digital documents\\nto online resources.\\nQuestion answering and dialogue systems . Question an-\\nswering (QA) involves automatically providing answers to\\nquestions raised by humans in a natural language. Question'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='to online resources.\\nQuestion answering and dialogue systems . Question an-\\nswering (QA) involves automatically providing answers to\\nquestions raised by humans in a natural language. Question\\nanswering requires the machine to understand natural language\\nand infer the answers, making it highly dependent on natural\\nlanguage understanding and information retrieval. To date,\\nQA systems for healthcare have developed from information\\nretrieval based QA systems [82]–[85] and knowledge-based\\nQA systems [86]–[89] to hybrid QA systems [90], [91].\\nCompared to question answering, dialogue is also presented\\nin an interactive manner between humans and machines.\\nCommon dialogue systems in smart healthcare include task-\\noriented dialogue systems [92]–[94], and non-task-oriented\\n(a.k.a. chat-oriented) [95] dialogue systems, which assume\\ndifferent functions in various applications.\\nKnowledge engineering . Knowledge engineering (KE) is\\na ﬁeld within artiﬁcial intelligence that tries to construct and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='different functions in various applications.\\nKnowledge engineering . Knowledge engineering (KE) is\\na ﬁeld within artiﬁcial intelligence that tries to construct and\\nuse knowledge-based systems [96]. It does not refer to a pure\\nNLP technique, but receives much attention in NLP for smart\\nhealthcare since medical text is one of the major sources\\nfor knowledge engineering. Within knowledge engineering,\\nknowledge acquisition and knowledge representation are cou-\\npling with information extraction, aiming at the acquisition\\nand representation of medical knowledge in a certain way,\\ne.g., knowledge graphs [97]–[99]. Besides, knowledge engi-\\nneering also concerns building knowledge-based systems to\\nexploit existing knowledge, such as knowledge-based ques-\\ntion answering (KBQA) systems [86]–[89], knowledge-based\\ninformation retrieval systems [100], text generation systems\\n[65], [101], etc.\\nNatural language understanding . Natural language un-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 4, 'page_label': '5', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='tion answering (KBQA) systems [86]–[89], knowledge-based\\ninformation retrieval systems [100], text generation systems\\n[65], [101], etc.\\nNatural language understanding . Natural language un-\\nderstanding (NLU) focuses on machines’ comprehension of\\nhuman language in the form of unstructured text or speech.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='6 IEEE REVIEWS IN BIOMEDICAL ENGINEERING\\nMany of the aforementioned tasks, e.g., question answering,\\ninformation retrieval, require NLU to fully understand the in-\\nput queries. The difﬁculties of natural language understanding\\ncome from the diversity, ambiguity, and potential dependence\\nof natural language, making slow progress in natural language\\nunderstanding compared with other NLP techniques. After\\nyears of development in both general areas and smart health-\\ncare, the mainstream route of NLU is still to use various meth-\\nods to conduct slot ﬁlling and intent detection [102]–[104].\\nNLU is the core of multiple intelligent agents, assuming a\\nrole in understanding human intentions during human-machine\\ninteractions [102], [105], [106], medical queries [103], [104],\\netc.\\nCausal inference. Generally, causal inference is a discipline\\nconcerning the determination of actual effects of speciﬁc\\nthings, events or phenomena. Causal inference in NLP has'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='etc.\\nCausal inference. Generally, causal inference is a discipline\\nconcerning the determination of actual effects of speciﬁc\\nthings, events or phenomena. Causal inference in NLP has\\nlong received insufﬁcient attention since the goal of classical\\nNLP applications is simply to make accurate predictions with\\nall available statistical correlations regardless of the underlying\\ncausal relationship [107]. Recently, with growing concerns\\nabout uninterpretable black box models, the importance of\\ncausal inference has gradually been recognized by NLP re-\\nsearchers, especially in the area of healthcare. Speciﬁcally, re-\\ncent advances of causal inference in NLP for smart healthcare\\nhave been made in uncovering causality from medical text\\n[108]–[110] and realizing reliable NLP-driven applications\\nwith discovered causal effects [108]–[110].\\nSpeech recognition and speech synthesis. Speech recogni-\\ntion (SR) aims to convert human speech into text information.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='with discovered causal effects [108]–[110].\\nSpeech recognition and speech synthesis. Speech recogni-\\ntion (SR) aims to convert human speech into text information.\\nContrary to speech recognition, speech synthesis, a.k.a. text-to-\\nspeech (TTS), is concerned with representing text information\\nwith speech. Basically, SR-oriented and SS-oriented studies\\nattempt to build automatic computer systems for interconver-\\nsion between speech and text in the area of smart healthcare,\\nmaking human-machine interaction as natural and ﬂexible as\\nhuman-human interaction [111]. For speech recognition, these\\nefforts encompass the improvement in acoustic modelling\\n[112], [113], language modelling [114], and the whole system\\npipeline [115], [116] to enhance recognition accuracy. For\\nspeech synthesis, recent advancements have been made in\\ninvestigating and making synthesized speech natural [117],\\n[118], intelligible [119]–[123] and expressive [124]–[126],'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='speech synthesis, recent advancements have been made in\\ninvestigating and making synthesized speech natural [117],\\n[118], intelligible [119]–[123] and expressive [124]–[126],\\nwhich will help stimulate the enthusiasm of human-machine\\ninteraction [127].\\nIII. A PPLICATIONS OF NLP FOR SMART HEALTHCARE\\nNLP has been widely applied in smart healthcare and\\nhas brought dramatic improvements in many applications. As\\nshown in Fig. 3, a typical NLP-driven application is composed\\nof two parts: user interface (UI) and backend. The user\\nprovides text or speech input to the backend through the UI,\\nand then, the backend processes these inputs with the NLP\\nmodels and feeds the results back to the user by providing\\nspeciﬁc services through the UI. Knowledge bases are also\\nrequired at the backend for applications that essentially rely on\\nknowledge, for example, the aforementioned KBQA systems.\\nThe NLP techniques described in the previous section play a\\nkey role in both UI and backend.\\ntext\\n speech'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='knowledge, for example, the aforementioned KBQA systems.\\nThe NLP techniques described in the previous section play a\\nkey role in both UI and backend.\\ntext\\n speech\\n speech\\nrecognition\\nmachine\\ntranslation\\nBackend\\nUser Interfaces\\nUser\\nSupporting \\nModels\\nKnowledge \\nBases\\nServices\\nInputs\\nFig. 3. Basic architecture of NLP-driven applications . A typical NLP-\\ndriven application is composed of user interface and backend, where the UI\\ntakes inputs from the user and feedback the results to the user, and the backend\\nprocesses these inputs with the NLP models with or without the knowledge\\nbases according to the speciﬁc task type.\\nThe UI enables information exchange between users and\\nintelligent systems through speech, text, etc. Easily accessible\\nUIs are critical for enhancing the experience of using intelli-\\ngent systems and realizing smart healthcare. Such user inter-\\nfaces can be implemented by using NLP techniques, especially\\nspeech recognition and natural language understanding.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='gent systems and realizing smart healthcare. Such user inter-\\nfaces can be implemented by using NLP techniques, especially\\nspeech recognition and natural language understanding.\\nAccording to their application scenarios, smart healthcare\\napplications employing NLP techniques can be classiﬁed into\\n5 major categories, i.e., clinical practice, hospital manage-\\nment, personal care, public health, and drug development. A\\nsummary of the applications and related NLP techniques is\\npresented in Table II. Below we introduce the ﬁve categories\\nin detail.\\nA. Clinical practice\\nClinical communication and data collection . Clinical\\ndata, including but not limited to demographics, medical\\nhistory, comorbidities, medical notes, physical examination\\nnotes, electronic recordings from medical devices, and clinical\\nlaboratory testing data and medical images [128], are the most\\nimportant data for diagnosis, treatment and even further retro-\\nspection. Patient-provider communication is an important way'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 5, 'page_label': '6', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='laboratory testing data and medical images [128], are the most\\nimportant data for diagnosis, treatment and even further retro-\\nspection. Patient-provider communication is an important way\\nto obtain ﬁrst-hand clinical data. When necessary, machine\\ntranslation may assist doctors in communicating with patients\\nwho speak different languages or have low literacy and limited\\nlevels of health education [129], [130]. Meanwhile, free text\\nnotes can be taken through speech recognition [131]–[133],\\nwhich can signiﬁcantly reduce medical staff’s time on labour-\\nintensive clinical documentation.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ZHOU et al.: NATURAL LANGUAGE PROCESSING FOR SMART HEALTHCARE 7\\nClinical decision support. Clinical decision support (CDS)\\nsystems can provide physicians with diagnosis and treatment\\nsuggestions, which play an increasingly important role in\\nclinical medicine with the surge of clinical cases and growing\\nconcerns regarding public healthcare. With the development of\\nquestion answering systems, clinical decision support based\\non question answering [84], [134], [135] has emerged and\\nbecome common, as it is closer to traditional patient-provider\\ncommunication. NLP techniques have shown great ability to\\nbuild clinical decision support systems by extracting various\\nuseful information for making diagnosis and therapeutic de-\\ncisions, such as family history information [136], entities and\\nrelations [137], [138], treatment and prognosis data [139],\\nclinical data concepts and features [140], and even causal\\nrelations [109], [110]. In addition, to ensure quality control'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='relations [137], [138], treatment and prognosis data [139],\\nclinical data concepts and features [140], and even causal\\nrelations [109], [110]. In addition, to ensure quality control\\nand future quality improvement, NLP can also contribute to\\nthe assessment of clinical procedures [141], [142], warning\\nof potentially harmful adverse drug events (ADEs) [143],\\ndisease symptoms [144], [145], and outcome-related causal\\neffects [146]. Finally, NLP is also powerful in enhancing\\nthe interpretability and reliability of clinical decision support\\nsystems for practical deployment, by providing supporting\\nevidence for diagnosis or treatment decisions in an evidence-\\nbased fashion [108], [147]–[150].\\nB. Hospital management\\nMedical resource allocation . Due to limited medical re-\\nsources, including hospital spaces, personnel, and materi-\\nals, efﬁcient resource allocation is critical in hospitals and\\nother medical facilities. By building patient triage systems,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='sources, including hospital spaces, personnel, and materi-\\nals, efﬁcient resource allocation is critical in hospitals and\\nother medical facilities. By building patient triage systems,\\nmedical resources can attend to critical cases with priority\\nand enhance medical resource allocation effectiveness and\\nefﬁciency [151], [152]. Virtual assistants [153]–[155], hospital\\nautomation systems [156], [157] and collaborative robots\\n[158], [159] with voice control can further reduce the burden\\non medical staff, thereby improving hospital management\\nefﬁciency. There are also some interesting works that have\\nexplored the prediction of patient readmission to rearrange\\nmedical resources/interventions and reduce the readmission\\nrate [160]–[162]. In addition, by leveraging text generation\\ntechniques, part of text writing in healthcare, especially routine\\nreports, can be taken over by machines, freeing medical staff\\nfrom many administrative duties and making them available'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='techniques, part of text writing in healthcare, especially routine\\nreports, can be taken over by machines, freeing medical staff\\nfrom many administrative duties and making them available\\nfor direct patient care [70], [163].\\nData management . To manage large volumes of medical\\ndocumentation, text classiﬁcation, information extraction and\\ntext summarization can be used to generate category labels,\\ninformative keywords and simpliﬁed summaries [18], [52],\\n[62], [63], [164] for management, while information retrieval\\nsystems, especially those systems based on semantic search\\n[76], [165] and question answering [77], can be used in\\nhealthcare information systems to ease the retrieval process.\\nService quality control . Sentiment analysis with patient\\nexperience feedback will help hospitals improve their service\\nquality and patient experience. Such analysis required sub-\\nstantial personnel resources in the past, while NLP makes this\\nwork easier and greatly improves the efﬁciency of sentiment'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='quality and patient experience. Such analysis required sub-\\nstantial personnel resources in the past, while NLP makes this\\nwork easier and greatly improves the efﬁciency of sentiment\\nanalysis [166]–[168].\\nC. Personal care\\nPersonal health assistants . Personal health assistants en-\\nable people to easily access useful medical information and\\nhealthcare services without visiting the healthcare institutions.\\nPersonal health assistants may incorporate several subsystems,\\nsuch as medical information access systems [169] and remote\\nhealthcare systems [170], for various purposes.\\nAssisting elderly individuals and disabled individuals .\\nNLP techniques can help elderly individuals and disabled\\nindividuals to greatly enhance their quality of life and so-\\ncial integration. V oice-controlled home automation systems\\nand robots may assist the elderly and the disabled in their\\ndaily lives [171], while robots (especially androids and other\\nrobots that communicate with people) can even encourage and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='and robots may assist the elderly and the disabled in their\\ndaily lives [171], while robots (especially androids and other\\nrobots that communicate with people) can even encourage and\\naccompany them through social interactions [172], [173]. In\\naddition, NLP techniques are also of great value for providing\\nessential aids to people with various disabilities, e.g., speech\\nimpairments [122], [174]–[178], hearing loss [179], dyslexia\\n[180], or neurological disorders [119]–[121].\\nD. Public health\\nHealth knowledge popularization and medical educa-\\ntion. Health knowledge popularization and medical education\\nare essential public health interventions since they can im-\\nprove people’s health literacy and help them develop healthy\\nliving habits. Through knowledge engineering, accurate and\\ncomplete medical knowledge bases can be established to\\npromote the popularization of medical knowledge among the\\npopulation [86]–[89], [97]–[100], [181]. Speciﬁcally, people'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='complete medical knowledge bases can be established to\\npromote the popularization of medical knowledge among the\\npopulation [86]–[89], [97]–[100], [181]. Speciﬁcally, people\\ncan easily access medical knowledge through question an-\\nswering systems [182], [183], information retrieval systems\\n[79], [81], and machine translation systems [1], [130], [184],\\n[185], facilitating the popularization and education of medical\\nknowledge. In addition, text generation techniques, such as\\nquestion generation and text summarization, can also be used\\nin medical education to generate medical case-based questions\\n[186] and construct simpliﬁed summaries [61].\\nPopulation screening. In addition to the health knowledge\\npopularization, population screening, which refers to the pro-\\ncess of assessing the prevalence of a disease or condition in\\na population or subgroup, is also an important intervention\\nfor delivering public health. The population screening starts'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 6, 'page_label': '7', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='cess of assessing the prevalence of a disease or condition in\\na population or subgroup, is also an important intervention\\nfor delivering public health. The population screening starts\\nwith identifying target populations, followed by the screening\\ntest. After that, further actions such as further tests, advice, or\\ntreatment can be taken considering the screening results [187].\\nNLP can play two main roles in population screening. First,\\nNLP helps identify populations with higher health risk factors,\\nwhich may improve the efﬁciency of population screening\\n[188]. Second, NLP can also assist in the analysis of healthcare\\nquestionnaires and surveys [189], especially for open-ended\\nquestions.\\nE. Drug development\\nDrug discovery . NLP helps construct textual representa-\\ntions of biochemical entities for mapping the interactions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='8 IEEE REVIEWS IN BIOMEDICAL ENGINEERING\\nbetween diseases, drugs/chemical compounds, and biomacro-\\nmolecules (e.g., genes, proteins); predicting molecular prop-\\nerties; and designing novel molecules. Readers are referred to\\nthe comprehensive review by ¨Ozt¨urk et al [190] for a deeper\\nunderstanding of NLP methodologies for drug discovery.\\nPreclinical research. NLP techniques, especially informa-\\ntion extraction, are also able to identify the relations between\\nchemical structures and biological activity [191] and further\\nhelp researchers search for potentially effective chemical com-\\npounds, i.e., virtual screening [192], [193], in a huge chemical\\nspace. In addition, they are also applied in the prediction\\nof adverse drug reactions, including side effect prediction\\n[194], toxicity prediction [195], [196], and etc., in preclinical\\nresearch.\\nClinical research. Across the clinical research stage, NLP\\nmay enable efﬁcient clinical trial design [110], patient recruit-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='research.\\nClinical research. Across the clinical research stage, NLP\\nmay enable efﬁcient clinical trial design [110], patient recruit-\\nment [197]–[199], clinical trial analytics [200], and etc.\\nDrug review and safety monitoring . Recently, the FDA\\nand other institutions have reported being interested in using\\nNLP for adverse drug event discovery and drug safety moni-\\ntoring [201]–[203], showing the full range of NLP’s key role\\nin drug development.\\nIV. NLP- DRIVEN SMART HEALTHCARE FOR SPECIFIC\\nMEDICAL ISSUES\\nNLP-driven smart healthcare plays an important role in\\nmany medical issues. In this section, we discuss how NLP-\\ndriven smart healthcare works in medical issues by taking two\\nspeciﬁc medical issues, i.e., COVID-19 pandemic and mental\\nhealth, as examples.\\nA. COVID-19 pandemic\\nWorldwide outbreak of COVID-19 has triggered an unprece-\\ndented global health crisis and has attracted much attention\\nfrom researchers [204]. No wonder, the COVID-19 pandemic'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='A. COVID-19 pandemic\\nWorldwide outbreak of COVID-19 has triggered an unprece-\\ndented global health crisis and has attracted much attention\\nfrom researchers [204]. No wonder, the COVID-19 pandemic\\nhas become one of the most inﬂuential medical issues over\\nthe past few years. In the COVID-19 pandemic, NLP-driven\\nsmart healthcare can be utilized for pandemic prevention,\\ndiagnosing, and drug development.\\nEarly forecasts of COVID-19 cases and pandemic knowl-\\nedge popularization are crucial to the prevention of the\\nCOVID-19 pandemic. In [205], an NLP module is embedded\\ninto an improved susceptible–infected model to build the\\nproposed hybrid AI model for COVID-19 prediction, showing\\nthat the forecasting accuracy of COVID-19 cases can be im-\\nproved by incorporating text inputs and with NLP techniques.\\nIn [206], the authors conclude that NLP techniques, e.g.,\\nNLP-aided information retrieval, literature-based discovery,\\nquestion answering and etc., can be applied to address the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='In [206], the authors conclude that NLP techniques, e.g.,\\nNLP-aided information retrieval, literature-based discovery,\\nquestion answering and etc., can be applied to address the\\ninformation/knowledge needs of both researchers and the\\npublic in the COVID-19 pandemic.\\nIn clinical practice, NLP can be utilized to identify posi-\\ntively diagnosed COVID19 patients from free text narratives\\n[207], assess thoracic CT imaging reports [208], and identify\\nindividuals with the greatest risk of severe complications due\\nto COVID-19 [209], and provide COVID-19 testing advice\\n[210]. Such applications would be very useful to accelerate\\nthe diagnosis of COVID-19, mitigate its worst effects, and\\nalso reduce costs for combating the COVID-19 pandemic.\\nNLP has also been applied to drug development confronting\\nCOVID-19. In [211], the authors developed an NLP method\\nto automatically recognize the associations among potential\\ntargeted host organ systems, associated clinical manifestations'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='COVID-19. In [211], the authors developed an NLP method\\nto automatically recognize the associations among potential\\ntargeted host organ systems, associated clinical manifestations\\nand pathways, and suggest potential drug candidates. NLP\\nmodels have also made great impacts in COVID-19 vaccine\\ndiscovery through protein interaction prediction, molecular\\nreaction modelling [212]. In addition, great opportunities\\nfor NLP can also be found in clinical design, regulatory\\ndecision-making, and pharmacovigilance [213]. These appli-\\ncations would signiﬁcantly reduce the time and cost of drug\\ndevelopment for COVID-19.\\nB. Mental health\\nThe mental health issues have received widespread and\\ncontinuously increasing attention for many years. Specially,\\nthe World Health Organization (WHO) claimed that the pan-\\ndemic and the resulting lockdowns, economic security, fear\\nand uncertainty would further cause devastating impacts on\\npeople’s mental health the world over in the past several'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='demic and the resulting lockdowns, economic security, fear\\nand uncertainty would further cause devastating impacts on\\npeople’s mental health the world over in the past several\\nyears [214]. NLP-driven smart healthcare has great value in\\npredicting/diagnosing and treating mental health conditions.\\nNLP techniques have been applied to early predict or\\nidentify/screen various mental disorders, such as psychiatric\\nillness [215], late-life depression [216], severe mental illness\\n(schizophrenia, schizoaffective disorder and bipolar disorder)\\n[217]. In addition, some works have shown that NLP tech-\\nniques can predict risk-taking behaviours (e.g., suicide) with\\ngood discrimination [218], [219] so that early interventions\\ncan be taken to save lives. The data collected for such\\nanalysis may include text data such as social media posts,\\nscreening surveys, EHRs [220], and also speech data come\\nfrom narrative interviews [221], etc.\\nNLP techniques could also (automatically) provide effective'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='screening surveys, EHRs [220], and also speech data come\\nfrom narrative interviews [221], etc.\\nNLP techniques could also (automatically) provide effective\\npsychotherapeutic interventions through web-based psycho-\\neducational interventions, online counseling, etc., to augment\\ntherapist-based mental health interventions, showing potential\\nfuture opportunities for their integration into online men-\\ntal health tools [222]. For example, the insights of [223]\\ncould help improve counselor training and generate real-time\\ncounseling quality monitoring and answer suggestion support\\ntools. In addition, several mental health related areas that may\\nbeneﬁt from NLP techniques, including characterizing and\\nunderstanding mental disorders, measuring health outcomes,\\nstudying of social and occupational functioning, etc, were\\nshown in [219]. Speciﬁcally, [224] showed that the older\\nwould respond better to digital assistants employing a socially-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 7, 'page_label': '8', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='studying of social and occupational functioning, etc, were\\nshown in [219]. Speciﬁcally, [224] showed that the older\\nwould respond better to digital assistants employing a socially-\\noriented interaction style rather than the one with a task-\\noriented style, which is promising to promote mental health\\nin older adults by providing social interaction and company.\\nV. L IMITATIONS AND OUTLOOK\\nAlthough recent advancements in deep learning and neural\\nNLP have brought extraordinary enhancement to smart health-\\ncare, there are still some limitations that current methods have\\nyet to overcome.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ZHOU et al.: NATURAL LANGUAGE PROCESSING FOR SMART HEALTHCARE 9\\nTABLE II\\nAPPLICATIONS DRIVEN BY NLP IN ALL SMART HEALTHCARE SCENARIOS .\\nCategory Sub-Category Representative Applications Related Techniques\\nClinical Practice\\nclinical communication\\nand data collection\\npatient-provider communication [129], [130] machine translation\\nclinical documentation [131]–[133] speech recognition\\nclinical decision support\\nbuild QA-based clinical decision support systems [84], [134], [135] information extraction\\nbuild clinical decision support systems with extracted information: family\\nhistory information [136], entities and relations [137], [138], treatment and\\nprognosis data [139], clinical data concepts and features [140], causal relations\\n[109], [110]\\nquestion answering\\nhealthcare quality control: assess clinical procedures [141], [142], warning of\\nADE [143], disease symptoms [144], [145], and outcome-related causal effects\\n[146]\\ninformation extraction, causal in-\\nference'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ADE [143], disease symptoms [144], [145], and outcome-related causal effects\\n[146]\\ninformation extraction, causal in-\\nference\\nprovide supporting evidence for decisions under evidence-based fashion [108],\\n[147]–[150]\\ninformation retrieval, causal infer-\\nence\\nHospital Management\\nmedical resource allocation\\npatient triage [151], [152] information extraction\\nenable users to communicate and control intelligent systems through virtual\\nassistants [153]–[155], hospital automation systems [156], [157] and collabo-\\nrative robots [158], [159]\\nspeech recognition, natural lan-\\nguage understanding\\npredict and reduce readmission rate [160]–[162] information extraction\\nfree medical staff from routine text writing [70], [163] information extraction\\ndata management manage clinical documents [18], [52], [62], [63], [164] text generation\\nease the HIS retrieval process based on semantic search [76], [165] and question\\nanswering [77]\\ntext classiﬁcation, text summariza-\\ntion, information extraction'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ease the HIS retrieval process based on semantic search [76], [165] and question\\nanswering [77]\\ntext classiﬁcation, text summariza-\\ntion, information extraction\\nservice quality control improve service quality and patient experience [166]–[168] information retrieval, question an-\\nswering\\nPersonal Care\\npersonal health assistants access online medical information [169] information retrieval\\nenable remote healthcare [170] speech recognition\\nassisting the elderly and\\nthe disabled\\ndaily assistance [171] speech recognition, natural lan-\\nguage understanding\\nsocial interaction and company [172], [173] speech recognition, speech synthe-\\nsis\\nassist people with speech impairments [122], [174]–[178], hearing loss [179],\\ndyslexia [180], or neurological disorders [119]–[121]\\nspeech recognition, speech synthe-\\nsis\\nPublic Health\\nhealth knowledge\\npopularization and\\nmedical education\\nacquisition and representation of medical knowledge [86]–[89], [97]–[100],\\n[181]\\nknowledge engineering'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='sis\\nPublic Health\\nhealth knowledge\\npopularization and\\nmedical education\\nacquisition and representation of medical knowledge [86]–[89], [97]–[100],\\n[181]\\nknowledge engineering\\nease the access of medical knowledge [1], [79], [81], [130], [182], [184], [185]question answering, information\\nretrieval, machine translation\\ngenerate medical case-based questions [186] question generation\\nconstruct simpliﬁed summaries [61] text summarization\\npopulation screening identify target populations [188] information extraction\\nanalyse of healthcare questionnaire and surveys [189] information extraction\\nDrug Development\\ndrug discovery map the interactions between diseases, chemical compounds, and biomacro-\\nmolecules, predict molecular properties, and design novel molecules [190]\\ninformation extraction, information\\nretrieval, knowledge engineering\\npreclinical research drug screening [191]–[193] information extraction\\npredict adverse drug reactions: side effect prediction [194], and toxicity'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='retrieval, knowledge engineering\\npreclinical research drug screening [191]–[193] information extraction\\npredict adverse drug reactions: side effect prediction [194], and toxicity\\nprediction [195], [196]\\ninformation extraction\\nclinical research\\nclinical trial design [110] information extraction, causal in-\\nference\\npatient recruitment [197]–[199] information extraction\\nclinical trial analytics [200] information extraction\\ndrug review and safety\\nmonitoring\\nadverse drug events discovery and drug safety monitoring [201]–[203] information extraction\\nUnderstanding human language . Although substantial\\nefforts have been made to enable natural language understand-\\ning, the ﬂexibility of human language still makes full under-\\nstanding difﬁcult, especially when ambiguity in biomedical\\ntexts is encountered. Misunderstanding could lead to inaccu-\\nrate actions taken by robots, useless information returned by\\nengines, and even wrong decisions made by decision support'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='texts is encountered. Misunderstanding could lead to inaccu-\\nrate actions taken by robots, useless information returned by\\nengines, and even wrong decisions made by decision support\\nsystems, leading to economic loss, time wasting, and even\\nmore serious consequences.\\nInterpretability. Although applications that rely on neural\\nNLP to extract features and make decisions show excellent\\nperformance in real tasks, they are usually challenged by\\nusers due to their weakness in interpretability. Interpretability\\nis essential for smart healthcare applications, especially in\\nclinical scenarios that require quality assurance in cases of\\nlow conﬁdence. One of the major interpretability issues is\\nthat the learned features are usually not understood by hu-\\nmans. In addition, when tuning pre-trained language models\\nto downstream tasks, no enough intuitions on data for ﬁne-\\ntuning or types of applications can be given to guarantee good\\nperformance. Although efforts have been made to achieve'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='to downstream tasks, no enough intuitions on data for ﬁne-\\ntuning or types of applications can be given to guarantee good\\nperformance. Although efforts have been made to achieve\\ninterpretable NLP-driven applications, existing theories and\\nmethodologies are still not convincing and acceptable for\\nmany healthcare researchers and institutions. Before the inter-\\npretability issue is fully explored, the role of decision support\\nsystems in clinical practice can only be auxiliary from the\\nperspectives of medical ethics and practical application.\\nImplementation. There are still many issues concerning the\\nimplementation of NLP-driven applications in smart health-\\ncare. With the development of neural NLP, large deep neu-\\nral networks (e.g., pre-trained language models) have been\\nquickly migrated to smart healthcare. What followed are the\\nincreased requirements in computing power and training cost,\\nand the concerns about the reliability of neural NLP systems.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 8, 'page_label': '9', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='quickly migrated to smart healthcare. What followed are the\\nincreased requirements in computing power and training cost,\\nand the concerns about the reliability of neural NLP systems.\\nPatient privacy also prevents these models from achieving\\nmore prominent effects in smart healthcare for further practice.\\nThe consideration of medical ethics when applying such\\nsystems makes practical implementation more difﬁcult.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='10 IEEE REVIEWS IN BIOMEDICAL ENGINEERING\\nIn addition to tackling the aforementioned limitations, there\\nare some other directions to enhance existing NLP systems for\\nsmart healthcare.\\nCombining multiple NLP techniques . One direction to\\nenhance existing NLP systems can be the combination of\\nmultiple NLP techniques. For example, text generation can\\nwork as a data augmentation method for achieving comparable\\nresults in many applications with limited original data, such\\nas training QA systems [65], [85] and other clinically relevant\\ntasks [225], [226]. Through automatic question generation,\\nquestionnaires and surveys for population screening can be\\ngenerated from EHRs, which may outperform handcrafted\\nones. Machine translation has also proven beneﬁcial for var-\\nious text-based tasks by increasing the availability of mul-\\ntilingual healthcare information [227]–[229], implying the\\npossibility of improving the performance of current CDS'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ious text-based tasks by increasing the availability of mul-\\ntilingual healthcare information [227]–[229], implying the\\npossibility of improving the performance of current CDS\\nsystems. In addition, exploration of general knowledge and\\ndomain knowledge in the ﬁeld of NLP for smart healthcare\\ndeserves further attention and veriﬁcation.\\nEnd-to-end applications. Current NLP driven applications\\nfor smart healthcare usually focus on dealing with tasks step\\nby step and do not fully explore the feature extraction capa-\\nbility of advanced neural NLP for complex smart healthcare\\ntasks. A deeper integration of NLP techniques and healthcare\\napplications in an end-to-end manner can map the inputs\\nand outputs directly, signiﬁcantly simplify traditional pipelines\\nfor complex applications, eliminate the biases of intermediate\\ncomponents, and therefore achieve better performance. Taking\\npopulation screening as an example, although NLP has been\\napplied to identify populations and analyse screening test'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='components, and therefore achieve better performance. Taking\\npopulation screening as an example, although NLP has been\\napplied to identify populations and analyse screening test\\nresults in traditional screening procedures, NLP techniques can\\nbe further applied to build end-to-end population screening\\nsystems, with which the correlations between populations\\nand optimal actions can be found to improve the screening\\nperformance and the quality of healthcare. Another example\\nwould be reducing the readmission rate. As mentioned before,\\nsome works have revealed that NLP has the ability to predict\\npatient readmission, but further studies on providing appropri-\\nate interventions to reduce the readmission rate are not fully\\nconducted. We look forward to studies that integrate the two\\nparts to reveal every possibility for readmission rate reducing.\\nFew-shot learning and incorporating domain knowledge.\\nBy exploiting the learning capability of neural networks and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='parts to reveal every possibility for readmission rate reducing.\\nFew-shot learning and incorporating domain knowledge.\\nBy exploiting the learning capability of neural networks and\\nlarge available corpora, neural NLP has shown powerful ability\\nin learning language representations. However, for downstream\\ntasks or smart healthcare applications, there is still a long\\nway for NLP to go. Taking clinical decision support as an\\nexample, there are a lot of rare diseases with only a small\\nnumber of observations available for training a clinical deci-\\nsion support system to distinguish rare diseases from common\\ndiseases. This is a quite challenging task, especially when\\nthere are similar outcomes among some rare diseases and\\ncommon diseases. In addition, high-quality labelled data are\\nundoubtedly essential to guarantee task accuracy in developing\\npractical applications for smart healthcare. However, quality-\\ncontrolled annotation not only requires a large amount of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='undoubtedly essential to guarantee task accuracy in developing\\npractical applications for smart healthcare. However, quality-\\ncontrolled annotation not only requires a large amount of\\ncost, but is also challenging due to the bias of experts’ level\\nof expertise. Therefore, even with well-learned pre-trained\\nlanguage models, few-shot learning algorithms and domain\\nknowledge are expected to be applied so that the ﬁne-tuned\\nmodels would be effective in learning from few rare disease\\nobservations or limited high-quality labelled data.\\nIncorporating multimodal and longitudinal data. Finally,\\nwe also anticipate future intelligent systems to utilize all avail-\\nable AI techniques, not only NLP, for practical applications\\nwith high accuracy and reliability. The past few years have\\nwitnessed the dominance of data-driven approaches in many\\napplications across various ﬁelds. NLP, computer vision, and\\nother machine learning algorithms can be applied to analyse'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='witnessed the dominance of data-driven approaches in many\\napplications across various ﬁelds. NLP, computer vision, and\\nother machine learning algorithms can be applied to analyse\\nmedical text, medical images, electronic recordings (e.g., heart\\nsound), sensors data, laboratory results, and even genetic\\ninformation. With multimodal learning, useful information\\nextracted from these modalities can be combined together to\\nperfectly ﬁt the need for a complete and accurate analysis\\nof available healthcare data and patients’ health status. In\\naddition, all of these data and clinical events can be longi-\\ntudinal, where time series analysis can be applied to extract\\nlong-term dependencies and improve health care delivery. By\\ncombining these techniques to analyse multimodal and lon-\\ngitudinal data, future intelligent systems would become more\\npowerful and reliable for patients, physicians, and healthcare\\ninstitutions for applications such as 24/7 health monitoring,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='gitudinal data, future intelligent systems would become more\\npowerful and reliable for patients, physicians, and healthcare\\ninstitutions for applications such as 24/7 health monitoring,\\nchronic-condition management, healthy lifestyle promotion,\\nand precision medicine.\\nVI. C ONCLUSION\\nIn the context of smart healthcare, NLP takes text or speech\\nas the input in various scenarios involving humans and ma-\\nchines, and realizes the functions of analysing and understand-\\ning human language. In this paper, we review existing studies\\nconcerning NLP for smart healthcare from the perspectives\\nof technique and application. We elaborate on different NLP\\napproaches and the NLP pipeline for smart healthcare from the\\ntechnical point of view. Table I provides the comparisons of\\ndifferent NLP approaches and their representative algorithms.\\nVarious text-oriented and speech-oriented NLP tasks are elab-\\norated to conclude existing methodologies for tackling such'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='different NLP approaches and their representative algorithms.\\nVarious text-oriented and speech-oriented NLP tasks are elab-\\norated to conclude existing methodologies for tackling such\\ntasks. By introducing smart healthcare applications employing\\nNLP techniques in various smart healthcare scenarios (in-\\ncluding clinical practice, hospital management, personal care,\\npublic health, and drug development), we show the strength\\nand possibility of NLP for delivering smart healthcare. Table II\\nprovides a detailed list of representative applications in smart\\nhealthcare and their related NLP techniques. We further dis-\\ncuss two speciﬁc medical issues, i.e., COVID-19 pandemic and\\nmental health, in which NLP-driven smart healthcare plays an\\nimportant role. After that, we discuss the limitations of current\\nworks across understanding human language, interpretability,\\nand implementation of NLP systems for smart healthcare.\\nFinally, we identify several directions for future works, notably'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 9, 'page_label': '10', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='works across understanding human language, interpretability,\\nand implementation of NLP systems for smart healthcare.\\nFinally, we identify several directions for future works, notably\\ncombining multiple NLP techniques, developing end-to-end\\napplications, few-shot learning, and incorporating multimodal\\nand longitudinal data.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ZHOU et al.: NATURAL LANGUAGE PROCESSING FOR SMART HEALTHCARE 11\\nREFERENCES\\n[1] S. Tian, W. Yang, J. M. L. Grange, P. Wang, W. Huang, and Z. Ye,\\n“Smart healthcare: Making medical care more intelligent,” Global\\nHealth Journal, vol. 3, no. 3, pp. 62–65, Sep. 2019.\\n[2] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent trends in\\ndeep learning based natural language processing,” arXiv:1708.02709\\n[cs], Nov. 2018.\\n[3] Y . Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural prob-\\nabilistic language model,” Journal of Machine Learning Research ,\\nvol. 3, no. Feb, pp. 1137–1155, 2003.\\n[4] J. Crim, R. McDonald, and F. Pereira, “Automatically annotating\\ndocuments with normalized gene lists,” BMC Bioinformatics , vol. 6,\\nno. 1, p. S13, May 2005.\\n[5] J. Vilares, M. A. Alonso, and M. Vilares, “Extraction of complex\\nindex terms in non-English IR: A shallow parsing based approach,”\\nInformation Processing & Management, vol. 44, no. 4, pp. 1517–1537,\\nJul. 2008.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='index terms in non-English IR: A shallow parsing based approach,”\\nInformation Processing & Management, vol. 44, no. 4, pp. 1517–1537,\\nJul. 2008.\\n[6] L. Chiticariu, Y . Li, and F. R. Reiss, “Rule-based information extraction\\nis dead! Long live rule-based information extraction systems!” in\\nProceedings of the 2013 Conference on Empirical Methods in Natural\\nLanguage Processing . Seattle, Washington, USA: Association for\\nComputational Linguistics, Oct. 2013, pp. 827–832.\\n[7] N. Kang, B. Singh, Z. Afzal, E. M. van Mulligen, and J. Kors,\\n“Using rule-based natural language processing to improve disease\\nnormalization in biomedical text,” Journal of the American Medical\\nInformatics Association : JAMIA , vol. 20, Oct. 2012.\\n[8] W.-H. Weng, K. B. Wagholikar, A. T. McCray, P. Szolovits, and H. C.\\nChueh, “Medical subdomain classiﬁcation of clinical notes using a\\nmachine learning-based natural language processing approach,” BMC\\nMedical Informatics and Decision Making , vol. 17, p. 155, Dec. 2017.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='machine learning-based natural language processing approach,” BMC\\nMedical Informatics and Decision Making , vol. 17, p. 155, Dec. 2017.\\n[9] D. Dessi, R. Helaoui, V . Kumar, D. R. Recupero, and D. Riboni, “TF-\\nIDF vs word embeddings for morbidity identiﬁcation in clinical notes:\\nAn initial study,” arXiv:2105.09632 [cs], Mar. 2020.\\n[10] O. Ozyegen, D. Kabe, and M. Cevik, “Word-level text highlighting\\nof medical texts forTelehealth services,” arXiv:2105.10400 [cs] , May\\n2021.\\n[11] M. Rahimian, J. L. Warner, S. K. Jain, R. B. Davis, J. A. Zerillo, and\\nR. M. Joyce, “Signiﬁcant and distinctive n-grams in oncology notes:\\nA text-mining method to analyze the effect of OpenNotes on clinical\\ndocumentation,” JCO Clinical Cancer Informatics, no. 3, pp. 1–9, Dec.\\n2019.\\n[12] A. Yazdani, R. Safdari, A. Golkar, and S. Rostam Niakan Kalhori,\\n“Words prediction based on N-gram model for free-text entry in\\nelectronic health records,” Health Information Science and Systems ,\\nvol. 7, Feb. 2019.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='“Words prediction based on N-gram model for free-text entry in\\nelectronic health records,” Health Information Science and Systems ,\\nvol. 7, Feb. 2019.\\n[13] V . Yip, M. Mete, U. Topaloglu, and S. Kockara, “Concept discovery for\\npathology reports using an N-gram model,” Summit on Translational\\nBioinformatics, vol. 2010, pp. 43–47, Mar. 2010.\\n[14] M. Beeksma, S. Verberne, A. van den Bosch, E. Das, I. Hendrickx,\\nand S. Groenewoud, “Predicting life expectancy with a long short-term\\nmemory recurrent neural network using electronic medical records,”\\nBMC Medical Informatics and Decision Making , vol. 19, no. 1, p. 36,\\nFeb. 2019.\\n[15] A. N. Jagannatha and H. Yu, “Bidirectional RNN for medical event\\ndetection in electronic health records,” in Proceedings of the 2016\\nConference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies . San\\nDiego, California: Association for Computational Linguistics, Jun.\\n2016, pp. 473–482.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='Computational Linguistics: Human Language Technologies . San\\nDiego, California: Association for Computational Linguistics, Jun.\\n2016, pp. 473–482.\\n[16] C. Liu, H. Sun, N. Du, S. Tan, H. Fei, W. Fan, T. Yang, H. Wu, Y . Li,\\nand C. Zhang, “Augmented LSTM framework to construct medical\\nself-diagnosis android,” in 2016 IEEE 16th International Conference\\non Data Mining (ICDM) , Dec. 2016, pp. 251–260.\\n[17] Y .-S. Zhao, K.-L. Zhang, H.-C. Ma, and K. Li, “Leveraging text\\nskeleton for de-identiﬁcation of electronic medical records,” BMC\\nMedical Informatics and Decision Making , vol. 18, no. Suppl 1, p. 18,\\nMar. 2018.\\n[18] M. Hughes, I. Li, S. Kotoulas, and T. Suzumura, “Medical text\\nclassiﬁcation using convolutional neural networks,” Studies in Health\\nTechnology and Informatics, vol. 235, pp. 246–250, 2017.\\n[19] X. Li, H. Wang, H. He, J. Du, J. Chen, and J. Wu, “Intelligent diagnosis\\nwith chinese electronic medical records based on convolutional neural'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='[19] X. Li, H. Wang, H. He, J. Du, J. Chen, and J. Wu, “Intelligent diagnosis\\nwith chinese electronic medical records based on convolutional neural\\nnetworks,” BMC Bioinformatics, vol. 20, no. 1, p. 62, Feb. 2019.\\n[20] Y . Li, B. Qian, X. Zhang, and H. Liu, “Graph neural network-based\\ndiagnosis prediction,” Big Data, vol. 8, no. 5, pp. 379–390, Oct. 2020.\\n[21] Z. Sun, H. Yin, H. Chen, T. Chen, L. Cui, and F. Yang, “Disease\\nprediction via graph neural networks,” IEEE Journal of Biomedical\\nand Health Informatics , vol. 25, no. 3, pp. 818–826, Mar. 2021.\\n[22] T. Wu, Y . Wang, Y . Wang, E. Zhao, and Y . Yuan, “Leveraging graph-\\nbased hierarchical medical entity embedding for healthcare applica-\\ntions,” Scientiﬁc Reports, vol. 11, no. 1, p. 5858, Mar. 2021.\\n[23] T. Mayer, E. Cabrio, and S. Villata, “Transformer-based argument\\nmining for healthcare applications,” in ECAI 2020 - 24th European\\nConference on Artiﬁcial Intelligence, Santiago de Compostela / Online,\\nSpain, Aug. 2020.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='mining for healthcare applications,” in ECAI 2020 - 24th European\\nConference on Artiﬁcial Intelligence, Santiago de Compostela / Online,\\nSpain, Aug. 2020.\\n[24] D. Zhang, J. Thadajarassiri, C. Sen, and E. Rundensteiner, “Time-\\naware transformer-based network for clinical notes series prediction,”\\nin Machine Learning for Healthcare Conference . PMLR, Sep. 2020,\\npp. 566–588.\\n[25] S. Tokala, V . Gambhir, and A. Mukherjee, “Deep learning for social\\nmedia health text classiﬁcation,” in Proceedings of the 2018 EMNLP\\nWorkshop SMM4H: The 3rd Social Media Mining for Health Applica-\\ntions Workshop & Shared Task . Brussels, Belgium: Association for\\nComputational Linguistics, Oct. 2018, pp. 61–64.\\n[26] E. Choi, M. T. Bahadori, J. A. Kulas, A. Schuetz, W. F. Stewart, and\\nJ. Sun, “RETAIN: An interpretable predictive model for healthcare\\nusing reverse time attention mechanism,” arXiv:1608.05745 [cs], Feb.\\n2017.\\n[27] J. Chu, W. Dong, K. He, H. Duan, and Z. Huang, “Using neural'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='using reverse time attention mechanism,” arXiv:1608.05745 [cs], Feb.\\n2017.\\n[27] J. Chu, W. Dong, K. He, H. Duan, and Z. Huang, “Using neural\\nattention networks to detect adverse medical events from electronic\\nhealth records,” Journal of Biomedical Informatics , vol. 87, pp. 118–\\n130, Nov. 2018.\\n[28] P. Chakraborty, F. Wang, J. Hu, and D. Sow, “Explicit-blurred\\nmemory network for analyzing patient electronic health records,”\\narXiv:1911.06472 [cs, stat] , Jul. 2020.\\n[29] J. Song, Y . Wang, S. Tang, Y . Zhang, Z. Chen, Z. Zhang, T. Zhang,\\nand F. Wu, “Local–Global memory neural network for medication\\nprediction,” IEEE Transactions on Neural Networks and Learning\\nSystems, vol. 32, no. 4, pp. 1723–1736, Apr. 2021.\\n[30] H.-J. Yoon, J. Gounley, M. T. Young, and G. Tourassi, “Information ex-\\ntraction from cancer pathology reports with graph convolution networks\\nfor natural language texts,” in 2019 IEEE International Conference on\\nBig Data (Big Data) , Dec. 2019, pp. 4561–4564.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='traction from cancer pathology reports with graph convolution networks\\nfor natural language texts,” in 2019 IEEE International Conference on\\nBig Data (Big Data) , Dec. 2019, pp. 4561–4564.\\n[31] R. Cai, B. Zhu, L. Ji, T. Hao, J. Yan, and W. Liu, “An CNN-LSTM\\nattention approach to understanding user query intent from online\\nhealth communities,” in 2017 IEEE International Conference on Data\\nMining Workshops (ICDMW), Nov. 2017, pp. 430–437.\\n[32] B. Tang, X. Wang, J. Yan, and Q. Chen, “Entity recognition in chinese\\nclinical text using attention-based CNN-LSTM-CRF,” BMC Medical\\nInformatics and Decision Making , vol. 19, no. 3, p. 74, Apr. 2019.\\n[33] E. Choi, Z. Xu, Y . Li, M. W. Dusenberry, G. Flores, Y . Xue, and A. M.\\nDai, “Learning the graphical structure of electronic health records with\\ngraph convolutional transformer,” arXiv:1906.04716 [cs, stat] , Jan.\\n2020.\\n[34] J. Wang, X. Chen, Y . Zhang, Y . Zhang, J. Wen, H. Lin, Z. Yang, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='graph convolutional transformer,” arXiv:1906.04716 [cs, stat] , Jan.\\n2020.\\n[34] J. Wang, X. Chen, Y . Zhang, Y . Zhang, J. Wen, H. Lin, Z. Yang, and\\nX. Wang, “Document-level biomedical relation extraction using graph\\nconvolutional network and multihead attention: Algorithm development\\nand validation,” JMIR Medical Informatics , vol. 8, no. 7, p. e17638,\\nJul. 2020.\\n[35] X. Qiu, T. Sun, Y . Xu, Y . Shao, N. Dai, and X. Huang, “Pre-trained\\nmodels for natural language processing: A survey,” arXiv:2003.08271\\n[cs], Apr. 2020.\\n[36] A. L. Beam, B. Kompa, A. Schmaltz, I. Fried, G. Weber, N. P. Palmer,\\nX. Shi, T. Cai, and I. S. Kohane, “Clinical concept embeddings learned\\nfrom massive sources of multimodal medical data,” arXiv:1804.01486\\n[cs, stat], Aug. 2019.\\n[37] X. Cai, J. Gao, K. Y . Ngiam, B. C. Ooi, Y . Zhang, and X. Yuan, “Med-\\nical concept embedding with time-aware attention,” in Proceedings of\\nthe 27th International Joint Conference on Artiﬁcial Intelligence , ser.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 10, 'page_label': '11', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ical concept embedding with time-aware attention,” in Proceedings of\\nthe 27th International Joint Conference on Artiﬁcial Intelligence , ser.\\nIJCAI’18. Stockholm, Sweden: AAAI Press, Jul. 2018, pp. 3984–\\n3990.\\n[38] M. Kholghi, L. De Vine, L. Sitbon, G. Zuccon, and A. Nguyen, “The\\nbeneﬁts of word embeddings features for active learning in clinical\\ninformation extraction,” in Proceedings of the Australasian Language\\nTechnology Association Workshop 2016 , Melbourne, Australia, Dec.\\n2016, pp. 25–34.\\n[39] Y . Zhang, Q. Chen, Z. Yang, H. Lin, and Z. Lu, “BioWordVec,\\nimproving biomedical word embeddings with subword information and\\nMeSH,” Scientiﬁc Data, vol. 6, no. 1, p. 52, May 2019.\\n[40] S. Dubois, N. Romano, D. C. Kale, N. Shah, and K. Jung, “Effective\\nrepresentations of clinical notes,” arXiv:1705.07025 [cs, stat] , Aug.\\n2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='12 IEEE REVIEWS IN BIOMEDICAL ENGINEERING\\n[41] Q. Jin, B. Dhingra, W. W. Cohen, and X. Lu, “Probing biomedical\\nembeddings from language models,”arXiv:1904.02181 [cs], Apr. 2019.\\n[42] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang,\\n“BioBERT: A pre-trained biomedical language representation model\\nfor biomedical text mining,” Bioinformatics, vol. 36, no. 4, pp. 1234–\\n1240, Feb. 2020.\\n[43] K. Huang, J. Altosaar, and R. Ranganath, “ClinicalBERT: Modeling\\nclinical notes and predicting hospital readmission,” arXiv:1904.05342\\n[cs], Nov. 2020.\\n[44] L. Rasmy, Y . Xiang, Z. Xie, C. Tao, and D. Zhi, “Med-BERT: Pre-\\ntrained contextualized embeddings on large-scale structured electronic\\nhealth records for disease prediction,” npj Digital Medicine , vol. 4,\\nno. 1, pp. 1–13, May 2021.\\n[45] Y . Li, S. Rao, J. R. A. Solares, A. Hassaine, R. Ramakrishnan,\\nD. Canoy, Y . Zhu, K. Rahimi, and G. Salimi-Khorshidi, “BEHRT:\\nTransformer for electronic health records,” Scientiﬁc Reports, vol. 10,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='D. Canoy, Y . Zhu, K. Rahimi, and G. Salimi-Khorshidi, “BEHRT:\\nTransformer for electronic health records,” Scientiﬁc Reports, vol. 10,\\nno. 1, p. 7155, Apr. 2020.\\n[46] E. T. R. Schneider, J. V . A. de Souza, Y . B. Gumiel, C. Moro, and E. C.\\nParaiso, “A GPT-2 language model for biomedical texts in portuguese,”\\nin 2021 IEEE 34th International Symposium on Computer-Based\\nMedical Systems (CBMS) , Jun. 2021, pp. 474–479.\\n[47] A. Akkasi, E. Varo ˘glu, and N. Dimililer, “ChemTok: A new rule based\\ntokenizer for chemical named entity recognition,” BioMed Research\\nInternational, vol. 2016, 2016.\\n[48] H.-J. Dai, P.-T. Lai, Y .-C. Chang, and R. T.-H. Tsai, “Enhancing of\\nchemical compound and drug name recognition using representative\\ntag scheme and ﬁne-grained tokenization,”Journal of Cheminformatics,\\nvol. 7, no. Suppl 1 Text mining for chemistry and the CHEMDNER\\ntrack, p. S14, 2015.\\n[49] H. Liu, T. Christiansen, W. A. Baumgartner, and K. Verspoor, “Bi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='vol. 7, no. Suppl 1 Text mining for chemistry and the CHEMDNER\\ntrack, p. S14, 2015.\\n[49] H. Liu, T. Christiansen, W. A. Baumgartner, and K. Verspoor, “Bi-\\noLemmatizer: A lemmatization tool for morphological processing of\\nbiomedical text,” Journal of Biomedical Semantics , vol. 3, p. 3, Apr.\\n2012.\\n[50] Y . Qiao, C. Xiong, Z. Liu, and Z. Liu, “Understanding the Behaviors\\nof BERT in Ranking,” arXiv:1904.07531 [cs], Apr. 2019.\\n[51] X. Liu, F. Zhang, Z. Hou, Z. Wang, L. Mian, J. Zhang, and J. Tang,\\n“Self-supervised learning: Generative or contrastive,” IEEE Transac-\\ntions on Knowledge and Data Engineering , pp. 1–1, 2021.\\n[52] Y . Wang, S. Sohn, S. Liu, F. Shen, L. Wang, E. J. Atkinson, S. Amin,\\nand H. Liu, “A clinical text classiﬁcation paradigm using weak supervi-\\nsion and deep representation,” BMC Medical Informatics and Decision\\nMaking, vol. 19, no. 1, p. 1, Jan. 2019.\\n[53] S. Hassanpour and C. P. Langlotz, “Information extraction from multi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='sion and deep representation,” BMC Medical Informatics and Decision\\nMaking, vol. 19, no. 1, p. 1, Jan. 2019.\\n[53] S. Hassanpour and C. P. Langlotz, “Information extraction from multi-\\ninstitutional radiology reports,” Artiﬁcial intelligence in medicine ,\\nvol. 66, pp. 29–39, Jan. 2016.\\n[54] N. Perera, M. Dehmer, and F. Emmert-Streib, “Named entity recog-\\nnition and relation detection for biomedical information extraction,”\\nFrontiers in Cell and Developmental Biology , vol. 8, p. 673, Aug.\\n2020.\\n[55] A. Thillaisundaram and T. Togia, “Biomedical relation extraction\\nwith pre-trained language representations and minimal task-speciﬁc\\narchitecture,” in Proceedings of The 5th Workshop on BioNLP Open\\nShared Tasks. Hong Kong, China: Association for Computational\\nLinguistics, Nov. 2019, pp. 84–89.\\n[56] S. ˇZitnik, M. ˇZitnik, B. Zupan, and M. Bajec, “Sieve-based relation\\nextraction of gene regulatory networks from biological literature,”BMC'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='Linguistics, Nov. 2019, pp. 84–89.\\n[56] S. ˇZitnik, M. ˇZitnik, B. Zupan, and M. Bajec, “Sieve-based relation\\nextraction of gene regulatory networks from biological literature,”BMC\\nBioinformatics, vol. 16, no. Suppl 16, p. S1, Oct. 2015.\\n[57] P. Jindal and D. Roth, “Extraction of events and temporal expressions\\nfrom clinical narratives,” Journal of Biomedical Informatics , vol. 46,\\npp. S13–S19, Dec. 2013.\\n[58] A. Garg and M. Agarwal, “Machine translation: A literature review,”\\narXiv:1901.01122 [cs], Dec. 2018.\\n[59] A. B ´erard, Z. M. Kim, V . Nikoulina, E. L. Park, and M. Gall ´e, “A\\nmultilingual neural machine translation model for biomedical data,” in\\nProceedings of the 1st Workshop on NLP for COVID-19 (Part 2) at\\nEMNLP 2020 . Online: Association for Computational Linguistics,\\nDec. 2020.\\n[60] K. Kirchhoff, A. M. Turner, A. Axelrod, and F. Saavedra, “Application\\nof statistical machine translation to public health information: A feasi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='Dec. 2020.\\n[60] K. Kirchhoff, A. M. Turner, A. Axelrod, and F. Saavedra, “Application\\nof statistical machine translation to public health information: A feasi-\\nbility study,” Journal of the American Medical Informatics Association\\n: JAMIA, vol. 18, no. 4, pp. 473–478, 2011.\\n[61] M. Afzal, F. Alam, K. M. Malik, and G. M. Malik, “Clinical Con-\\ntext–Aware biomedical text summarization using deep neural network:\\nModel development and validation,” Journal of Medical Internet Re-\\nsearch, vol. 22, no. 10, p. e19810, Oct. 2020.\\n[62] J. Lopez, “Automatic summarization of medical conversations, a\\nreview,” in TALN-RECITAL 2019-PFIA 2019 . Toulouse, France:\\nATALA, Jul. 2019, pp. 487–498.\\n[63] G. Manas, V . Aribandi, U. Kursuncu, A. Alambo, V . L. Shalin,\\nK. Thirunarayan, J. Beich, M. Narasimhan, and A. Sheth, “Knowledge-\\ninfused abstractive summarization of clinical diagnostic interviews:\\nFramework development study,” JMIR Mental Health , vol. 8, no. 5,\\np. e20865, May 2021.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='infused abstractive summarization of clinical diagnostic interviews:\\nFramework development study,” JMIR Mental Health , vol. 8, no. 5,\\np. e20865, May 2021.\\n[64] I. Pistol, D. Trandab ˘at,, and M. R ˘aschip, “Medi-test: Generating tests\\nfrom medical reference texts,” Data, vol. 3, no. 4, p. 70, Dec. 2018.\\n[65] S. Shen, Y . Li, N. Du, X. Wu, Y . Xie, S. Ge, T. Yang, K. Wang,\\nX. Liang, and W. Fan, “On the generation of medical question-answer\\npairs,” arXiv:1811.00681 [cs], Dec. 2019.\\n[66] W. Wang, T. Hao, and W. Liu, “Automatic question generation for\\nlearning evaluation in medicine,” in Advances in Web Based Learning\\n– ICWL 2007 , ser. Lecture Notes in Computer Science, H. Leung,\\nF. Li, R. Lau, and Q. Li, Eds. Berlin, Heidelberg: Springer, 2008, pp.\\n242–251.\\n[67] D. Li, Z. Ren, P. Ren, Z. Chen, M. Fan, J. Ma, and M. de Rijke, “Semi-\\nsupervised variational reasoning for medical dialogue generation,”\\nProceedings of the 44th International ACM SIGIR Conference on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='supervised variational reasoning for medical dialogue generation,”\\nProceedings of the 44th International ACM SIGIR Conference on\\nResearch and Development in Information Retrieval , pp. 544–554, Jul.\\n2021.\\n[68] S. Lin, P. Zhou, X. Liang, J. Tang, R. Zhao, Z. Chen, and L. Lin,\\n“Graph-evolving meta-learning for low-resource medical dialogue gen-\\neration,” arXiv:2012.11988 [cs], Dec. 2020.\\n[69] W. Yang, G. Zeng, B. Tan, Z. Ju, S. Chakravorty, X. He, S. Chen,\\nX. Yang, Q. Wu, Z. Yu, E. Xing, and P. Xie, “On the generation of\\nmedical dialogues for COVID-19,” arXiv:2005.05442 [cs], Jun. 2020.\\n[70] S. Pauws, A. Gatt, E. Krahmer, and E. Reiter, “Making effective use of\\nhealthcare data using data-to-text technology,” arXiv:1808.03507 [cs],\\nAug. 2018.\\n[71] V . Kougia, J. Pavlopoulos, and I. Androutsopoulos, “A survey on\\nbiomedical image captioning,” arXiv:1905.13302 [cs], May 2019.\\n[72] Y . Xiong, B. Du, and P. Yan, “Reinforced transformer for medical'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='biomedical image captioning,” arXiv:1905.13302 [cs], May 2019.\\n[72] Y . Xiong, B. Du, and P. Yan, “Reinforced transformer for medical\\nimage captioning,” in Machine Learning in Medical Imaging , ser.\\nLecture Notes in Computer Science, H.-I. Suk, M. Liu, P. Yan, and\\nC. Lian, Eds. Cham: Springer International Publishing, 2019, pp.\\n673–680.\\n[73] X. He, Z. Cai, W. Wei, Y . Zhang, L. Mou, E. Xing, and P. Xie,\\n“Towards visual question answering on pathology images,” inProceed-\\nings of the 59th Annual Meeting of the Association for Computational\\nLinguistics and the 11th International Joint Conference on Natural\\nLanguage Processing (Volume 2: Short Papers) . Online: Association\\nfor Computational Linguistics, Aug. 2021, pp. 708–718.\\n[74] L.-M. Zhan, B. Liu, L. Fan, J. Chen, and X.-M. Wu, “Medical visual\\nquestion answering via conditional reasoning,” in Proceedings of the\\n28th ACM International Conference on Multimedia, ser. MM ’20. New'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='question answering via conditional reasoning,” in Proceedings of the\\n28th ACM International Conference on Multimedia, ser. MM ’20. New\\nYork, NY , USA: Association for Computing Machinery, Oct. 2020, pp.\\n2345–2354.\\n[75] B. Afrae, D. Yousra, A. Imane, B. A. Mohamed, and A. B. Abdel-\\nhakim, “A new visual question answering system for medical images\\ncharacterization,” in Proceedings of the 4th International Conference\\non Smart City Applications , ser. SCA ’19. New York, NY , USA:\\nAssociation for Computing Machinery, Oct. 2019, pp. 1–7.\\n[76] H. Wu, G. Toti, K. I. Morley, Z. M. Ibrahim, A. Folarin, R. Jackson,\\nI. Kartoglu, A. Agrawal, C. Stringer, D. Gale, G. Gorrell, A. Roberts,\\nM. Broadbent, R. Stewart, and R. J. Dobson, “SemEHR: A general-\\npurpose semantic search system to surface semantic data from clinical\\nnotes for tailored care, trial recruitment, and clinical research,” Journal\\nof the American Medical Informatics Association : JAMIA , vol. 25,\\nno. 5, pp. 530–537, Jan. 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 11, 'page_label': '12', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='notes for tailored care, trial recruitment, and clinical research,” Journal\\nof the American Medical Informatics Association : JAMIA , vol. 25,\\nno. 5, pp. 530–537, Jan. 2018.\\n[77] J. Gobeill, A. Gaudinat, E. Pasche, D. Vishnyakova, P. Gaudet,\\nA. Bairoch, and P. Ruch, “Deep question answering for protein anno-\\ntation,” Database: The Journal of Biological Databases and Curation ,\\nvol. 2015, Sep. 2015.\\n[78] A. Montazeralghaem, R. Rahimi, and J. Allan, “Relevance ranking\\nbased on query-aware context analysis,” Advances in Information\\nRetrieval, vol. 12035, pp. 446–460, Mar. 2020.\\n[79] B. Xu, H. Lin, Y . Lin, Y . Ma, L. Yang, J. Wang, and Z. Yang,\\n“Improve biomedical information retrieval using modiﬁed learning to\\nrank methods,”IEEE/ACM Transactions on Computational Biology and\\nBioinformatics, vol. 15, no. 6, pp. 1797–1809, Nov. 2018.\\n[80] J. Urbain, O. Frieder, and N. Goharian, “Passage relevance models for\\ngenomics search,” BMC Bioinformatics , vol. 10, no. Suppl 3, p. S3,\\nMar. 2009.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ZHOU et al.: NATURAL LANGUAGE PROCESSING FOR SMART HEALTHCARE 13\\n[81] S. Mohan, N. Fiorini, S. Kim, and Z. Lu, “A fast deep learning\\nmodel for textual relevance in biomedical information retrieval,” in\\nProceedings of the 2018 World Wide Web Conference, ser. WWW ’18.\\nRepublic and Canton of Geneva, CHE: International World Wide Web\\nConferences Steering Committee, Apr. 2018, pp. 77–86.\\n[82] D. Hristovski, D. Dinevski, A. Kastrin, and T. C. Rindﬂesch, “Biomedi-\\ncal question answering using semantic relations,” BMC Bioinformatics,\\nvol. 16, no. 1, p. 6, Jan. 2015.\\n[83] V . Vinod, S. Agrawal, V . Gaurav, P. R, and S. Choudhary, “Multilingual\\nmedical question answering and information retrieval for rural health\\nintelligence access,” arXiv:2106.01251 [cs], Jun. 2021.\\n[84] M. A. H. Zahid, A. Mittal, R. C. Joshi, and G. Atluri, “CLINIQA:\\nA machine intelligence based clinical question answering system,”\\narXiv:1805.05927 [cs], May 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='[84] M. A. H. Zahid, A. Mittal, R. C. Joshi, and G. Atluri, “CLINIQA:\\nA machine intelligence based clinical question answering system,”\\narXiv:1805.05927 [cs], May 2018.\\n[85] P. Wang, T. Shi, and C. K. Reddy, “Text-to-SQL generation for question\\nanswering on electronic medical records,” in Proceedings of The Web\\nConference 2020. Taipei Taiwan: ACM, Apr. 2020, pp. 350–361.\\n[86] Z. Jiang, C. Chi, and Y . Zhan, “Research on medical question an-\\nswering system based on knowledge graph,” IEEE Access, vol. 9, pp.\\n21 094–21 101, 2021.\\n[87] H. Liu, Q. Hu, Y . Zhang, C. Xing, and M. Sheng, “A knowledge-\\nbased health question answering system,” in Smart Health, ser. Lecture\\nNotes in Computer Science, H. Chen, D. D. Zeng, E. Karahanna, and\\nI. Bardhan, Eds. Cham: Springer International Publishing, 2017, pp.\\n286–291.\\n[88] D. Demner-Fushman and J. Lin, “Answering clinical questions with\\nknowledge-based and statistical techniques,” Computational Linguis-\\ntics, vol. 33, no. 1, pp. 63–103, Mar. 2007.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='286–291.\\n[88] D. Demner-Fushman and J. Lin, “Answering clinical questions with\\nknowledge-based and statistical techniques,” Computational Linguis-\\ntics, vol. 33, no. 1, pp. 63–103, Mar. 2007.\\n[89] R. M. Terol, P. Mart ´ınez-Barco, and M. Palomar, “A knowledge based\\nmethod for the medical question answering problem,” Computers in\\nBiology and Medicine , vol. 37, no. 10, pp. 1511–1521, Oct. 2007.\\n[90] Z. Liu, E. Peng, S. Yan, G. Li, and T. Hao, “T-know: A knowledge\\ngraph-based question answering and infor-mation retrieval system for\\ntraditional chinese medicine,” in Proceedings of the 27th International\\nConference on Computational Linguistics: System Demonstrations .\\nSanta Fe, New Mexico: Association for Computational Linguistics,\\nAug. 2018, pp. 15–19.\\n[91] E. Mutabazi, J. Ni, G. Tang, and W. Cao, “A review on medical textual\\nquestion answering systems based on deep learning approaches,”\\nApplied Sciences, vol. 11, no. 12, p. 5456, Jan. 2021.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='[91] E. Mutabazi, J. Ni, G. Tang, and W. Cao, “A review on medical textual\\nquestion answering systems based on deep learning approaches,”\\nApplied Sciences, vol. 11, no. 12, p. 5456, Jan. 2021.\\n[92] H. Shim, D. Lowet, S. Luca, and B. Vanrumste, “Building blocks\\nof a task-oriented dialogue system in the healthcare domain,” in\\nProceedings of the Second Workshop on Natural Language Processing\\nfor Medical Conversations . Online: Association for Computational\\nLinguistics, Jun. 2021, pp. 47–57.\\n[93] Z. Wei, Q. Liu, B. Peng, H. Tou, T. Chen, X. Huang, K.-f. Wong,\\nand X. Dai, “Task-oriented dialogue system for automatic diagnosis,”\\nin Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics (Volume 2: Short Papers) . Melbourne,\\nAustralia: Association for Computational Linguistics, Jul. 2018, pp.\\n201–207.\\n[94] L. Xu, Q. Zhou, K. Gong, X. Liang, J. Tang, and L. Lin, “End-to-end\\nknowledge-routed relational dialogue system for automatic diagnosis,”'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='201–207.\\n[94] L. Xu, Q. Zhou, K. Gong, X. Liang, J. Tang, and L. Lin, “End-to-end\\nknowledge-routed relational dialogue system for automatic diagnosis,”\\narXiv:1901.10623 [cs], Mar. 2019.\\n[95] H. Kawata, K. Ookawara, M. Muta, S. Masuko, and J. Hoshino,\\n“Lifestyle agent: The chat-oriented dialogue system for lifestyle man-\\nagement,” in Entertainment Computing – ICEC 2017 , ser. Lecture\\nNotes in Computer Science, N. Munekata, I. Kunita, and J. Hoshino,\\nEds. Cham: Springer International Publishing, 2017, pp. 396–399.\\n[96] R. Studer, V . R. Benjamins, and D. Fensel, “Knowledge engineering:\\nPrinciples and methods,” Data & Knowledge Engineering , vol. 25,\\nno. 1, pp. 161–197, Mar. 1998.\\n[97] T. Goodwin and S. M. Harabagiu, “Automatic generation of a qualiﬁed\\nmedical knowledge graph and its usage for retrieving patient cohorts\\nfrom electronic medical records,” in 2013 IEEE Seventh International\\nConference on Semantic Computing , Sep. 2013, pp. 363–370.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='medical knowledge graph and its usage for retrieving patient cohorts\\nfrom electronic medical records,” in 2013 IEEE Seventh International\\nConference on Semantic Computing , Sep. 2013, pp. 363–370.\\n[98] A. Rossanez, J. C. dos Reis, R. d. S. Torres, and H. de Ribaupierre,\\n“KGen: A knowledge graph generator from biomedical scientiﬁc\\nliterature,” BMC Medical Informatics and Decision Making , vol. 20,\\nno. 4, p. 314, Dec. 2020.\\n[99] M. Rotmensch, Y . Halpern, A. Tlimat, S. Horng, and D. Sontag,\\n“Learning a health knowledge graph from electronic medical records,”\\nScientiﬁc Reports, vol. 7, no. 1, p. 5994, Jul. 2017.\\n[100] H. Wang, Q. Zhang, and J. Yuan, “Semantically enhanced medical\\ninformation retrieval system: A tensor factorization based approach,”\\nIEEE Access, vol. 5, pp. 7584–7593, 2017.\\n[101] Y . Pan, Q. Chen, W. Peng, X. Wang, B. Hu, X. Liu, J. Chen, and\\nW. Zhou, “MedWriter: Knowledge-aware medical text generation,” in\\nProceedings of the 28th International Conference on Computational'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='W. Zhou, “MedWriter: Knowledge-aware medical text generation,” in\\nProceedings of the 28th International Conference on Computational\\nLinguistics. Barcelona, Spain (Online): International Committee on\\nComputational Linguistics, Dec. 2020, pp. 2363–2368.\\n[102] A. Stoica, T. Kadar, C. Lemnaru, R. Potolea, and M. D ˆıns ¸oreanu,\\n“Intent detection and slot ﬁlling with capsule net architectures for a\\nromanian home assistant,” Sensors, vol. 21, no. 4, p. 1230, Jan. 2021.\\n[103] A. Neuraz, L. C. Llanos, A. Burgun, and S. Rosset, “Natural language\\nunderstanding for task oriented dialog in the biomedical domain in a\\nlow resources context,” arXiv:1811.09417 [cs], Nov. 2018.\\n[104] C. Zhang, N. Du, W. Fan, Y . Li, C.-T. Lu, and P. S. Yu, “Bringing\\nsemantic structures to user intent detection in online medical queries,”\\nin 2017 IEEE International Conference on Big Data (Big Data) , Dec.\\n2017, pp. 1019–1026.\\n[105] I. Giachos, E. C. Papakitsos, and G. Chorozoglou, “Exploring natural'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='in 2017 IEEE International Conference on Big Data (Big Data) , Dec.\\n2017, pp. 1019–1026.\\n[105] I. Giachos, E. C. Papakitsos, and G. Chorozoglou, “Exploring natural\\nlanguage understanding in robotic interfaces,” International Journal of\\nAdvances in Intelligent Informatics, vol. 3, no. 1, pp. 10–19, Mar. 2017.\\n[106] J. Thomason, A. Padmakumar, J. Sinapov, N. Walker, Y . Jiang,\\nH. Yedidsion, J. Hart, P. Stone, and R. J. Mooney, “Improving grounded\\nnatural language understanding through human-robot dialog,” 2019\\nInternational Conference on Robotics and Automation (ICRA) , pp.\\n6934–6941, May 2019.\\n[107] A. Feder, K. A. Keith, E. Manzoor, R. Pryzant, D. Sridhar, Z. Wood-\\nDoughty, J. Eisenstein, J. Grimmer, R. Reichart, M. E. Roberts, B. M.\\nStewart, V . Veitch, and D. Yang, “Causal inference in natural lan-\\nguage processing: Estimation, prediction, interpretation and beyond,”\\narXiv:2109.00725 [cs], Sep. 2021.\\n[108] J. Zeng, M. F. Gensheimer, D. L. Rubin, S. Athey, and R. D. Shachter,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='guage processing: Estimation, prediction, interpretation and beyond,”\\narXiv:2109.00725 [cs], Sep. 2021.\\n[108] J. Zeng, M. F. Gensheimer, D. L. Rubin, S. Athey, and R. D. Shachter,\\n“Uncovering interpretable potential confounders in electronic medical\\nrecords,” medRxiv : the preprint server for health sciences , 2021.\\n[109] S. Doan, E. W. Yang, S. S. Tilak, P. W. Li, D. S. Zisook, and M. Torii,\\n“Extracting health-related causality from twitter messages using natural\\nlanguage processing,” BMC Medical Informatics and Decision Making,\\nvol. 19, no. 3, p. 79, Apr. 2019.\\n[110] G. Nordon, G. Koren, V . Shalev, B. Kimelfeld, U. Shalit, and K. Radin-\\nsky, “Building causal graphs from medical literature and electronic\\nmedical records,” Proceedings of the AAAI Conference on Artiﬁcial\\nIntelligence, vol. 33, no. 01, pp. 1102–1109, Jul. 2019.\\n[111] R. Stiefelhagen, C. Fugen, R. Gieselmann, H. Holzapfel, K. Nickel,\\nand A. Waibel, “Natural human-robot interaction using speech, head'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='Intelligence, vol. 33, no. 01, pp. 1102–1109, Jul. 2019.\\n[111] R. Stiefelhagen, C. Fugen, R. Gieselmann, H. Holzapfel, K. Nickel,\\nand A. Waibel, “Natural human-robot interaction using speech, head\\npose and gestures,” in 2004 IEEE/RSJ International Conference on\\nIntelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566) ,\\nvol. 3, Sep. 2004, pp. 2422–2427 vol.3.\\n[112] G. P. Finley, E. Edwards, W. Salloum, A. Robinson, N. Sadoughi,\\nN. Axtmann, M. Korenevsky, M. Brenndoerfer, M. Miller, and\\nD. Suendermann-Oeft, “Semi-supervised acoustic model retraining\\nfor medical ASR,” in Speech and Computer , ser. Lecture Notes in\\nComputer Science, A. Karpov, O. Jokisch, and R. Potapova, Eds.\\nCham: Springer International Publishing, 2018, pp. 177–187.\\n[113] J. Sas and T. Poreba, “Optimal acoustic model complexity selection in\\npolish medical speech recognition,” Journal of Medical Informatics &\\nTechnologies, vol. V ol. 17, 2011.\\n[114] J. M. Paulett and C. P. Langlotz, “Improving language models for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='polish medical speech recognition,” Journal of Medical Informatics &\\nTechnologies, vol. V ol. 17, 2011.\\n[114] J. M. Paulett and C. P. Langlotz, “Improving language models for\\nradiology speech recognition,” Journal of Biomedical Informatics ,\\nvol. 42, no. 1, pp. 53–58, Feb. 2009.\\n[115] C.-C. Chiu, A. Tripathi, K. Chou, C. Co, N. Jaitly, D. Jaunzeikare,\\nA. Kannan, P. Nguyen, H. Sak, A. Sankar, J. Tansuwan, N. Wan,\\nY . Wu, and X. Zhang, “Speech recognition for medical conversations,”\\narXiv:1711.07274 [cs, eess, stat] , Jun. 2018.\\n[116] E. Edwards, W. Salloum, G. P. Finley, J. Fone, G. Cardiff, M. Miller,\\nand D. Suendermann-Oeft, “Medical speech recognition: Reaching\\nparity with humans,” in Speech and Computer , ser. Lecture Notes\\nin Computer Science, A. Karpov, R. Potapova, and I. Mporas, Eds.\\nCham: Springer International Publishing, 2017, pp. 512–524.\\n[117] T. He, W. Zhao, and L. Xu, “DOP-tacotron: A fast chinese TTS system'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 12, 'page_label': '13', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='in Computer Science, A. Karpov, R. Potapova, and I. Mporas, Eds.\\nCham: Springer International Publishing, 2017, pp. 512–524.\\n[117] T. He, W. Zhao, and L. Xu, “DOP-tacotron: A fast chinese TTS system\\nwith local-based attention,” in 2020 Chinese Control And Decision\\nConference (CCDC), Aug. 2020, pp. 4345–4350.\\n[118] K. Sugiura, Y . Shiga, H. Kawai, T. Misu, and C. Hori, “Non-monologue\\nHMM-based speech synthesis for service robots: A cloud robotics\\napproach,” in 2014 IEEE International Conference on Robotics and\\nAutomation (ICRA), May 2014, pp. 2237–2242.\\n[119] H. Akbari, B. Khalighinejad, J. L. Herrero, A. D. Mehta, and N. Mes-\\ngarani, “Towards reconstructing intelligible speech from the human\\nauditory cortex,” Scientiﬁc Reports, vol. 9, no. 1, p. 874, Jan. 2019.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='14 IEEE REVIEWS IN BIOMEDICAL ENGINEERING\\n[120] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, “Speech synthesis\\nfrom neural decoding of spoken sentences,” Nature, vol. 568, no. 7753,\\npp. 493–498, Apr. 2019.\\n[121] C. Herff, L. Diener, M. Angrick, E. Mugler, M. C. Tate, M. A. Goldrick,\\nD. J. Krusienski, M. W. Slutzky, and T. Schultz, “Generating natural,\\nintelligible speech from brain activity in motor, premotor, and inferior\\nfrontal cortices,” Frontiers in Neuroscience, vol. 13, p. 1267, 2019.\\n[122] C. Jreige, R. Patel, and H. T. Bunnell, “V ocaliD: Personalizing text-\\nto-speech synthesis for individuals with severe speech impairment,” in\\nProceedings of the 11th International ACM SIGACCESS Conference\\non Computers and Accessibility, ser. Assets ’09. New York, NY , USA:\\nAssociation for Computing Machinery, Oct. 2009, pp. 259–260.\\n[123] M. Marge, C. Espy-Wilson, N. G. Ward, A. Alwan, Y . Artzi, M. Bansal,\\nG. Blankenship, J. Chai, H. Daum ´e, D. Dey, M. Harper, T. Howard,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='[123] M. Marge, C. Espy-Wilson, N. G. Ward, A. Alwan, Y . Artzi, M. Bansal,\\nG. Blankenship, J. Chai, H. Daum ´e, D. Dey, M. Harper, T. Howard,\\nC. Kennington, I. Kruijff-Korbayov ´a, D. Manocha, C. Matuszek,\\nR. Mead, R. Mooney, R. K. Moore, M. Ostendorf, H. Pon-Barry, A. I.\\nRudnicky, M. Scheutz, R. S. Amant, T. Sun, S. Tellex, D. Traum, and\\nZ. Yu, “Spoken language interaction with robots: Recommendations for\\nfuture research,” Computer Speech & Language , vol. 71, p. 101255,\\nJan. 2022.\\n[124] J. James, B. T. Balamurali, C. I. Watson, and B. MacDonald, “Empa-\\nthetic speech synthesis and testing for healthcare robots,” International\\nJournal of Social Robotics , Sep. 2020.\\n[125] X. Li, B. MacDonald, and C. I. Watson, “Expressive facial speech\\nsynthesis on a robotic platform,” in Proceedings of the 2009 IEEE/RSJ\\nInternational Conference on Intelligent Robots and Systems , ser.\\nIROS’09. St. Louis, MO, USA: IEEE Press, Oct. 2009, pp. 5009–\\n5014.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='International Conference on Intelligent Robots and Systems , ser.\\nIROS’09. St. Louis, MO, USA: IEEE Press, Oct. 2009, pp. 5009–\\n5014.\\n[126] S. Roehling, B. Macdonald, and C. Watson, “Towards expressive\\nspeech synthesis in english on a robotic platform,” in In Proceedings\\nof the Australasian International Conference on Speech Science and\\nTechnology, 2006, pp. 130–135.\\n[127] K. K ¨uhne, M. H. Fischer, and Y . Zhou, “The human takes it all: Hu-\\nmanlike synthesized voices are perceived as less eerie and more likable.\\nevidence from a subjective ratings study,” Frontiers in Neurorobotics,\\nvol. 14, p. 105, 2020.\\n[128] F. Jiang, Y . Jiang, H. Zhi, Y . Dong, H. Li, S. Ma, Y . Wang, Q. Dong,\\nH. Shen, and Y . Wang, “Artiﬁcial intelligence in healthcare: Past,\\npresent and future,” Stroke and Vascular Neurology, vol. 2, no. 4, Dec.\\n2017.\\n[129] K. N. Dew, A. M. Turner, Y . K. Choi, A. Bosold, and K. Kirchhoff,\\n“Development of machine translation technology for assisting health'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='2017.\\n[129] K. N. Dew, A. M. Turner, Y . K. Choi, A. Bosold, and K. Kirchhoff,\\n“Development of machine translation technology for assisting health\\ncommunication: A systematic review,”Journal of Biomedical Informat-\\nics, vol. 85, pp. 56–67, Sep. 2018.\\n[130] G. Randhawa, M. Ferreyra, R. Ahmed, O. Ezzat, and K. Pottie, “Using\\nmachine translation in clinical practice,” Canadian Family Physician ,\\nvol. 59, no. 4, pp. 382–383, Apr. 2013.\\n[131] F. Goss, S. Blackley, C. Ortega, L. Kowalski, A. Landman, C. Lin,\\nM. Meteer, S. Bakes, S. Gradwohl, D. Bates, and Z. Li, “A clinician\\nsurvey of using speech recognition for clinical documentation in the\\nelectronic health record,” International Journal of Medical Informatics,\\nvol. 130, Jul. 2019.\\n[132] K. Saxena, R. Diamond, R. F. Conant, T. H. Mitchell, i. G. Gallopyn,\\nand K. E. Yakimow, “Provider adoption of speech recognition and its\\nimpact on satisfaction, documentation quality, efﬁciency, and cost in an'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='and K. E. Yakimow, “Provider adoption of speech recognition and its\\nimpact on satisfaction, documentation quality, efﬁciency, and cost in an\\ninpatient EHR,” AMIA Summits on Translational Science Proceedings ,\\nvol. 2018, pp. 186–195, May 2018.\\n[133] Y . Zhao, “Speech-recognition technology in health care and special-\\nneeds assistance [life sciences],” IEEE Signal Processing Magazine ,\\nvol. 26, no. 3, pp. 87–90, May 2009.\\n[134] T. R. Goodwin and S. M. Harabagiu, “Medical question answering for\\nclinical decision support,” Proceedings of the ... ACM International\\nConference on Information & Knowledge Management. ACM Interna-\\ntional Conference on Information and Knowledge Management , vol.\\n2016, pp. 297–306, Oct. 2016.\\n[135] G. Xu, W. Rong, Y . Wang, Y . Ouyang, and Z. Xiong, “External\\nfeatures enriched model for biomedical question answering,” BMC\\nBioinformatics, vol. 22, no. 1, p. 272, May 2021.\\n[136] X. Shi, D. Jiang, Y . Huang, X. Wang, Q. Chen, J. Yan, and B. Tang,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='features enriched model for biomedical question answering,” BMC\\nBioinformatics, vol. 22, no. 1, p. 272, May 2021.\\n[136] X. Shi, D. Jiang, Y . Huang, X. Wang, Q. Chen, J. Yan, and B. Tang,\\n“Family history information extraction via deep joint learning,” BMC\\nMedical Informatics and Decision Making , vol. 19, no. Suppl 10, Dec.\\n2019.\\n[137] A. Gupta, I. Banerjee, and D. L. Rubin, “Automatic information\\nextraction from unstructured mammography reports using distributed\\nsemantics,” Journal of Biomedical Informatics, vol. 78, pp. 78–86, Feb.\\n2018.\\n[138] J. Yang, Y . Liu, M. Qian, C. Guan, and X. Yuan, “Information\\nextraction from electronic medical records using multitask recurrent\\nneural network with contextual word embedding,” Applied Sciences ,\\nvol. 9, no. 18, p. 3658, Jan. 2019.\\n[139] S. Zheng, S. K. Jabbour, S. E. O’Reilly, J. J. Lu, L. Dong, L. Ding,\\nY . Xiao, N. Yue, F. Wang, and W. Zou, “Automated information\\nextraction on treatment and prognosis for Non–Small cell lung can-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='Y . Xiao, N. Yue, F. Wang, and W. Zou, “Automated information\\nextraction on treatment and prognosis for Non–Small cell lung can-\\ncer radiotherapy patients: Clinical study,” JMIR Medical Informatics ,\\nvol. 6, no. 1, p. e8, Feb. 2018.\\n[140] H. Liang, B. Y . Tsui, H. Ni, C. C. S. Valentim, S. L. Baxter, G. Liu,\\nW. Cai, D. S. Kermany, X. Sun, J. Chen, L. He, J. Zhu, P. Tian, H. Shao,\\nL. Zheng, R. Hou, S. Hewett, G. Li, P. Liang, X. Zang, Z. Zhang,\\nL. Pan, H. Cai, R. Ling, S. Li, Y . Cui, S. Tang, H. Ye, X. Huang,\\nW. He, W. Liang, Q. Zhang, J. Jiang, W. Yu, J. Gao, W. Ou, Y . Deng,\\nQ. Hou, B. Wang, C. Yao, Y . Liang, S. Zhang, Y . Duan, R. Zhang,\\nS. Gibson, C. L. Zhang, O. Li, E. D. Zhang, G. Karin, N. Nguyen,\\nX. Wu, C. Wen, J. Xu, W. Xu, B. Wang, W. Wang, J. Li, B. Pizzato,\\nC. Bao, D. Xiang, W. He, S. He, Y . Zhou, W. Haw, M. Goldbaum,\\nA. Tremoulet, C.-N. Hsu, H. Carter, L. Zhu, K. Zhang, and H. Xia,\\n“Evaluation and accurate diagnoses of pediatric diseases using artiﬁcial'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='A. Tremoulet, C.-N. Hsu, H. Carter, L. Zhu, K. Zhang, and H. Xia,\\n“Evaluation and accurate diagnoses of pediatric diseases using artiﬁcial\\nintelligence,” Nature Medicine, vol. 25, no. 3, pp. 433–438, Mar. 2019.\\n[141] H. Harkema, W. W. Chapman, M. Saul, E. S. Dellon, R. E. Schoen, and\\nA. Mehrotra, “Developing a natural language processing application\\nfor measuring the quality of colonoscopy procedures,” Journal of the\\nAmerican Medical Informatics Association: JAMIA , vol. 18 Suppl 1,\\npp. i150–156, Dec. 2011.\\n[142] A. Mehrotra, E. S. Dellon, R. E. Schoen, M. Saul, F. Bishehsari,\\nC. Farmer, and H. Harkema, “Applying a natural language processing\\ntool to electronic health records to assess performance on colonoscopy\\nquality measures,” Gastrointestinal Endoscopy , vol. 75, no. 6, pp.\\n1233–1239.e14, Jun. 2012.\\n[143] S. Wunnava, X. Qin, T. Kakar, C. Sen, E. A. Rundensteiner, and\\nX. Kong, “Adverse drug event detection from electronic health records'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='1233–1239.e14, Jun. 2012.\\n[143] S. Wunnava, X. Qin, T. Kakar, C. Sen, E. A. Rundensteiner, and\\nX. Kong, “Adverse drug event detection from electronic health records\\nusing hierarchical recurrent neural networks with dual-level embed-\\nding,” Drug Safety, vol. 42, no. 1, pp. 113–122, Jan. 2019.\\n[144] R. G. Jackson, R. Patel, N. Jayatilleke, A. Kolliakou, M. Ball,\\nG. Gorrell, A. Roberts, R. J. Dobson, and R. Stewart, “Natural\\nlanguage processing to extract symptoms of severe mental illness from\\nclinical text: The clinical record interactive search comprehensive data\\nextraction (CRIS-CODE) project,”BMJ Open, vol. 7, no. 1, p. e012012,\\nJan. 2017.\\n[145] J. Luo, L. Lan, D. Yang, S. Huang, M. Li, J. Yin, J. Xiao, and X. Zhou,\\n“Early prediction of organ failures in patients with acute pancreatitis\\nusing text mining,” Scientiﬁc Programming, vol. 2021, p. e6683942,\\nMay 2021.\\n[146] X. Wang, X. Xu, W. Tong, R. Roberts, and Z. Liu, “InferBERT: A'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='using text mining,” Scientiﬁc Programming, vol. 2021, p. e6683942,\\nMay 2021.\\n[146] X. Wang, X. Xu, W. Tong, R. Roberts, and Z. Liu, “InferBERT: A\\ntransformer-based causal inference framework for enhancing pharma-\\ncovigilance,” Frontiers in Artiﬁcial Intelligence , vol. 4, p. 67, 2021.\\n[147] S. V . Wang, O. V . Patterson, J. J. Gagne, J. S. Brown, R. Ball,\\nP. Jonsson, A. Wright, L. Zhou, W. Goettsch, and A. Bate, “Transparent\\nreporting on research using unstructured electronic health record data to\\ngenerate ‘real world’ evidence of comparative effectiveness and safety,”\\nDrug Safety, vol. 42, no. 11, pp. 1297–1309, Nov. 2019.\\n[148] A. Lee, B. E. Alving, M. B. Horup, and L. Thrysoee, “Information\\nretrieval as a part of evidence-based practice: Retrieval skills, behavior\\nand needs among nurses at a large university hospital:,” Nordic Journal\\nof Nursing Research , Aug. 2019.\\n[149] T. B. Patrick, G. Demiris, L. C. Folk, D. E. Moxley, J. A. Mitchell, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='and needs among nurses at a large university hospital:,” Nordic Journal\\nof Nursing Research , Aug. 2019.\\n[149] T. B. Patrick, G. Demiris, L. C. Folk, D. E. Moxley, J. A. Mitchell, and\\nD. Tao, “Evidence-based retrieval in evidence-based medicine,”Journal\\nof the Medical Library Association , vol. 92, no. 2, pp. 196–199, Apr.\\n2004.\\n[150] N. Ford, D. Miller, A. Booth, A. O’rourke, J. Ralph, and E. Turnock,\\n“Information retrieval for evidence-based decision making,” JOURNAL\\nOF DOCUMENTATION, vol. 55, Oct. 1999.\\n[151] N. W. Sterling, R. E. Patzer, M. Di, and J. D. Schrager, “Prediction\\nof emergency department patient disposition based on natural language\\nprocessing of triage notes,” International Journal of Medical Informat-\\nics, vol. 129, pp. 184–188, Sep. 2019.\\n[152] B. Tahayori, N. Chini-Foroush, and H. Akhlaghi, “Advanced natural\\nlanguage processing technique to predict patient disposition based on\\nemergency triage notes,” Emergency Medicine Australasia , vol. 33,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 13, 'page_label': '14', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='language processing technique to predict patient disposition based on\\nemergency triage notes,” Emergency Medicine Australasia , vol. 33,\\nno. 3, pp. 480–484, 2021.\\n[153] E. Sezgin, Y . Huang, U. Ramtekkar, and S. Lin, “Readiness for voice\\nassistants to support healthcare delivery during a health crisis and\\npandemic,” npj Digital Medicine , vol. 3, no. 1, pp. 1–4, Sep. 2020.\\n[154] S. Sp ¨anig, A. Emberger-Klein, J.-P. Sowa, A. Canbay, K. Menrad, and\\nD. Heider, “The virtual doctor: An interactive artiﬁcial intelligence'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ZHOU et al.: NATURAL LANGUAGE PROCESSING FOR SMART HEALTHCARE 15\\nbased on deep learning for non-invasive prediction of diabetes,” Arti-\\nﬁcial Intelligence in Medicine , vol. 100, p. 101706, Sep. 2019.\\n[155] M. Gandhi, V . K. Singh, and V . Kumar, “IntelliDoctor - AI based\\nmedical assistant,” in 2019 Fifth International Conference on Science\\nTechnology Engineering and Mathematics (ICONSTEM) , vol. 1, Mar.\\n2019, pp. 162–168.\\n[156] E. I. Agustin, R. T. Yunardi, and A. A. Firdaus, “V oice recognition\\nsystem for controlling electrical appliances in smart hospital room,”\\nTELKOMNIKA (Telecommunication Computing Electronics and Con-\\ntrol), vol. 17, no. 2, pp. 965–972, Apr. 2019.\\n[157] A. Ismail, S. Abdlerazek, and I. M. El-Henawy, “Development of smart\\nhealthcare system based on speech recognition using support vector\\nmachine and dynamic time warping,” Sustainability, vol. 12, no. 6, p.\\n2403, Jan. 2020.\\n[158] L. Grasse, S. J. Boutros, and M. S. Tata, “Speech interaction to control a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='machine and dynamic time warping,” Sustainability, vol. 12, no. 6, p.\\n2403, Jan. 2020.\\n[158] L. Grasse, S. J. Boutros, and M. S. Tata, “Speech interaction to control a\\nhands-free delivery robot for high-risk health care scenarios,” Frontiers\\nin Robotics and AI , vol. 8, p. 40, 2021.\\n[159] J. Holland, L. Kingston, C. McCarthy, E. Armstrong, P. O’Dwyer,\\nF. Merz, and M. McConnell, “Service robots in the healthcare sector,”\\nRobotics, vol. 10, no. 1, p. 47, Mar. 2021.\\n[160] C. M. Lineback, R. Garg, E. Oh, A. M. Naidech, J. L. Holl, and\\nS. Prabhakaran, “Prediction of 30-day readmission after stroke using\\nmachine learning and natural language processing,” Frontiers in Neu-\\nrology, vol. 12, p. 1069, 2021.\\n[161] A. Rajkomar, E. Oren, K. Chen, A. M. Dai, N. Hajaj, M. Hardt, P. J.\\nLiu, X. Liu, J. Marcus, M. Sun, P. Sundberg, H. Yee, K. Zhang,\\nY . Zhang, G. Flores, G. E. Duggan, J. Irvine, Q. Le, K. Litsch,\\nA. Mossin, J. Tansuwan, D. Wang, J. Wexler, J. Wilson, D. Ludwig,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='Liu, X. Liu, J. Marcus, M. Sun, P. Sundberg, H. Yee, K. Zhang,\\nY . Zhang, G. Flores, G. E. Duggan, J. Irvine, Q. Le, K. Litsch,\\nA. Mossin, J. Tansuwan, D. Wang, J. Wexler, J. Wilson, D. Ludwig,\\nS. L. V olchenboum, K. Chou, M. Pearson, S. Madabushi, N. H. Shah,\\nA. J. Butte, M. D. Howell, C. Cui, G. S. Corrado, and J. Dean,\\n“Scalable and accurate deep learning with electronic health records,”\\nnpj Digital Medicine , vol. 1, no. 1, p. 18, Dec. 2018.\\n[162] A. Rumshisky, M. Ghassemi, T. Naumann, P. Szolovits, V . M. Cas-\\ntro, T. H. McCoy, and R. H. Perlis, “Predicting early psychiatric\\nreadmission with natural language processing of narrative discharge\\nsummaries,” Translational Psychiatry , vol. 6, no. 10, p. e921, Oct.\\n2016.\\n[163] O. Alfarghaly, R. Khaled, A. Elkorany, M. Helal, and A. Fahmy, “Au-\\ntomated radiology report generation using conditioned transformers,”\\nInformatics in Medicine Unlocked , vol. 24, p. 100557, Jan. 2021.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='tomated radiology report generation using conditioned transformers,”\\nInformatics in Medicine Unlocked , vol. 24, p. 100557, Jan. 2021.\\n[164] B. Chintagunta, N. Katariya, X. Amatriain, and A. Kannan, “Medically\\naware GPT-3 as a data generator for medical dialogue summarization,”\\nin Proceedings of the Second Workshop on Natural Language Process-\\ning for Medical Conversations. Online: Association for Computational\\nLinguistics, Jun. 2021, pp. 66–76.\\n[165] A. M. Ibrahim, “Ontology-driven information retrieval for healthcare\\ninformation system : A case study,” International Journal of Network\\nSecurity & Its Applications , vol. 5, no. 1, pp. 61–69, Jan. 2013.\\n[166] M. Khanbhai, P. Anyadi, J. Symons, K. Flott, A. Darzi, and E. Mayer,\\n“Applying natural language processing and machine learning tech-\\nniques to patient experience feedback: A systematic review,” BMJ\\nHealth & Care Informatics , vol. 28, no. 1, p. e100262, Mar. 2021.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='“Applying natural language processing and machine learning tech-\\nniques to patient experience feedback: A systematic review,” BMJ\\nHealth & Care Informatics , vol. 28, no. 1, p. e100262, Mar. 2021.\\n[167] K. Nawab, G. Ramsey, and R. Schreiber, “Natural language processing\\nto extract meaningful information from patient experience feedback,”\\nApplied Clinical Informatics , vol. 11, no. 2, pp. 242–252, Mar. 2020.\\n[168] K. Doing-Harris, D. L. Mowery, C. Daniels, W. W. Chapman, and\\nM. Conway, “Understanding patient satisfaction with received health-\\ncare services: A natural language processing approach,” AMIA Annual\\nSymposium Proceedings, vol. 2016, pp. 524–533, Feb. 2017.\\n[169] D. Rodger, A. Skuse, M. Wilmore, S. Humphreys, J. Dalton,\\nM. Flabouris, V . L. Clifton, D. Rodger, A. Skuse, M. Wilmore,\\nS. Humphreys, J. Dalton, M. Flabouris, and V . L. Clifton, “Preg-\\nnant women’s use of information and communications technologies\\nto access pregnancy-related health information in South Australia,”'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='S. Humphreys, J. Dalton, M. Flabouris, and V . L. Clifton, “Preg-\\nnant women’s use of information and communications technologies\\nto access pregnancy-related health information in South Australia,”\\nAustralian Journal of Primary Health , vol. 19, no. 4, pp. 308–312,\\nDec. 2013.\\n[170] B. Zhou, K. Wu, P. Lv, J. Wang, G. Chen, B. Ji, and S. Liu, “A\\nnew remote health-care system based on moving robot intended for\\nthe elderly at home,” Journal of Healthcare Engineering, vol. 2018, p.\\n4949863, Feb. 2018.\\n[171] P. J. Rani, J. Bakthakumar, B. P. Kumaar, U. P. Kumaar, and S. Kumar,\\n“V oice controlled home automation system using natural language\\nprocessing (NLP) and internet of things (IoT),” in 2017 Third Inter-\\nnational Conference on Science Technology Engineering Management\\n(ICONSTEM), Mar. 2017, pp. 368–373.\\n[172] A. Tapus, M. J. Mataric, and B. Scassellati, “Socially assistive\\nrobotics,” IEEE Robotics Automation Magazine , vol. 14, no. 1, pp.\\n35–42, Mar. 2007.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='(ICONSTEM), Mar. 2017, pp. 368–373.\\n[172] A. Tapus, M. J. Mataric, and B. Scassellati, “Socially assistive\\nrobotics,” IEEE Robotics Automation Magazine , vol. 14, no. 1, pp.\\n35–42, Mar. 2007.\\n[173] N. Mavridis, “A review of verbal and non-verbal human–robot interac-\\ntive communication,” Robotics and Autonomous Systems , vol. 63, pp.\\n22–35, Jan. 2015.\\n[174] J. R. Green, R. L. MacDonald, P.-P. Jiang, J. Cattiau, R. Heywood,\\nR. Cave, K. Seaver, M. A. Ladewig, J. Tobin, M. P. Brenner, P. C.\\nNelson, and K. Tomanek, “Automatic speech recognition of disordered\\nspeech: Personalized models outperforming human listeners on short\\nphrases,” in Interspeech 2021. ISCA, Aug. 2021, pp. 4778–4782.\\n[175] K. Hux, K. Knollman-Porter, J. Brown, and S. E. Wallace, “Compre-\\nhension of synthetic speech and digitized natural speech by adults with\\naphasia,”Journal of Communication Disorders, vol. 69, pp. 15–26, Sep.\\n2017.\\n[176] K. Hux, J. A. Brown, S. Wallace, K. Knollman-Porter, A. Saylor, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='aphasia,”Journal of Communication Disorders, vol. 69, pp. 15–26, Sep.\\n2017.\\n[176] K. Hux, J. A. Brown, S. Wallace, K. Knollman-Porter, A. Saylor, and\\nE. Lapp, “Effect of text-to-speech rate on reading comprehension by\\nadults with aphasia,” American Journal of Speech-Language Pathology,\\nvol. 29, no. 1, pp. 168–184, Jul. 2020.\\n[177] S. Cassidy, B. Stenger, L. Van Dongen, K. Yanagisawa, R. Anderson,\\nV . Wan, S. Baron-Cohen, and R. Cipolla, “Expressive visual text-to-\\nspeech as an assistive technology for individuals with autism spectrum\\nconditions,” Computer Vision and Image Understanding , vol. 148, pp.\\n193–200, Jul. 2016.\\n[178] B. Repova, M. Zabrodsky, J. Plzak, D. Kalfert, J. Matousek, and\\nJ. Betka, “Text-to-speech synthesis as an alternative communication\\nmeans after total laryngectomy,” Biomedical Papers of the Medical\\nFaculty of the University Palacky, Olomouc, Czechoslovakia, Apr. 2020.\\n[179] F. C. Lyall, P. J. Clamp, and D. Hajioff, “Smartphone speech-to-text'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='Faculty of the University Palacky, Olomouc, Czechoslovakia, Apr. 2020.\\n[179] F. C. Lyall, P. J. Clamp, and D. Hajioff, “Smartphone speech-to-text\\napplications for communication with profoundly deaf patients,” The\\nJournal of Laryngology and Otology , vol. 130, no. 1, pp. 104–106,\\nJan. 2016.\\n[180] S. Nittrouer, L. M. Krieg, and J. H. Lowenstein, “Speech recognition\\nin noise by children with and without dyslexia: How is it related to\\nreading?” Research in developmental disabilities , vol. 77, pp. 98–113,\\nJun. 2018.\\n[181] D. Ria ˜no, M. Peleg, and A. ten Teije, “Ten years of knowledge repre-\\nsentation for health care (2009–2018): Topics, trends, and challenges,”\\nArtiﬁcial Intelligence in Medicine , vol. 100, p. 101713, Sep. 2019.\\n[182] Q. Bao, L. Ni, and J. Liu, “HHH: An online medical chatbot system\\nbased on knowledge graph and hierarchical bi-directional attention,”\\nProceedings of the Australasian Computer Science Week Multiconfer-\\nence, pp. 1–10, Feb. 2020.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='based on knowledge graph and hierarchical bi-directional attention,”\\nProceedings of the Australasian Computer Science Week Multiconfer-\\nence, pp. 1–10, Feb. 2020.\\n[183] W. Xie, R. Ding, J. Yan, and Y . Qu, “A mobile-based question-\\nanswering and early warning system for assisting diabetes manage-\\nment,” Wireless Communications and Mobile Computing, vol. 2018, p.\\ne9163160, Jun. 2018.\\n[184] P. Peters, Y . Qian, and J. Ding, “Translating medical terminology and\\nbilingual terminography,” Lexicography: Journal of ASIALEX , vol. 3,\\nno. 2, pp. 99–113, 2016.\\n[185] A. Renato, J. Casta ˜no, M. d. P. A. Williams, H. Berinsky, M. L.\\nGambarte, H. Park, D. P ´erez-Rey, C. Otero, and D. Luna, “A machine\\ntranslation approach for medical terms,” in HEALTHINF, 2018.\\n[186] J. Leo, G. Kurdi, N. Matentzoglu, B. Parsia, U. Sattler, S. Forge,\\nG. Donato, and W. Dowling, “Ontology-based generation of medical,\\nmulti-term MCQs,” International Journal of Artiﬁcial Intelligence in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='G. Donato, and W. Dowling, “Ontology-based generation of medical,\\nmulti-term MCQs,” International Journal of Artiﬁcial Intelligence in\\nEducation, vol. 29, no. 2, pp. 145–188, May 2019.\\n[187] NHS, “Nhs population screening explained,” nhs population screening\\nexplained, Feb 2013. [Online]. Available: https://www.gov.uk/guidance/\\nnhs-population-screening-explained\\n[188] P. M, G. M, Newton-DameRemle, T. E, P. E, M. H, and G. N, “The\\nstate of population health surveillance using electronic health records:\\nA narrative review,” Population Health Management, Jun. 2015.\\n[189] D. Georgiou, A. MacFarlane, and T. Russell-Rose, “Extracting senti-\\nment from healthcare survey data: An evaluation of sentiment analysis\\ntools,” in 2015 Science and Information Conference (SAI) , Jul. 2015,\\npp. 352–361.\\n[190] H. ¨Ozt¨urk, A. ¨Ozg¨ur, P. Schwaller, T. Laino, and E. Ozkirimli, “Explor-\\ning chemical space using natural language processing methodologies'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 14, 'page_label': '15', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='pp. 352–361.\\n[190] H. ¨Ozt¨urk, A. ¨Ozg¨ur, P. Schwaller, T. Laino, and E. Ozkirimli, “Explor-\\ning chemical space using natural language processing methodologies\\nfor drug discovery,”Drug Discovery Today, vol. 25, no. 4, pp. 689–705,\\nApr. 2020.\\n[191] F. Lake, “Artiﬁcial intelligence in drug discovery: What is new, and\\nwhat is next?” Future Drug Discovery , vol. 1, no. 2, p. FDD19, Oct.\\n2019.\\n[192] T.-H. Pham, Y . Qiu, J. Zeng, L. Xie, and P. Zhang, “A deep learning\\nframework for high-throughput mechanism-driven phenotype com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='16 IEEE REVIEWS IN BIOMEDICAL ENGINEERING\\npound screening and its application to COVID-19 drug repurposing,”\\nNature Machine Intelligence , vol. 3, no. 3, pp. 247–257, Mar. 2021.\\n[193] J. Schwartz, M. Awale, and J.-L. Reymond, “SMIfp (SMILES ﬁnger-\\nprint) chemical space for virtual screening and visualization of large\\ndatabases of organic molecules,” Journal of Chemical Information and\\nModeling, vol. 53, no. 8, pp. 1979–1989, Aug. 2013.\\n[194] F. Zhang, B. Sun, X. Diao, W. Zhao, and T. Shu, “Prediction of adverse\\ndrug reactions based on knowledge graph embedding,” BMC Medical\\nInformatics and Decision Making , vol. 21, no. 1, p. 38, Feb. 2021.\\n[195] K. Bouhedjar, A. Boukelia, A. K. Nacereddine, A. Boucheham, A. Be-\\nlaidi, and A. Djerourou, “A natural language processing approach\\nbased on embedding deep learning from heterogeneous compounds\\nfor quantitative structure–activity relationship modeling,” Chemical\\nBiology & Drug Design , vol. 96, no. 3, pp. 961–972, 2020.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='based on embedding deep learning from heterogeneous compounds\\nfor quantitative structure–activity relationship modeling,” Chemical\\nBiology & Drug Design , vol. 96, no. 3, pp. 961–972, 2020.\\n[196] W. Jeon and D. Kim, “FP2VEC: A new molecular featurizer for\\nlearning molecular properties,” Bioinformatics, vol. 35, no. 23, pp.\\n4979–4985, Dec. 2019.\\n[197] L. Chen, Y . Gu, X. Ji, C. Lou, Z. Sun, H. Li, Y . Gao, and Y . Huang,\\n“Clinical trial cohort selection based on multi-level rule-based natural\\nlanguage processing system,” Journal of the American Medical Infor-\\nmatics Association : JAMIA, vol. 26, no. 11, pp. 1218–1226, Jul. 2019.\\n[198] S. Harrer, P. Shah, B. Antony, and J. Hu, “Artiﬁcial intelligence for\\nclinical trial design,” Trends in Pharmacological Sciences , vol. 40,\\nno. 8, pp. 577–591, Aug. 2019.\\n[199] H. Tissot, F. Asselbergs, A. Shah, D. Brealey, S. Harris, R. Agbakoba,\\nA. Folarin, L. Romao, L. Roguski, and R. Dobson, “Natural language'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='no. 8, pp. 577–591, Aug. 2019.\\n[199] H. Tissot, F. Asselbergs, A. Shah, D. Brealey, S. Harris, R. Agbakoba,\\nA. Folarin, L. Romao, L. Roguski, and R. Dobson, “Natural language\\nprocessing for mimicking clinical trial recruitment in critical care:\\nA semi-automated simulation based on the LeoPARDS trial,” IEEE\\nJournal of Biomedical and Health Informatics , vol. PP, pp. 1–1, Mar.\\n2020.\\n[200] X. Chen, H. Xie, G. Cheng, L. K. M. Poon, M. Leng, and F. L. Wang,\\n“Trends and features of the applications of natural language processing\\ntechniques for clinical trials text analysis,” Applied Sciences , vol. 10,\\nno. 6, p. 2157, Jan. 2020.\\n[201] C. L. Ventola, “Big data and pharmacovigilance: Data mining for\\nadverse drug events and interactions,” Pharmacy and Therapeutics ,\\nvol. 43, no. 6, pp. 340–351, Jun. 2018.\\n[202] X. Wang, G. Hripcsak, M. Markatou, and C. Friedman, “Active\\ncomputerized pharmacovigilance using natural language processing,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='vol. 43, no. 6, pp. 340–351, Jun. 2018.\\n[202] X. Wang, G. Hripcsak, M. Markatou, and C. Friedman, “Active\\ncomputerized pharmacovigilance using natural language processing,\\nstatistics, and electronic health records: A feasibility study,” Journal\\nof the American Medical Informatics Association : JAMIA , vol. 16,\\nno. 3, pp. 328–337, 2009.\\n[203] F. Liu, A. Jagannatha, and H. Yu, “Towards drug safety surveillance\\nand pharmacovigilance: Current progress in detecting medication and\\nadverse drug events from electronic health records,” Drug Safety ,\\nvol. 42, no. 1, pp. 95–97, Jan. 2019.\\n[204] B. Zhou, G. Yang, Z. Shi, and S. Ma, “Interpretable Temporal Attention\\nNetwork for COVID-19 forecasting,”Applied Soft Computing, vol. 120,\\np. 108691, May 2022.\\n[205] N. Zheng, S. Du, J. Wang, H. Zhang, W. Cui, Z. Kang, T. Yang, B. Lou,\\nY . Chi, H. Long, M. Ma, Q. Yuan, S. Zhang, D. Zhang, F. Ye, and\\nJ. Xin, “Predicting COVID-19 in china using hybrid AI model,” IEEE'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='Y . Chi, H. Long, M. Ma, Q. Yuan, S. Zhang, D. Zhang, F. Ye, and\\nJ. Xin, “Predicting COVID-19 in china using hybrid AI model,” IEEE\\nTransactions on Cybernetics, vol. 50, no. 7, pp. 2891–2904, Jul. 2020.\\n[206] Q. Chen, R. Leaman, A. Allot, L. Luo, C.-H. Wei, S. Yan, and Z. Lu,\\n“Artiﬁcial intelligence in action: Addressing the COVID-19 pandemic\\nwith natural language processing,” Annual Review of Biomedical Data\\nScience, vol. 4, no. 1, pp. 313–339, 2021.\\n[207] A. Chapman, K. Peterson, A. Turano, T. Box, K. Wallace, and\\nM. Jones, “A natural language processing system for national COVID-\\n19 surveillance in the US department of veterans affairs,” in Proceed-\\nings of the 1st Workshop on NLP for COVID-19 at ACL 2020. Online:\\nAssociation for Computational Linguistics, 2020.\\n[208] R. C. Cury, I. Megyeri, T. Lindsey, R. Macedo, J. Batlle, S. Kim,\\nB. Baker, R. Harris, and R. H. Clark, “Natural language processing\\nand machine learning for detection of respiratory illness by chest CT'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='B. Baker, R. Harris, and R. H. Clark, “Natural language processing\\nand machine learning for detection of respiratory illness by chest CT\\nimaging and tracking of COVID-19 pandemic in the united states,”\\nRadiology: Cardiothoracic Imaging , vol. 3, no. 1, p. e200596, Feb.\\n2021.\\n[209] D. DeCaprio, J. Gartner, C. J. McCall, T. Burgess, K. Garcia,\\nS. Kothari, and S. Sayed, “Building a COVID-19 vulnerability index,”\\nJournal of Medical Artiﬁcial Intelligence , vol. 3, no. 0, Dec. 2020.\\n[210] S. M. Meystre, P. M. Heider, Y . Kim, M. Davis, J. Obeid, J. Madory,\\nand A. V . Alekseyenko, “Natural language processing enabling\\nCOVID-19 predictive analytics to support data-driven patient advising\\nand pooled testing,” Journal of the American Medical Informatics\\nAssociation: JAMIA, vol. 29, no. 1, pp. 12–21, Dec. 2021.\\n[211] L. Wang, L. Jiang, D. Pan, Q. Wang, Z. Yin, Z. Kang, H. Tian, X. Geng,\\nJ. Shao, W. Pan, J. Yin, L. Fang, Y . Wang, W. Zhang, Z. Li, J. Zheng,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='[211] L. Wang, L. Jiang, D. Pan, Q. Wang, Z. Yin, Z. Kang, H. Tian, X. Geng,\\nJ. Shao, W. Pan, J. Yin, L. Fang, Y . Wang, W. Zhang, Z. Li, J. Zheng,\\nW. Hu, Y . Pan, D. Yu, S. Guo, W. Lu, Q. Li, Y . Zhou, and H. Xu, “Novel\\napproach by natural language processing for COVID-19 knowledge\\ndiscovery,” Biomedical Journal, Apr. 2022.\\n[212] A. Keshavarzi Arshadi, J. Webb, M. Salem, E. Cruz, S. Calad-\\nThomson, N. Ghadirian, J. Collins, E. Diez-Cecilia, B. Kelly,\\nH. Goodarzi, and J. S. Yuan, “Artiﬁcial Intelligence for COVID-19\\nDrug Discovery and Vaccine Development,” Frontiers in Artiﬁcial\\nIntelligence, vol. 3, 2020.\\n[213] Z. Liu, R. A. Roberts, M. Lal-Nag, X. Chen, R. Huang, and W. Tong,\\n“AI-based language models powering drug discovery and develop-\\nment,” Drug Discovery Today , vol. 26, no. 11, pp. 2593–2607, Nov.\\n2021.\\n[214] WHO, “10 global health issues to track in 2021,”\\nhttps://www.who.int/news-room/spotlight/10-global-health-issues-\\nto-track-in-2021, 2020.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='2021.\\n[214] WHO, “10 global health issues to track in 2021,”\\nhttps://www.who.int/news-room/spotlight/10-global-health-issues-\\nto-track-in-2021, 2020.\\n[215] H.-J. Dai, C.-H. Su, Y .-Q. Lee, Y .-C. Zhang, C.-K. Wang, C.-J. Kuo,\\nand C.-S. Wu, “Deep learning-based natural language processing for\\nscreening psychiatric patients,” Frontiers in Psychiatry, vol. 11, 2021.\\n[216] D. D. DeSouza, J. Robin, M. Gumus, and A. Yeung, “Natural language\\nprocessing as an emerging tool to detect late-life depression,” Frontiers\\nin Psychiatry, vol. 12, 2021.\\n[217] R. G. Jackson, R. Patel, N. Jayatilleke, A. Kolliakou, M. Ball,\\nG. Gorrell, A. Roberts, R. J. Dobson, and R. Stewart, “Natural\\nlanguage processing to extract symptoms of severe mental illness from\\nclinical text: The clinical record interactive search comprehensive data\\nextraction (CRIS-CODE) project,”BMJ Open, vol. 7, no. 1, p. e012012,\\nJan. 2017.\\n[218] J. Cohen, J. Wright-Berryman, L. Rohlfs, D. Trocinski, L. Daniel, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='extraction (CRIS-CODE) project,”BMJ Open, vol. 7, no. 1, p. e012012,\\nJan. 2017.\\n[218] J. Cohen, J. Wright-Berryman, L. Rohlfs, D. Trocinski, L. Daniel, and\\nT. W. Klatt, “Integration and validation of a natural language processing\\nmachine learning suicide risk prediction model based on open-ended\\ninterview language in the emergency department,” Frontiers in Digital\\nHealth, vol. 4, 2022.\\n[219] D. Harvey, F. Lobban, P. Rayson, A. Warner, and S. Jones, “Natural\\nlanguage processing methods and bipolar disorder: Scoping review,”\\nJMIR Mental Health , vol. 9, no. 4, p. e35928, Apr. 2022.\\n[220] T. Zhang, A. M. Schoene, S. Ji, and S. Ananiadou, “Natural language\\nprocessing applied to mental illness detection: A narrative review,” npj\\nDigital Medicine, vol. 5, no. 1, pp. 1–13, Apr. 2022.\\n[221] G. Bedi, F. Carrillo, G. A. Cecchi, D. F. Slezak, M. Sigman, N. B. Mota,\\nS. Ribeiro, D. C. Javitt, M. Copelli, and C. M. Corcoran, “Automated'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='[221] G. Bedi, F. Carrillo, G. A. Cecchi, D. F. Slezak, M. Sigman, N. B. Mota,\\nS. Ribeiro, D. C. Javitt, M. Copelli, and C. M. Corcoran, “Automated\\nanalysis of free speech predicts psychosis onset in high-risk youths,”\\nnpj Schizophrenia, vol. 1, no. 1, pp. 1–7, Aug. 2015.\\n[222] R. A. Calvo, D. N. Milne, M. S. Hussain, and H. Christensen, “Natural\\nlanguage processing in mental health applications using non-clinical\\ntexts†,” Natural Language Engineering , vol. 23, no. 5, pp. 649–685,\\nSep. 2017.\\n[223] T. Althoff, K. Clark, and J. Leskovec, “Large-scale analysis of coun-\\nseling conversations: An application of natural language processing\\nto mental health,” Transactions of the Association for Computational\\nLinguistics, vol. 4, pp. 463–476, 2016.\\n[224] V . Chattaraman, W.-S. Kwon, J. E. Gilbert, and K. Ross, “Should AI-\\nBased, conversational digital assistants employ social- or task-oriented\\ninteraction style? A task-competency and reciprocity perspective for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='Based, conversational digital assistants employ social- or task-oriented\\ninteraction style? A task-competency and reciprocity perspective for\\nolder adults,” Computers in Human Behavior , vol. 90, pp. 315–330,\\nJan. 2019.\\n[225] A. Amin-Nejad, J. Ive, and S. Velupillai, “Exploring transformer text\\ngeneration for medical dataset augmentation,” in Proceedings of the\\n12th Language Resources and Evaluation Conference . Marseille,\\nFrance: European Language Resources Association, May 2020, pp.\\n4699–4708.\\n[226] J. Ive, N. Viani, J. Kam, L. Yin, S. Verma, S. Puntis, R. N. Cardinal,\\nA. Roberts, R. Stewart, and S. Velupillai, “Generation and evaluation\\nof artiﬁcial mental health records for natural language processing,” npj\\nDigital Medicine, vol. 3, no. 1, pp. 1–9, May 2020.\\n[227] X. Soto, O. Perez-de-Vi ˜naspre, G. Labaka, and M. Oronoz, “Neural\\nmachine translation of clinical texts between long distance languages,”\\nJournal of the American Medical Informatics Association , vol. 26,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 15, 'page_label': '16', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='machine translation of clinical texts between long distance languages,”\\nJournal of the American Medical Informatics Association , vol. 26,\\nno. 12, pp. 1478–1487, Dec. 2019.\\n[228] K. Wołk and K. Marasek, “Neural-based machine translation for\\nmedical text domain. based on european medicines agency leaﬂet\\ntexts,” Procedia Computer Science , vol. 64, pp. 2–9, Jan. 2015.\\n[229] K. Wolk and K. P. Marasek, “Translation of medical texts using neural\\nnetworks,” International Journal of Reliable and Quality E-Healthcare\\n(IJRQEH), vol. 5, no. 4, pp. 51–66, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 16, 'page_label': '17', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='ZHOU et al.: NATURAL LANGUAGE PROCESSING FOR SMART HEALTHCARE 17\\nBinggui Zhou received the B.Eng. degree from\\nJinan University, Zhuhai, China, in 2018, and the\\nM.Sc. degree from the University of Macau, Macao,\\nChina, in 2021, respectively. He is currently working\\ntoward the Ph.D. degree in Electrical and Computer\\nEngineering with the University of Macau, Macao,\\nChina. He also serves as a Research Assistant\\nwith the School of Intelligent Systems Science and\\nEngineering, Jinan University, Zhuhai, China. His\\nresearch interests include Natural Language Process-\\ning, Artiﬁcial Intelligence, and AI assisted Wireless\\nCommunications.\\nGuanghua Yang received his Ph.D. degree in elec-\\ntrical and electronic engineering from the University\\nof Hong Kong, Hong Kong, in 2006. From 2006\\nto 2013, he served as post-doctoral fellow, research\\nassociate at the University of Hong Kong. Since\\nApril 2017, he has been with Jinan University, where\\nhe is currently a Full Professor in the School of'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 16, 'page_label': '17', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='associate at the University of Hong Kong. Since\\nApril 2017, he has been with Jinan University, where\\nhe is currently a Full Professor in the School of\\nIntelligent Systems Science and Engineering. His\\nresearch interests are in the general areas of AI and\\nits applications.\\nZheng Shi received his B.S. degree in communi-\\ncation engineering from Anhui Normal University,\\nChina, in 2010 and his M.S. degree in communi-\\ncation and information system from Nanjing Uni-\\nversity of Posts and Telecommunications (NUPT),\\nChina, in 2013. He obtained his Ph.D. degree in\\nElectrical and Computer Engineering from Univer-\\nsity of Macau, Macao, in 2017. He is currently an\\nAssociate Professor with the School of Intelligent\\nSystems Science and Engineering, Jinan University,\\nZhuhai, China. His current research interests include\\nhybrid automatic repeat request, non-orthogonal multiple access, machine\\nlearning and Internet of Things.\\nShaodan Ma received the double Bachelor’s degrees'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-09-27T02:04:50+00:00', 'author': '', 'keywords': '', 'moddate': '2022-09-27T02:04:50+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\NLP research paper for Healthcare.pdf', 'total_pages': 17, 'page': 16, 'page_label': '17', 'source_file': 'NLP research paper for Healthcare.pdf', 'file_type': 'pdf'}, page_content='hybrid automatic repeat request, non-orthogonal multiple access, machine\\nlearning and Internet of Things.\\nShaodan Ma received the double Bachelor’s degrees\\nin science and economics and the M.Eng. degree\\nin electronic engineering from Nankai University,\\nTianjin, China, in 1999 and 2002, respectively, and\\nthe Ph.D. degree in electrical and electronic engi-\\nneering from The University of Hong Kong, Hong\\nKong, in 2006. From 2006 to 2011, she was a post-\\ndoctoral fellow at The University of Hong Kong.\\nSince August 2011, she has been with the University\\nof Macau, where she is currently a Professor. Her\\nresearch interests include array signal processing,\\nmachine learning, wireless sensing and mmwave communications.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='arXiv:2506.05176v3  [cs.CL]  11 Jun 2025\\nTechnical Report\\nQwen3 Embedding: Advancing Text Embedding and\\nReranking Through Foundation Models\\nYanzhao Zhang* Mingxin Li* Dingkun Long* Xin Zhang*\\nHuan Lin Baosong Yang Pengjun Xie An Yang\\nDayiheng Liu Junyang Lin Fei Huang Jingren Zhou\\nTongyi Lab Alibaba Group\\nhttps://huggingface.co/Qwen\\nhttps://modelscope.cn/organization/qwen\\nhttps://github.com/QwenLM/Qwen3-Embedding\\nAbstract\\nIn this work, we introduce the Qwen3 Embedding series, a significant advancement\\nover its predecessor, the GTE-Qwen series, in text embedding and reranking capabili-\\nties, built upon the Qwen3 foundation models. Leveraging the Qwen3 LLMs’ robust\\ncapabilities in multilingual text understanding and generation, our innovative multi-\\nstage training pipeline combines large-scale unsupervised pre-training with supervised\\nfine-tuning on high-quality datasets. Effective model merging strategies further ensure'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='stage training pipeline combines large-scale unsupervised pre-training with supervised\\nfine-tuning on high-quality datasets. Effective model merging strategies further ensure\\nthe robustness and adaptability of the Qwen3 Embedding series. During the training\\nprocess, the Qwen3 LLMs serve not only as backbone models but also play a crucial role\\nin synthesizing high-quality, rich, and diverse training data across multiple domains\\nand languages, thus enhancing the training pipeline. The Qwen3 Embedding series\\noffers a spectrum of model sizes (0.6B, 4B, 8B) for both embedding and reranking tasks,\\naddressing diverse deployment scenarios where users can optimize for either efficiency\\nor effectiveness. Empirical evaluations demonstrate that the Qwen3 Embedding series\\nachieves state-of-the-art results across diverse benchmarks. Notably, it excels on the\\nmultilingual evaluation benchmark MTEB for text embedding, as well as in various'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='achieves state-of-the-art results across diverse benchmarks. Notably, it excels on the\\nmultilingual evaluation benchmark MTEB for text embedding, as well as in various\\nretrieval tasks, including code retrieval, cross-lingual retrieval and multilingual retrieval.\\nTo facilitate reproducibility and promote community-driven research and development,\\nthe Qwen3 Embedding models are publicly available under the Apache 2.0 license.\\n1 Introduction\\nText embedding and reranking are fundamental components in numerous natural language pro-\\ncessing and information retrieval applications, including web search, question answering, recom-\\nmendation systems, and beyond (Karpukhin et al., 2020; Huang et al., 2020; Zhao et al., 2023; 2024).\\nHigh-quality embeddings enable models to capture semantic relationships between texts, while\\neffective reranking mechanisms ensure that the most relevant results are prioritized. Recently,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='High-quality embeddings enable models to capture semantic relationships between texts, while\\neffective reranking mechanisms ensure that the most relevant results are prioritized. Recently,\\nemerging application paradigms such as Retrieval-Augmented Generation (RAG) and agent sys-\\ntems, driven by the advancement of large language models (e.g., Qwen3 (Yang et al., 2025), GPT-4o\\n(Hurst et al., 2024)), have introduced new requirements and challenges for text embedding and\\nreranking, both in terms of model training paradigms and application scenarios. Despite significant\\nadvancements, training embedding and reranking models that perform well in scalability, contextual\\nunderstanding, and alignment with specific downstream tasks remains challenging.\\nThe emergence of large language models (LLMs) has significantly advanced the development of text\\nembedding and reranking models. Prior to the introduction of LLMs, the predominant approach\\n∗ Equal contribution\\n1'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\ninvolved using encoder-only pretrained language models like BERT as the foundational model\\nfor training (Reimers & Gurevych, 2019). The richer world knowledge, text understanding, and\\nreasoning abilities inherent in LLMs have led to further enhancements in models trained on these\\narchitectures. Additionally, there has been considerable research facilitating the integration of LLMs\\ninto processes such as training data synthesis and quality data filtering (Wang et al., 2024; Lee et al.,\\n2024; 2025b). The fundamental characteristics of LLMs have also inspired the introduction of new\\ntraining paradigms. For instance, during the embedding model training process, incorporating\\ndifferentiated tasks across aspects such as instruction type, domain, and language has yielded\\nimproved performance in downstream tasks (Su et al., 2023). Similarly, for reranking model training,\\nadvancements have been realized through both zero-shot methods based on user prompts and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='improved performance in downstream tasks (Su et al., 2023). Similarly, for reranking model training,\\nadvancements have been realized through both zero-shot methods based on user prompts and\\napproaches combining supervised fine-tuning (Ma et al., 2023; Pradeep et al., 2023; Zhang et al.,\\n2024a; Zhuang et al., 2024).\\nIn this work, we introduce the Qwen3 Embedding series models, which are constructed on top\\nof the Qwen3 foundation models. The Qwen3 foundation has simultaneously released base and\\ninstruct model versions, and we exploit the robust multilingual text understanding and generation\\ncapabilities of these models to fully realize their potential in training embedding and reranking\\nmodels. To train the embedding models, we implement a multi-stage training pipeline that involves\\nlarge-scale unsupervised pre-training followed by supervised fine tuning on high-quality datasets.\\nWe also employ model merging with various model checkpoints to enhance robustness and general-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='large-scale unsupervised pre-training followed by supervised fine tuning on high-quality datasets.\\nWe also employ model merging with various model checkpoints to enhance robustness and general-\\nization. The Qwen3 instruct model allows for efficient synthesis of a vast, high-quality, multilingual,\\nand multi-task text relevance dataset. This synthetic data is utilized in the initial unsupervised\\ntraining stage, while a subset of high-quality, small-scale data is selected for the second stage of\\nsupervised training. For the reranking models, we adopt a two-stage training scheme in a similar\\nmanner, consisting of high-quality supervised fine tuning and a model merging stage. Based on\\ndifferent sizes of the Qwen3 backbone models (including 0.6B, 4B, and 8B), we ultimately trained\\nthree text embedding models and three text reranking models. To facilitate their application in\\ndownstream tasks, the Qwen3 Embedding series supports several practical features, such as flexible'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='three text embedding models and three text reranking models. To facilitate their application in\\ndownstream tasks, the Qwen3 Embedding series supports several practical features, such as flexible\\ndimension representation for embedding models and customizable instructions for both embedding\\nand reranking models.\\nWe evaluate the Qwen3 Embedding series across a comprehensive set of benchmarks spanning\\nmultiple tasks and domains. Experimental results demonstrate that our embedding and reranking\\nmodels achieve state-of-the-art performance, performing competitively against leading proprietary\\nmodels in several retrieval tasks. For example, the flagship model Qwen3-8B-Embedding attains a\\nscore of 70.58 on the MTEB Multilingual benchmark (Enevoldsen et al., 2025) and 80.68 on the MTEB\\nCode benchmark (Enevoldsen et al., 2025), surpassing the previous state-of-the-art proprietary\\nembedding model, Gemini-Embedding (Lee et al., 2025b). Moreover, our reranking model delivers'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Code benchmark (Enevoldsen et al., 2025), surpassing the previous state-of-the-art proprietary\\nembedding model, Gemini-Embedding (Lee et al., 2025b). Moreover, our reranking model delivers\\ncompetitive results across a range of retrieval tasks. The Qwen3-Reranker-0.6B model exceeds\\npreviously top-performing models in numerous retrieval tasks, while the larger Qwen3-Reranker-8B\\nmodel demonstrates even superior performance, improving ranking results by 3.0 points over the\\n0.6B model across multiple tasks. Furthermore, we include a constructive ablation study to elucidate\\nthe key factors contributing to the superior performance of the Qwen3 Embedding series, providing\\ninsights into its effectiveness.\\nIn the following sections, we describe the design of the model architecture, detail the training\\nprocedures, present the experimental results for both the embedding and reranking models of the\\nQwen3 Embedding Series, and conclude this technical report by summarizing the key findings and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='procedures, present the experimental results for both the embedding and reranking models of the\\nQwen3 Embedding Series, and conclude this technical report by summarizing the key findings and\\noutlining potential directions for future research.\\n2 Model Architecture\\nThe core idea behind embedding and reranking models is to evaluate relevance in a task-aware\\nmanner. Given a query q and a document d, embedding and reranking models assess their relevance\\nbased on a similarity criterion defined by instruction I. To enable the models for task-aware rele-\\nvance estimation, training data is often organized as {Ii, qi, d+\\ni , d−\\ni,1, · · ·, d−\\ni,n}, where d+\\ni represents a\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nFigure 1: Model architecture of Qwen3-Embedding (left) and Qwen3-Reranker (right).\\npositive (relevant) document for query qi, and d−\\ni,j are negative (irrelevant) documents. Training the\\nmodel on diverse text pairs broadens its applicability to a range of downstream tasks, including\\nretrieval, semantic textual similarity, classification, and clustering.\\nArchitecture The Qwen3 embedding and reranking models are built on the dense version of\\nQwen3 foundation models and are available in three sizes: 0.6B, 4B, and 8B parameters. We initialize\\nthese models using the Qwen3 foundation models to leverage their capabilities in text modeling\\nand instruction following. The model layers, hidden size, and context length for each model\\nconfiguration are detailed in Table 1.\\nEmbedding Models For text embeddings, we utilize LLMs with causal attention, appending an\\n[EOS] token at the end of the input sequence. The final embedding is derived from the hidden state'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Embedding Models For text embeddings, we utilize LLMs with causal attention, appending an\\n[EOS] token at the end of the input sequence. The final embedding is derived from the hidden state\\nof the last layer corresponding to this [EOS] token.\\nTo ensure embeddings follow instructions during downstream tasks, we concatenate the instruction\\nand the query into a single input context, while leaving the document unchanged before processing\\nwith LLMs. The input format for queries is as follows:\\n{Instruction} {Query}<|endoftext|>\\nReranking Models To more accurately evaluate text similarity, we employ LLMs for point-wise\\nreranking within a single context. Similar to the embedding model, to enable instruction-following\\ncapability, we include the instruction in the input context. We use the LLM chat template and frame\\nthe similarity assessment task as a binary classification problem. The input to LLMs adheres to the\\ntemplate shown below:\\n<|im_start|>system'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='the similarity assessment task as a binary classification problem. The input to LLMs adheres to the\\ntemplate shown below:\\n<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the\\nInstruct provided. Note that the answer can only be \"yes\" or\\n\"no\".<|im_end|>\\n,→\\n,→\\n<|im_start|>user\\n<Instruct>: {Instruction}\\n<Query>: {Query}\\n<Document>: {Document}<|im_end|>\\n<|im_start|>assistant\\n<think>\\\\n\\\\n</think>\\\\n\\\\n\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nModel Type Models Size Layers Sequence\\nLength\\nEmbedding\\nDimension\\nMRL\\nSupport\\nInstruction\\nAware\\nText Embedding\\nQwen3-Embedding-0.6B0.6B 28 32K 1024 Yes Yes\\nQwen3-Embedding-4B 4B 36 32K 2560 Yes Yes\\nQwen3-Embedding-8B 8B 36 32K 4096 Yes Yes\\nText Reranking\\nQwen3-Reranker-0.6B 0.6B 28 32K - - Yes\\nQwen3-Reranker-4B 4B 36 32K - - Yes\\nQwen3-Reranker-8B 8B 36 32K - - Yes\\nTable 1: Model architecture of Qwen3 Embedding models. “MRL Support” indicates whether the\\nembedding model supports custom dimensions for the final embedding. “Instruction Aware” notes\\nwhether the embedding or reranker model supports customizing the input instruction according to\\ndifferent tasks.\\nFigure 2: Training pipeline of Qwen3 Embedding and Reranking models.\\nTo calculate the relevance score based on the given input, we assess the likelihood of the next token\\nbeing ”yes” or ”no.” This is expressed mathematically as follows:\\nscore(q, d) = eP(yes|I,q,d)\\neP(yes|I,q,d) + eP(no|I,q,d)\\n3 Models Training'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='being ”yes” or ”no.” This is expressed mathematically as follows:\\nscore(q, d) = eP(yes|I,q,d)\\neP(yes|I,q,d) + eP(no|I,q,d)\\n3 Models Training\\nIn this section, we describe the multi-stage training pipeline adopted and present the key elements of\\nthis training recipe, including training objective, training data synthesis, and filtering of high-quality\\ntraining data.\\n3.1 Training Objective\\nBefore introducing our training pipeline, we first outline the optimized loss functions used for the\\nembedding and reranking models during the training process. For the embedding model, we utilize\\nan improved contrastive loss based on the InfoNCE framework (Oord et al., 2018). Given a batch of\\nN training instances, the loss is defined as:\\nLembedding = − 1\\nN\\nN\\n∑\\ni\\nlog e(s(qi,d+\\ni )/τ)\\nZi\\n, (1)\\nwhere s(·, ·) is a similarity function (we use cosine similarity), τ is a temperature parameter, and Zi\\nis the normalization factor that aggregates the similarity scores of the positive pair against various'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='is the normalization factor that aggregates the similarity scores of the positive pair against various\\nnegative pairs:\\nZi = e(s(qi,d+\\ni )/τ) +\\nK\\n∑\\nk\\nmik e(s(qi,d−\\ni,k)/τ) + ∑\\nj̸=i\\nmij e(s(qi,qj)/τ) + ∑\\nj̸=i\\nmij e(s(d+\\ni ,dj)/τ) + ∑\\nj̸=i\\nmij e(s(qi,dj)/τ)\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nwhere these terms represent similarities with: (1) the positive document d+\\ni , (2) K hard negatives\\nd−\\ni,k, (3) other in-batch queries qj, (4) other in-batch documents dj compared against the positive\\ndocument d+\\ni . (5) other in-batch documents dj compared against the query qi. The mask factor mij is\\ndesigned to mitigate the impact of false negatives and is defined as:\\nmij =\\n(\\n0 if sij > s(qi, d+\\ni ) +0.1 or dj == d+\\ni ,\\n1 otherwise,\\namong which sij is the corresponding score of qi, dj or qi, qj.\\nFor the reranking model, we optimize the Supervised Fine-Tuning (SFT) loss defined as:\\nLreranking = −log p(l|P(q, d)), (2)\\nwhere p(·|∗) denotes the probability assigned by LLM. The label l is “yes” for positive documents\\nand “no” for negatives. This loss function encourages the model to assign higher probabilities to\\ncorrect labels, thereby improving the ranking performance.\\n3.2 Multi-stage Training'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='and “no” for negatives. This loss function encourages the model to assign higher probabilities to\\ncorrect labels, thereby improving the ranking performance.\\n3.2 Multi-stage Training\\nThe multi-stage training approach is a common practice for training text embedding models (Li et al.,\\n2023; Wang et al., 2022; Chen et al., 2024). This strategy typically begins with initial training on large-\\nscale, semi-supervised data that includes noise, followed by fine-tuning using smaller, high-quality\\nsupervised datasets. This two-step process enhances the performance and generalization capabilities\\nof embedding models. Large-scale weakly supervised training data contribute significantly to\\nthe model’s generalization, while fine-tuning with high-quality data in subsequent stages further\\nimproves model performance. Both stages of training for embedding models utilize the optimization\\nobjective defined in Equation 1, whereas the reranking model training employs the loss function'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='improves model performance. Both stages of training for embedding models utilize the optimization\\nobjective defined in Equation 1, whereas the reranking model training employs the loss function\\ndefined in Equation 2 as the optimization target.\\nBuilding upon the existing multi-stage training framework, the Qwen3 Embedding series introduces\\nthe following key innovations:\\n• Large-Scale Synthetic Data-Driven Weak Supervision Training: Unlike previous works (e.g.,\\nGTE, E5, BGE models), where weakly supervised training data are primarily collected from\\nopen-source communities such as Q&A forums or academic papers, we propose leveraging\\nthe text understanding and generation capabilities of foundation models to synthesize pair\\ndata directly. This approach allows for arbitrary definition of various dimensions of the\\ndesired pair data, such as task, language, length, and difficulty within the synthesis prompts.\\nCompared to data collection from open-domain sources, foundation model-driven data'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='desired pair data, such as task, language, length, and difficulty within the synthesis prompts.\\nCompared to data collection from open-domain sources, foundation model-driven data\\nsynthesis offers greater controllability, enabling precise management of the quality and\\ndiversity of the generated data, particularly in low-resource scenarios and languages.\\n• High-Quality Synthetic Data Utilization in Supervised Fine Tuning: Due to the exceptional\\nperformance of the Qwen3 Foundation model, the synthesized data is of notably high quality.\\nTherefore, in the second stage of supervised training, selective incorporation of this high-\\nquality synthetic data further enhances the overall model performance and generalization\\ncapabilities.\\n• Model Merging: Inspired by previous work (Li et al., 2024), after completing the supervised\\nfine-tuning, we applied a model merging technique based on spherical linear interpolation'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='capabilities.\\n• Model Merging: Inspired by previous work (Li et al., 2024), after completing the supervised\\nfine-tuning, we applied a model merging technique based on spherical linear interpolation\\n(slerp). This technique involves merging multiple model checkpoints saved during the\\nfine-tuning process. This step aims to boost the model’s robustness and generalization\\nperformance across various data distributions.\\nIt is important to note that the reranking model’s training process does not include a first-stage\\nweakly supervised training phase.\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\n3.3 Synthetic Dataset\\nTo create a robust synthetic dataset for training models on various similarity tasks, we generate\\ndiverse text pairs spanning categories such as retrieval, bitext mining, classification, and semantic\\ntextual similarity (STS). The quality of these synthetic data pairs is ensured by utilizing the Qwen3-\\n32B model as the foundational model for data synthesis. We have designed a diverse prompting\\nstrategy to improve the variety and authenticity of the generated data. For instance, in the text\\nretrieval task, we synthesize data using the multilingual pre-training corpus from Qwen3. During\\nthe data synthesis process, specific roles are assigned to each document to simulate potential\\nusers querying that document. This injection of user perspectives enhances the diversity and\\nrealism of the synthetic queries. Specifically, we utilize a retrieval model to identify the top five'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='users querying that document. This injection of user perspectives enhances the diversity and\\nrealism of the synthetic queries. Specifically, we utilize a retrieval model to identify the top five\\nrole candidates for each document from a role library and present these documents along with\\ntheir role candidates to the prompt. This guides the model in outputting the most suitable role\\nconfiguration for query generation. Moreover, the prompt incorporates various dimensions such as\\nquery type (e.g., keyword, factual, summary, judgment), query length, difficulty, and language. This\\nmultidimensional approach ensures the quality and diversity of the synthetic data.\\nFinally, we create a total of approximately 150 million pairs of multi-task weak supervision training\\ndata. Our experiments reveal that the embedding model trained with these synthetic data performs\\nexceptionally well in downstream evaluations, particularly surpassing many previously supervised'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='data. Our experiments reveal that the embedding model trained with these synthetic data performs\\nexceptionally well in downstream evaluations, particularly surpassing many previously supervised\\nmodels in the MTEB Multilingual benchmarks. This motivates us to filter the synthetic data to\\nidentify high-quality pairs for inclusion in a second stage of supervised training. We employ a\\nsimple cosine similarity calculation to select data pairs, retaining those with a cosine similarity\\ngreater than 0.7 from randomly sampled data. Ultimately, approximately 12 million high-quality\\nsupervised training data pairs are selected for further training.\\nModel Size Mean(Task)Mean(Type)BitextMiningClass-ificationClus-tering Inst.RetrievalMultilabelClass. PairClass.Rerank Retrieval STS\\nSelected Open-Source Models'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Model Size Mean(Task)Mean(Type)BitextMiningClass-ificationClus-tering Inst.RetrievalMultilabelClass. PairClass.Rerank Retrieval STS\\nSelected Open-Source Models\\nNV-Embed-v2 7B 56.29 49.58 57.84 57.29 40.80 1.04 18.63 78.94 63.82 56.72 71.10GritLM-7B 7B 60.92 53.74 70.53 61.83 49.75 3.45 22.77 79.94 63.78 58.31 73.33BGE-M3 0.6B 59.56 52.18 79.11 60.35 40.88 -3.11 20.1 80.76 62.79 54.60 74.12multilingual-e5-large-instruct 0.6B63.22 55.08 80.13 64.94 50.75 -0.40 22.91 80.86 62.61 57.12 76.81gte-Qwen2-1.5B-instruct 1.5B59.45 52.69 62.51 58.32 52.05 0.74 24.02 81.58 62.58 60.78 71.61gte-Qwen2-7b-Instruct 7B 62.51 55.93 73.92 61.55 52.77 4.94 25.48 85.13 65.55 60.08 73.98\\nCommercial APIs\\ntext-embedding-3-large - 58.93 51.41 62.17 60.27 46.89 -2.68 22.03 79.17 63.89 59.27 71.68Cohere-embed-multilingual-v3.0 -61.12 53.23 70.50 62.95 46.89 -1.89 22.74 79.88 64.07 59.16 74.80Gemini Embedding - 68.37 59.59 79.28 71.82 54.59 5.18 29.16 83.63 65.58 67.71 79.40\\nQwen3 Embedding Models'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Qwen3 Embedding Models\\nQwen3-Embedding-0.6B0.6B64.33 56.00 72.22 66.83 52.33 5.09 24.59 80.83 61.41 64.64 76.17Qwen3-Embedding-4B 4B 69.45 60.86 79.36 72.33 57.15 11.56 26.77 85.05 65.08 69.60 80.86Qwen3-Embedding-8B 8B 70.58 61.69 80.89 74.00 57.65 10.06 28.66 86.40 65.63 70.88 81.08\\nTable 2: Performance on MTEB Multilingual (Enevoldsen et al., 2025). For compared models, the\\nscores are retrieved from MTEB online leaderboard on June 4th, 2025.\\n4 Evaluation\\nWe conduct comprehensive and fair evaluations across multiple benchmarks to assess the capabilities\\nof Qwen3 Embedding models.\\n4.1 Settings\\nFor the text embedding models, we utilize the Massive Multilingual Text Embedding Benchmark\\n(MMTEB) (Enevoldsen et al., 2025) for evaluation. MMTEB is a large-scale, community-driven\\nexpansion of MTEB (Muennighoff et al., 2023), covering over 500 quality-controlled evaluation tasks\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nModel Size Dim MTEB (Eng, v2) CMTEB MTEB (Code)\\nMean (Task) Mean (Type)Mean (Task) Mean (Type)\\nSelected Open-Source Models\\nNV-Embed-v2 7B 4096 69.81 65.00 63.0 62.0 -\\nGritLM-7B 7B 4096 67.07 63.22 - - 73.6α\\nmultilingual-e5-large-instruct 0.6B1024 65.53 61.21 - - 65.0α\\ngte-Qwen2-1.5b-instruct 1.5B 1536 67.20 63.26 67.12 67.79 -\\ngte-Qwen2-7b-instruct 7B 3584 70.72 65.77 71.62 72.19 56.41γ\\nCommercial APIs\\ntext-embedding-3-large - 3072 66.43 62.15 - - 58.95γ\\ncohere-embed-multilingual-v3.0 -1024 66.01 61.43 - - 51.94γ\\nGemini Embedding - 3072 73.30 67.67 - - 74.66γ\\nQwen3 Embedding Models\\nQwen3-Embedding-0.6B 0.6B 1024 70.70 64.88 66.33 67.44 75.41\\nQwen3-Embedding-4B 4B 2560 74.60 68.09 72.26 73.50 80.06\\nQwen3-Embedding-8B 8B 4096 75.22 68.70 73.83 75.00 80.68\\nTable 3: Performance on MTEB Engilish, MTEB Chinese, MTEB Code. αTaken from (Enevoldsen\\net al., 2025). γTaken from (Lee et al., 2025b). For other compared models, the scores are retrieved'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Table 3: Performance on MTEB Engilish, MTEB Chinese, MTEB Code. αTaken from (Enevoldsen\\net al., 2025). γTaken from (Lee et al., 2025b). For other compared models, the scores are retrieved\\nfrom MTEB online leaderboard on June 4th, 2025.\\nacross more than 250 languages. In addition to classic text tasks such as as a variety of retrieval,\\nclassification, and semantic textual similarity, MMTEB includes a diverse set of challenging and\\nnovel tasks, such as instruction following, long-document retrieval, and code retrieval, representing\\nthe largest multilingual collection of evaluation tasks for embedding models to date. Our MMTEB\\nevaluations encompass 216 individual evaluation tasks, consisting of 131 tasks for MTEB (Multilin-\\ngual) (Enevoldsen et al., 2025), 41 tasks for MTEB (English, v2) (Muennighoff et al., 2023), 32 tasks\\nfor CMTEB (Xiao et al., 2024), and 12 code retrieval tasks for MTEB (Code) (Enevoldsen et al., 2025).'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='for CMTEB (Xiao et al., 2024), and 12 code retrieval tasks for MTEB (Code) (Enevoldsen et al., 2025).\\nMoreover, we select a series of text retrieval tasks to assess the text reranking capabilities of our\\nmodels. We explore three types of retrieval tasks: (1) Basic Relevance Retrieval, categorized into\\nEnglish, Chinese, and Multilingual, evaluated on MTEB (Muennighoff et al., 2023), CMTEB (Xiao\\net al., 2024), MMTEB (Enevoldsen et al., 2025), and MLDR (Chen et al., 2024), respectively; (2) Code\\nRetrieval, evaluated on MTEB-Code (Enevoldsen et al., 2025), which comprises only code-related\\nretrieval data.; and (3) Complex Instruction Retrieval, evaluated on FollowIR (Weller et al., 2024).\\nCompared Methods We compare our models with the most prominent open-source text embed-\\nding models and commercial API services. The open-source models include the GTE (Li et al.,\\n2023; Zhang et al., 2024b), E5 (Wang et al., 2022), and BGE (Xiao et al., 2024) series, as well as NV-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='ding models and commercial API services. The open-source models include the GTE (Li et al.,\\n2023; Zhang et al., 2024b), E5 (Wang et al., 2022), and BGE (Xiao et al., 2024) series, as well as NV-\\nEmbed-v2 (Lee et al., 2025a), GritLM-7B Muennighoff et al. (2025). The commercial APIs evaluated\\nare text-embedding-3-large from OpenAI, Gemini-embedding from Google, and Cohere-embed-\\nmultilingual-v3.0. For reranking, we compare with the rerankers of jina1, mGTE (Zhang et al., 2024b)\\nand BGE-m3 (Chen et al., 2024).\\n4.2 Main Results\\nEmbedding In Table 2, we present the evaluation results on MMTEB (Enevoldsen et al., 2025),\\nwhich comprehensively covers a wide range of embedding tasks across multiple languages. Our\\nQwen3-Embedding-4B/8B models achieve the best performance, and our smallest model, Qwen3-\\nEmbedding-0.6B, only lags behind the best-performing baseline method (Gemini-Embedding),\\ndespite having only 0.6B parameters. In Table 3, we present the evaluation results on MTEB (English,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Embedding-0.6B, only lags behind the best-performing baseline method (Gemini-Embedding),\\ndespite having only 0.6B parameters. In Table 3, we present the evaluation results on MTEB (English,\\nv2) (Muennighoff et al., 2023), CMTEB (Xiao et al., 2024), and MTEB (Code) (Enevoldsen et al.,\\n2025). The scores reflect similar trends as MMTEB, with our Qwen3-Embedding-4B/8B models\\n1https://hf.co/jinaai/jina-reranker-v2-base-multilingual\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nBasic Relevance Retrieval\\nModel Param MTEB-R CMTEB-R MMTEB-R MLDR MTEB-Code FollowIR\\nQwen3-Embedding-0.6B 0.6B 61.82 71.02 64.64 50.26 75.41 5.09\\nJina-multilingual-reranker-v2-base 0.3B 58.22 63.37 63.73 39.66 58.98 -0.68\\ngte-multilingual-reranker-base 0.3B 59.51 74.08 59.44 66.33 54.18 -1.64\\nBGE-reranker-v2-m3 0.6B 57.03 72.16 58.36 59.51 41.38 -0.01\\nQwen3-Reranker-0.6B 0.6B 65.80 71.31 66.36 67.28 73.42 5.41\\nQwen3-Reranker-4B 4B 69.76 75.94 72.74 69.97 81.20 14.84\\nQwen3-Reranker-8B 8B 69.02 77.45 72.94 70.19 81.22 8.05\\nTable 4: Evaluation results for reranking models. We use the retrieval subsets of MTEB(eng, v2),\\nMTEB(cmn, v1) and MMTEB, which are MTEB-R, CMTEB-R and MMTEM-R. The rest are all retrieval\\ntasks. All scores are our runs based on the retrieval top-100 results from the first row.\\nModel MMTEB MTEB (Eng, v2)CMTEB MTEB (Code, v1)\\nQwen3-Embedding-0.6B w/ only synthetic data58.49 60.63 59.78 66.79\\nQwen3-Embedding-0.6B w/o synthetic data 61.21 65.59 63.37 74.58'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Model MMTEB MTEB (Eng, v2)CMTEB MTEB (Code, v1)\\nQwen3-Embedding-0.6B w/ only synthetic data58.49 60.63 59.78 66.79\\nQwen3-Embedding-0.6B w/o synthetic data 61.21 65.59 63.37 74.58\\nQwen3-Embedding-0.6B w/o model merge 62.56 68.18 64.76 74.89\\nQwen3-Embedding-0.6B 64.33 70.70 66.33 75.41\\nTable 5: Performance (mean task) on MMTEB, MTEB(eng, v2), CMTEB and MTEB(code, v1) for\\nQwen3-Embedding-0.6B model with different training setting.\\nconsistently outperforming others. Notably, the Qwen3-Embedding-0.6B model ranks just behind\\nthe Gemini-Embedding, while being competitive with the gte-Qwen2-7B-instruct.\\nReranking In Table 4, we present the evaluation results on various reranking tasks ( §4.1). We\\nutilize the Qwen3-Embedding-0.6B model to retrieve the top-100 candidates and then apply different\\nreranking models for further refinement. This approach ensures a fair evaluation of the reranking\\nmodels. Our results indicate that all three Qwen3-Reranker models enhance performance compared'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='reranking models for further refinement. This approach ensures a fair evaluation of the reranking\\nmodels. Our results indicate that all three Qwen3-Reranker models enhance performance compared\\nto the embedding model and surpass all baseline reranking methods, with Qwen3-Reranker-8B\\nachieving the highest performance across most tasks.\\n4.3 Analysis\\nTo further analyze and explore the key elements of the Qwen3 Embedding model training framework,\\nwe conduct an analysis from the following dimensions:\\nEffectiveness of Large-Scale Weakly Supervised Pre-TrainingWe first analyze the effectiveness\\nof the large-scale weak supervised training stage for the embedding models. As shown in Table 5,\\nthe Qwen3-Embedding-0.6B model trained solely on synthetic data (without subsequent training\\nstages, as indicated in the first row) achieves reasonable and strong performance compared to the\\nfinal Qwen3-Embedding-0.6B model (as shown in the last row). If we further remove the weak'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='stages, as indicated in the first row) achieves reasonable and strong performance compared to the\\nfinal Qwen3-Embedding-0.6B model (as shown in the last row). If we further remove the weak\\nsupervised training stage (i.e., without synthetic data training, as seen in the second row), the final\\nperformance shows a clear decline. This indicates that the large-scale weak supervised training\\nstage is crucial for achieving superior performance.\\nEffectiveness of Model MergingNext, we compare the performance differences arising from the\\nmodel merging stage. As shown in Table 5, the model trained without model merging techniques\\n(the third row, which uses data sampling to balance various tasks) performs considerably worse\\nthan the final Qwen3-Embedding-0.6B model (which employs model merging, as shown in the last\\nrow). This indicates that the model merging stage is also critical for developing strong models.\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\n5 Conclusion\\nIn this technical report, we present the Qwen3-Embedding series, a comprehensive suite of text\\nembedding and reranking models based on the Qwen3 foundation models. These models are\\ndesigned to excel in a wide range of text embedding and reranking tasks, including multilingual\\nretrieval, code retrieval, and complex instruction following. The Qwen3-Embedding models are\\nbuilt upon a robust multi-stage training pipeline that combines large-scale weakly supervised\\npre-training on synthetic data with supervised fine-tuning and model merging on high-quality\\ndatasets. The Qwen3 LLMs play a crucial role in synthesizing diverse training data across multiple\\nlanguages and tasks, thereby enhancing the models’ capabilities. Our comprehensive evaluations\\ndemonstrate that the Qwen3-Embedding models achieve state-of-the-art performance across various\\nbenchmarks, including MTEB, CMTEB, MMTEB, and several retrieval benchmarks. We are pleased'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='demonstrate that the Qwen3-Embedding models achieve state-of-the-art performance across various\\nbenchmarks, including MTEB, CMTEB, MMTEB, and several retrieval benchmarks. We are pleased\\nto open-source the Qwen3-Embedding and Qwen3-Reranker models (0.6B, 4B, and 8B), making\\nthem available for the community to use and build upon.\\nReferences\\nJianlyu Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. M3-embedding:\\nMulti-linguality, multi-functionality, multi-granularity text embeddings through self-knowledge\\ndistillation. In Findings of the Association for Computational Linguistics: ACL 2024, pp. 2318–2335,\\nBangkok, Thailand, August 2024. Association for Computational Linguistics. URL https://\\naclanthology.org/2024.findings-acl.137/.\\nKenneth Enevoldsen, Isaac Chung, Imene Kerboua, M´arton Kardos, Ashwin Mathur, David Stap,\\nJay Gala, Wissam Siblini, Dominik Krzemi ´nski, Genta Indra Winata, et al. MMTEB: Massive'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, M´arton Kardos, Ashwin Mathur, David Stap,\\nJay Gala, Wissam Siblini, Dominik Krzemi ´nski, Genta Indra Winata, et al. MMTEB: Massive\\nmultilingual text embedding benchmark. In The Thirteenth International Conference on Learning\\nRepresentations, 2025. URL https://openreview.net/forum?id=zl3pfz4VCV.\\nTao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. Scaling synthetic data creation\\nwith 1,000,000,000 personas. arXiv preprint arXiv:2406.20094, 2024.\\nJui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padman-\\nabhan, Giuseppe Ottaviano, and Linjun Yang. Embedding-based retrieval in facebook search. In\\nProceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,\\npp. 2553–2561, 2020.\\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark,\\nAJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark,\\nAJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint\\narXiv:2410.21276, 2024.\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick SH Lewis, Ledell Wu, Sergey Edunov, Danqi\\nChen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In EMNLP\\n(1), pp. 6769–6781, 2020.\\nChankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro,\\nand Wei Ping. Nv-embed: Improved techniques for training llms as generalist embedding models.\\narXiv preprint arXiv:2405.17428, 2024.\\nChankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro,\\nand Wei Ping. NV-embed: Improved techniques for training LLMs as generalist embedding\\nmodels. In The Thirteenth International Conference on Learning Representations, 2025a. URL https:\\n//openreview.net/forum?id=lgsyLSsDRe.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='models. In The Thirteenth International Conference on Learning Representations, 2025a. URL https:\\n//openreview.net/forum?id=lgsyLSsDRe.\\nJinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Shanbhogue, Iftekhar Naim, Gus-\\ntavo Hern´andez ´Abrego, Zhe Li, Kaifeng Chen, Henrique Schechter Vera, et al. Gemini embedding:\\nGeneralizable embeddings from gemini. arXiv preprint arXiv:2503.07891, 2025b.\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nMingxin Li, Zhijie Nie, Yanzhao Zhang, Dingkun Long, Richong Zhang, and Pengjun Xie. Improving\\ngeneral text embedding model: Tackling task conflict and data imbalance through model merging.\\narXiv preprint arXiv:2410.15035, 2024.\\nZehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards\\ngeneral text embeddings with multi-stage contrastive learning, 2023. URL https://arxiv.org/\\nabs/2308.03281.\\nXueguang Ma, Xinyu Zhang, Ronak Pradeep, and Jimmy Lin. Zero-shot listwise document reranking\\nwith a large language model. arXiv preprint arXiv:2305.02156, 2023.\\nNiklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. MTEB: Massive text embed-\\nding benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for\\nComputational Linguistics, pp. 2014–2037, Dubrovnik, Croatia, May 2023. Association for Computa-\\ntional Linguistics. URL https://aclanthology.org/2023.eacl-main.148/.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Computational Linguistics, pp. 2014–2037, Dubrovnik, Croatia, May 2023. Association for Computa-\\ntional Linguistics. URL https://aclanthology.org/2023.eacl-main.148/.\\nNiklas Muennighoff, Hongjin SU, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and\\nDouwe Kiela. Generative representational instruction tuning. In The Thirteenth International Con-\\nference on Learning Representations, 2025. URL https://openreview.net/forum?id=BC4lIvfSzv.\\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\\ntive coding. arXiv preprint arXiv:1807.03748, 2018.\\nRonak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin. Rankvicuna: Zero-shot listwise docu-\\nment reranking with open-source large language models. arXiv preprint arXiv:2309.15088, 2023.\\nNils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-\\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-\\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\\nand the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp.\\n3982–3992, Hong Kong, China, November 2019. Association for Computational Linguistics. URL\\nhttps://aclanthology.org/D19-1410/.\\nHongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih,\\nNoah A Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-finetuned text\\nembeddings. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 1102–1121,\\n2023.\\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\\nand Furu Wei. Text embeddings by weakly-supervised contrastive pre-training, 2022. URL\\nhttps://arxiv.org/abs/2212.03533.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training, 2022. URL\\nhttps://arxiv.org/abs/2212.03533.\\nLiang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Im-\\nproving text embeddings with large language models. In Proceedings of the 62nd Annual Meet-\\ning of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 11897–11916,\\nBangkok, Thailand, August 2024. Association for Computational Linguistics. URL https:\\n//aclanthology.org/2024.acl-long.642/.\\nOrion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme,\\nDawn Lawrie, and Luca Soldaini. Followir: Evaluating and teaching information retrieval models\\nto follow instructions. arXiv preprint arXiv:2403.15246, 2024.\\nShitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. C-pack:\\nPacked resources for general chinese embeddings. In Proceedings of the 47th International ACM'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. C-pack:\\nPacked resources for general chinese embeddings. In Proceedings of the 47th International ACM\\nSIGIR Conference on Research and Development in Information Retrieval, SIGIR ’24, pp. 641–649, New\\nYork, NY, USA, 2024. Association for Computing Machinery. URLhttps://doi.org/10.1145/\\n3626772.3657878.\\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\\nChengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025.\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nLonghui Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, and Min Zhang. A\\ntwo-stage adaptation of large language models for text ranking. In Findings of the Association for\\nComputational Linguistics ACL 2024, pp. 11880–11891, 2024a.\\nXin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialong Tang, Huan Lin, Baosong\\nYang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min Zhang. mGTE: Generalized\\nlong-context text representation and reranking models for multilingual text retrieval. In Franck\\nDernoncourt, Daniel Preo t ¸iuc-Pietro, and Anastasia Shimorina (eds.), Proceedings of the 2024\\nConference on Empirical Methods in Natural Language Processing: Industry Track, pp. 1393–1412,\\nMiami, Florida, US, November 2024b. Association for Computational Linguistics. doi: 10.18653/\\nv1/2024.emnlp-industry.103. URL https://aclanthology.org/2024.emnlp-industry.103/.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Miami, Florida, US, November 2024b. Association for Computational Linguistics. doi: 10.18653/\\nv1/2024.emnlp-industry.103. URL https://aclanthology.org/2024.emnlp-industry.103/.\\nWayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji-Rong Wen. Dense text retrieval based on pretrained\\nlanguage models: A survey. ACM Transactions on Information Systems, 42(4):1–60, 2024.\\nXiangyu Zhao, Maolin Wang, Xinjian Zhao, Jiansheng Li, Shucheng Zhou, Dawei Yin, Qing Li,\\nJiliang Tang, and Ruocheng Guo. Embedding in recommender systems: A survey. arXiv preprint\\narXiv:2310.18608, 2023.\\nShengyao Zhuang, Honglei Zhuang, Bevan Koopman, and Guido Zuccon. A setwise approach for\\neffective and highly efficient zero-shot ranking with large language models. In Proceedings of the\\n47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp.\\n38–47, 2024.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nA Appendix\\nA.1 Synthetic Data\\nWe construct four types of synthetic data—retrieval, bitext mining, semantic textual similarity, and\\nclassification to enable the model to adapt to various similarity tasks during pre-training. To ensure\\nboth multilingual and cross-lingual diversity, the data is generated using Qwen3 32B. Below is an\\nexample of a synthetic retrieval text pair. The retrieval data is synthesized using a document-to-\\nquery approach. We collect a multilingual corpus from the pre-training corpus of the Qwen3 base\\nmodel to serve as the document source. A two-stage generation pipeline is then applied, consisting\\nof: (1) configuration and (2) query generation. In the configuration stage, we use large language\\nmodels (LLMs) to determine the “Question Type”, “Difficulty”, and “Character” for the synthetic\\nquery. The candidate characters are retrieved from Persona Hub (Ge et al., 2024), selecting the top'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='models (LLMs) to determine the “Question Type”, “Difficulty”, and “Character” for the synthetic\\nquery. The candidate characters are retrieved from Persona Hub (Ge et al., 2024), selecting the top\\nfive most relevant to the given document. This step aims to enhance the diversity of the generated\\nqueries. The template used is as follows:\\nGiven a **Passage** and **Character**, select the appropriate option from\\nthree fields: Character, Question_Type, Difficulty, and return the output\\nin JSON format.\\n,→\\n,→\\nFirst, select the Character who are likely to be interested in the Passage\\nfrom the candidates. Then select the Question_Type that the Character\\nmight ask about the Passage; Finally, choose the Difficulty of the\\npossible question based on the Passage, the Character, and the\\nQuestion_Type.\\n,→\\n,→\\n,→\\n,→\\nCharacter: Given by input **Character**\\nQuestion_Type:\\n- keywords: ...\\n- acquire_knowledge: ...\\n- summary: ...\\n- yes_or_no: ...\\n- background: ...\\nDifficulty:\\n- high_school: ...'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content=',→\\n,→\\n,→\\n,→\\nCharacter: Given by input **Character**\\nQuestion_Type:\\n- keywords: ...\\n- acquire_knowledge: ...\\n- summary: ...\\n- yes_or_no: ...\\n- background: ...\\nDifficulty:\\n- high_school: ...\\n- university: ...\\n- phd: ...\\nHere are some examples\\n<Example1> <Example2> <Example3>\\nNow, generate the **output** based on the **Passage** and **Character** from\\nuser, the **Passage** will be in {language} language and the **Character**\\nwill be in English.\\n,→\\n,→\\nEnsure to generate only the JSON output with content in English.\\n**Passage**:\\n{passage}\\n**Character**:\\n{character}\\nIn the query generation stage, we use the configuration selected in the first stage to guide the\\ngeneration of queries. Additionally, we explicitly specify the desired length and language of the\\ngenerated query. The template used is as follows:\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content=\"Technical Report\\nGiven a **Character**, **Passage**, and **Requirement**, generate a query from\\nthe **Character**'s perspective that satisfies the **Requirement** and can\\nbe used to retrieve the **Passage**. Please return the result in JSON\\nformat.\\n,→\\n,→\\n,→\\nHere is an example:\\n<example>\\nNow, generate the **output** based on the **Character**, **Passage** and\\n**Requirement** from user, the **Passage** will be in {corpus_language}\\nlanguage, the **Character** and **Requirement** will be in English.\\n,→\\n,→\\nEnsure to generate only the JSON output, with the key in English and the value\\nin {queries_language} language.,→\\n**Character**\\n{character}\\n**Passage**\\n{passage}\\n**Requirment**\\n- Type: {type};\\n- Difficulty: {difficulty};\\n- Length: the length of the generated sentences should be {length} words;\\n- Languange: the language in which the results are generated should be\\n{language} language;,→\\nStage Dataset Size\\nWeakly Supervised Pre-Training Synthetic Data ∼ 150M\\nSupervised Fine Tuning\"),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='- Languange: the language in which the results are generated should be\\n{language} language;,→\\nStage Dataset Size\\nWeakly Supervised Pre-Training Synthetic Data ∼ 150M\\nSupervised Fine Tuning\\nMS MARCO, NQ, HotpotQA, NLI,\\nDureader, T2-Ranking, SimCLUE,\\nMIRACL, MLDR, Mr.TyDi,\\nMulti-CPR, CodeSearchNet .etc\\n+ High-quality Synthetic Data\\nLabeled Data: ∼ 7M\\nSynthetic Data: ∼ 12M\\nTable 6: Statistics of training data utilized at each stage.\\nA.2 Detail Results\\nMTEB(eng, v2) Param Mean\\n(Task)\\nMean\\n(Type)\\nClass-\\nification\\nClus-\\ntering\\nPair\\nClass. Rerank Retrieval STS Summ.\\nmultilingual-e5-large-instruct 0.6B 65.53 61.21 75.54 49.89 86.24 48.74 53.47 84.72 29.89\\nNV-Embed-v2 7.8B 69.81 65.00 87.19 47.66 88.69 49.61 62.84 83.82 35.21\\nGritLM-7B 7.2B 67.07 63.22 81.25 50.82 87.29 49.59 54.95 83.03 35.65\\ngte-Qwen2-1.5B-instruct 1.5B 67.20 63.26 85.84 53.54 87.52 49.25 50.25 82.51 33.94\\nstellaen1.5Bv5 1.5B 69.43 65.32 89.38 57.06 88.02 50.19 52.42 83.27 36.91'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='gte-Qwen2-1.5B-instruct 1.5B 67.20 63.26 85.84 53.54 87.52 49.25 50.25 82.51 33.94\\nstellaen1.5Bv5 1.5B 69.43 65.32 89.38 57.06 88.02 50.19 52.42 83.27 36.91\\ngte-Qwen2-7B-instruct 7.6B 70.72 65.77 88.52 58.97 85.9 50.47 58.09 82.69 35.74\\ngemini-embedding-exp-03-07 - 73.3 67.67 90.05 59.39 87.7 48.59 64.35 85.29 38.28\\nQwen3-Embedding-0.6B 0.6B 70.70 64.88 85.76 54.05 84.37 48.18 61.83 86.57 33.43\\nQwen3-Embedding-4B 4B 74.60 68.09 89.84 57.51 87.01 50.76 68.46 88.72 34.39\\nQwen3-Embedding-8B 8B 75.22 68.70 90.43 58.57 87.52 51.56 69.44 88.58 34.83\\nTable 7: Results on MTEB(eng, v2) (Muennighoff et al., 2023). We compare models from the online\\nleaderboard.\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Technical Report\\nMTEB(cmn, v1) Param Mean\\n(Task)\\nMean\\n(Type)\\nClass-\\nification\\nClus-\\ntering\\nPair\\nClass. Rerank Retrieval STS\\nmultilingual-e5-large-instruct 0.6B 58.08 58.24 69.80 48.23 64.52 57.45 63.65 45.81\\ngte-Qwen2-7B-instruct 7.6B 71.62 72.19 75.77 66.06 81.16 69.24 75.70 65.20\\ngte-Qwen2-1.5B-instruct 1.5B 67.12 67.79 72.53 54.61 79.5 68.21 71.86 60.05\\nQwen3-Embedding-0.6B 0.6B 66.33 67.44 71.40 68.74 76.42 62.58 71.03 54.52\\nQwen3-Embedding-4B 4B 72.26 73.50 75.46 77.89 83.34 66.05 77.03 61.26\\nQwen3-Embedding-8B 8B 73.84 75.00 76.97 80.08 84.23 66.99 78.21 63.53\\nTable 8: Results on C-MTEB (Xiao et al., 2024) (MTEB(cmn, v1).\\nMTEB(Code, v1) Avg.Apps COIR-CodeSearch-Net\\nCode-Edit-Search\\nCode-Feedback-MT\\nCode-Feedback-ST\\nCode-SearchNet-CCR\\nCode-SearchNet\\nCode-Trans-Ocean-Contest\\nCode-Trans-Ocean-DLCosQAStack-Overflow-QA\\nSynthetic-Text2SQL'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='Code-Edit-Search\\nCode-Feedback-MT\\nCode-Feedback-ST\\nCode-SearchNet-CCR\\nCode-SearchNet\\nCode-Trans-Ocean-Contest\\nCode-Trans-Ocean-DLCosQAStack-Overflow-QA\\nSynthetic-Text2SQL\\nBGEmultilingual 62.0422.93 68.14 60.48 60.52 76.70 73.23 83.43 86.84 32.64 27.93 92.93 58.67NV-Embed-v2 63.7429.72 61.85 73.96 60.27 81.72 68.82 86.61 89.14 33.40 34.82 92.36 60.90gte-Qwen2-7B-instruct 62.1728.39 71.79 67.06 57.66 85.15 66.24 86.96 81.83 32.17 31.26 84.34 53.22gte-Qwen2-1.5B-instruct 61.9828.91 71.56 59.60 49.92 81.92 72.08 91.08 79.02 32.73 32.23 90.27 54.49\\nBGE-M3 (Dense) 58.2214.77 58.07 59.83 47.86 69.27 53.55 61.98 86.22 29.37 27.36 80.71 49.65Jina-v3 58.85 28.99 67.83 57.24 59.66 78.13 54.17 85.50 77.37 30.91 35.15 90.79 41.49'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '', 'author': 'Yanzhao Zhang; Mingxin Li; Dingkun Long; Xin Zhang; Huan Lin; Baosong Yang; Pengjun Xie; An Yang; Dayiheng Liu; Junyang Lin; Fei Huang; Jingren Zhou', 'doi': 'https://doi.org/10.48550/arXiv.2506.05176', 'license': 'http://arxiv.org/licenses/nonexclusive-distrib/1.0/', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'title': 'Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models', 'trapped': '/False', 'arxivid': 'https://arxiv.org/abs/2506.05176v3', 'source': '..\\\\data\\\\pdf\\\\Quen3 Embedding research paper.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14', 'source_file': 'Quen3 Embedding research paper.pdf', 'file_type': 'pdf'}, page_content='BGE-M3 (Dense) 58.2214.77 58.07 59.83 47.86 69.27 53.55 61.98 86.22 29.37 27.36 80.71 49.65Jina-v3 58.85 28.99 67.83 57.24 59.66 78.13 54.17 85.50 77.37 30.91 35.15 90.79 41.49\\nQwen3-Embedding-0.6B 75.4175.34 84.69 64.42 90.82 86.39 91.72 91.01 86.05 31.36 36.48 89.99 76.74Qwen3-Embedding-4B 80.0689.18 87.93 76.49 93.21 89.51 95.59 92.34 90.99 35.04 37.98 94.32 78.21Qwen3-Embedding-8B 80.6891.07 89.51 76.97 93.70 89.93 96.35 92.66 93.73 32.81 38.04 94.75 78.75\\nQwen3-Reranker-0.6B 73.4269.43 85.09 72.37 83.83 78.05 94.76 88.8 84.69 33.94 36.83 93.24 62.48Qwen3-Reranker-4B 81.2094.25 90.91 82.53 95.25 88.54 97.58 92.48 93.66 36.78 35.14 97.11 75.06Qwen3-Reranker-8B 81.2294.55 91.88 84.58 95.64 88.43 95.67 92.78 90.83 34.89 37.43 97.3 73.4\\nTable 9: Performance on MTEB(Code, v1) (Enevoldsen et al., 2025). We report nDCG@10 scores.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='1\\nRetrieval-Augmented Generation for Large\\nLanguage Models: A Survey\\nYunfan Gaoa, Yun Xiongb, Xinyu Gao b, Kangxiang Jia b, Jinliu Pan b, Yuxi Bic, Yi Dai a, Jiawei Sun a, Meng\\nWangc, and Haofen Wang a,c\\naShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\\nbShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\ncCollege of Design and Innovation, Tongji University\\nAbstract—Large Language Models (LLMs) showcase impres-\\nsive capabilities but encounter challenges like hallucination,\\noutdated knowledge, and non-transparent, untraceable reasoning\\nprocesses. Retrieval-Augmented Generation (RAG) has emerged\\nas a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the\\ngeneration, particularly for knowledge-intensive tasks, and allows\\nfor continuous knowledge updates and integration of domain-\\nspecific information. RAG synergistically merges LLMs’ intrin-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='generation, particularly for knowledge-intensive tasks, and allows\\nfor continuous knowledge updates and integration of domain-\\nspecific information. RAG synergistically merges LLMs’ intrin-\\nsic knowledge with the vast, dynamic repositories of external\\ndatabases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing\\nthe Naive RAG, the Advanced RAG, and the Modular RAG.\\nIt meticulously scrutinizes the tripartite foundation of RAG\\nframeworks, which includes the retrieval, the generation and the\\naugmentation techniques. The paper highlights the state-of-the-\\nart technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG\\nsystems. Furthermore, this paper introduces up-to-date evalua-\\ntion framework and benchmark. At the end, this article delineates\\nthe challenges currently faced and points out prospective avenues\\nfor research and development 1.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='tion framework and benchmark. At the end, this article delineates\\nthe challenges currently faced and points out prospective avenues\\nfor research and development 1.\\nIndex Terms—Large language model, retrieval-augmented gen-\\neration, natural language processing, information retrieval\\nI. I NTRODUCTION\\nL\\nARGE language models (LLMs) have achieved remark-\\nable success, though they still face significant limitations,\\nespecially in domain-specific or knowledge-intensive tasks [1],\\nnotably producing “hallucinations” [2] when handling queries\\nbeyond their training data or requiring current information. To\\novercome challenges, Retrieval-Augmented Generation (RAG)\\nenhances LLMs by retrieving relevant document chunks from\\nexternal knowledge base through semantic similarity calcu-\\nlation. By referencing external knowledge, RAG effectively\\nreduces the problem of generating factually incorrect content.\\nIts integration into LLMs has resulted in widespread adoption,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='lation. By referencing external knowledge, RAG effectively\\nreduces the problem of generating factually incorrect content.\\nIts integration into LLMs has resulted in widespread adoption,\\nestablishing RAG as a key technology in advancing chatbots\\nand enhancing the suitability of LLMs for real-world applica-\\ntions.\\nRAG technology has rapidly developed in recent years, and\\nthe technology tree summarizing related research is shown\\nCorresponding Author.Email:haofen.wang@tongji.edu.cn\\n1Resources are available at https://github.com/Tongji-KGLLM/\\nRAG-Survey\\nin Figure 1. The development trajectory of RAG in the era\\nof large models exhibits several distinct stage characteristics.\\nInitially, RAG’s inception coincided with the rise of the\\nTransformer architecture, focusing on enhancing language\\nmodels by incorporating additional knowledge through Pre-\\nTraining Models (PTM). This early stage was characterized\\nby foundational work aimed at refining pre-training techniques'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='models by incorporating additional knowledge through Pre-\\nTraining Models (PTM). This early stage was characterized\\nby foundational work aimed at refining pre-training techniques\\n[3]–[5].The subsequent arrival of ChatGPT [6] marked a\\npivotal moment, with LLM demonstrating powerful in context\\nlearning (ICL) capabilities. RAG research shifted towards\\nproviding better information for LLMs to answer more com-\\nplex and knowledge-intensive tasks during the inference stage,\\nleading to rapid development in RAG studies. As research\\nprogressed, the enhancement of RAG was no longer limited\\nto the inference stage but began to incorporate more with LLM\\nfine-tuning techniques.\\nThe burgeoning field of RAG has experienced swift growth,\\nyet it has not been accompanied by a systematic synthesis that\\ncould clarify its broader trajectory. This survey endeavors to\\nfill this gap by mapping out the RAG process and charting\\nits evolution and anticipated future paths, with a focus on the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='could clarify its broader trajectory. This survey endeavors to\\nfill this gap by mapping out the RAG process and charting\\nits evolution and anticipated future paths, with a focus on the\\nintegration of RAG within LLMs. This paper considers both\\ntechnical paradigms and research methods, summarizing three\\nmain research paradigms from over 100 RAG studies, and\\nanalyzing key technologies in the core stages of “Retrieval,”\\n“Generation,” and “Augmentation.” On the other hand, current\\nresearch tends to focus more on methods, lacking analysis and\\nsummarization of how to evaluate RAG. This paper compre-\\nhensively reviews the downstream tasks, datasets, benchmarks,\\nand evaluation methods applicable to RAG. Overall, this\\npaper sets out to meticulously compile and categorize the\\nfoundational technical concepts, historical progression, and\\nthe spectrum of RAG methodologies and applications that\\nhave emerged post-LLMs. It is designed to equip readers and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 0, 'page_label': '1', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='foundational technical concepts, historical progression, and\\nthe spectrum of RAG methodologies and applications that\\nhave emerged post-LLMs. It is designed to equip readers and\\nprofessionals with a detailed and structured understanding of\\nboth large models and RAG. It aims to illuminate the evolution\\nof retrieval augmentation techniques, assess the strengths and\\nweaknesses of various approaches in their respective contexts,\\nand speculate on upcoming trends and innovations.\\nOur contributions are as follows:\\n• In this survey, we present a thorough and systematic\\nreview of the state-of-the-art RAG methods, delineating\\nits evolution through paradigms including naive RAG,\\narXiv:2312.10997v5  [cs.CL]  27 Mar 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='2\\nFig. 1. Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs,\\nresearch on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent\\nresearch has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models\\nin the pre-training stage through retrieval-augmented techniques.\\nadvanced RAG, and modular RAG. This review contex-\\ntualizes the broader scope of RAG research within the\\nlandscape of LLMs.\\n• We identify and discuss the central technologies integral\\nto the RAG process, specifically focusing on the aspects\\nof “Retrieval”, “Generation” and “Augmentation”, and\\ndelve into their synergies, elucidating how these com-\\nponents intricately collaborate to form a cohesive and\\neffective RAG framework.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='of “Retrieval”, “Generation” and “Augmentation”, and\\ndelve into their synergies, elucidating how these com-\\nponents intricately collaborate to form a cohesive and\\neffective RAG framework.\\n• We have summarized the current assessment methods of\\nRAG, covering 26 tasks, nearly 50 datasets, outlining\\nthe evaluation objectives and metrics, as well as the\\ncurrent evaluation benchmarks and tools. Additionally,\\nwe anticipate future directions for RAG, emphasizing\\npotential enhancements to tackle current challenges.\\nThe paper unfolds as follows: Section II introduces the\\nmain concept and current paradigms of RAG. The following\\nthree sections explore core components—“Retrieval”, “Gen-\\neration” and “Augmentation”, respectively. Section III focuses\\non optimization methods in retrieval,including indexing, query\\nand embedding optimization. Section IV concentrates on post-\\nretrieval process and LLM fine-tuning in generation. Section V\\nanalyzes the three augmentation processes. Section VI focuses'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='and embedding optimization. Section IV concentrates on post-\\nretrieval process and LLM fine-tuning in generation. Section V\\nanalyzes the three augmentation processes. Section VI focuses\\non RAG’s downstream tasks and evaluation system. Sec-\\ntion VII mainly discusses the challenges that RAG currently\\nfaces and its future development directions. At last, the paper\\nconcludes in Section VIII.\\nII. O VERVIEW OF RAG\\nA typical application of RAG is illustrated in Figure 2.\\nHere, a user poses a question to ChatGPT about a recent,\\nwidely discussed news. Given ChatGPT’s reliance on pre-\\ntraining data, it initially lacks the capacity to provide up-\\ndates on recent developments. RAG bridges this information\\ngap by sourcing and incorporating knowledge from external\\ndatabases. In this case, it gathers relevant news articles related\\nto the user’s query. These articles, combined with the original\\nquestion, form a comprehensive prompt that empowers LLMs\\nto generate a well-informed answer.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 1, 'page_label': '2', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='to the user’s query. These articles, combined with the original\\nquestion, form a comprehensive prompt that empowers LLMs\\nto generate a well-informed answer.\\nThe RAG research paradigm is continuously evolving, and\\nwe categorize it into three stages: Naive RAG, Advanced\\nRAG, and Modular RAG, as showed in Figure 3. Despite\\nRAG method are cost-effective and surpass the performance\\nof the native LLM, they also exhibit several limitations.\\nThe development of Advanced RAG and Modular RAG is\\na response to these specific shortcomings in Naive RAG.\\nA. Naive RAG\\nThe Naive RAG research paradigm represents the earli-\\nest methodology, which gained prominence shortly after the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='3\\nFig. 2. A representative instance of the RAG process applied to question answering. It mainly consists of 3 steps. 1) Indexing. Documents are split into chunks,\\nencoded into vectors, and stored in a vector database. 2) Retrieval. Retrieve the Top k chunks most relevant to the question based on semantic similarity. 3)\\nGeneration. Input the original question and the retrieved chunks together into LLM to generate the final answer.\\nwidespread adoption of ChatGPT. The Naive RAG follows\\na traditional process that includes indexing, retrieval, and\\ngeneration, which is also characterized as a “Retrieve-Read”\\nframework [7].\\nIndexing starts with the cleaning and extraction of raw data\\nin diverse formats like PDF, HTML, Word, and Markdown,\\nwhich is then converted into a uniform plain text format. To\\naccommodate the context limitations of language models, text\\nis segmented into smaller, digestible chunks. Chunks are then\\nencoded into vector representations using an embedding model'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='accommodate the context limitations of language models, text\\nis segmented into smaller, digestible chunks. Chunks are then\\nencoded into vector representations using an embedding model\\nand stored in vector database. This step is crucial for enabling\\nefficient similarity searches in the subsequent retrieval phase.\\nRetrieval. Upon receipt of a user query, the RAG system\\nemploys the same encoding model utilized during the indexing\\nphase to transform the query into a vector representation.\\nIt then computes the similarity scores between the query\\nvector and the vector of chunks within the indexed corpus.\\nThe system prioritizes and retrieves the top K chunks that\\ndemonstrate the greatest similarity to the query. These chunks\\nare subsequently used as the expanded context in prompt.\\nGeneration. The posed query and selected documents are\\nsynthesized into a coherent prompt to which a large language\\nmodel is tasked with formulating a response. The model’s'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='Generation. The posed query and selected documents are\\nsynthesized into a coherent prompt to which a large language\\nmodel is tasked with formulating a response. The model’s\\napproach to answering may vary depending on task-specific\\ncriteria, allowing it to either draw upon its inherent parametric\\nknowledge or restrict its responses to the information con-\\ntained within the provided documents. In cases of ongoing\\ndialogues, any existing conversational history can be integrated\\ninto the prompt, enabling the model to engage in multi-turn\\ndialogue interactions effectively.\\nHowever, Naive RAG encounters notable drawbacks:\\nRetrieval Challenges . The retrieval phase often struggles\\nwith precision and recall, leading to the selection of misaligned\\nor irrelevant chunks, and the missing of crucial information.\\nGeneration Difficulties. In generating responses, the model\\nmay face the issue of hallucination, where it produces con-\\ntent not supported by the retrieved context. This phase can'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='Generation Difficulties. In generating responses, the model\\nmay face the issue of hallucination, where it produces con-\\ntent not supported by the retrieved context. This phase can\\nalso suffer from irrelevance, toxicity, or bias in the outputs,\\ndetracting from the quality and reliability of the responses.\\nAugmentation Hurdles . Integrating retrieved information\\nwith the different task can be challenging, sometimes resulting\\nin disjointed or incoherent outputs. The process may also\\nencounter redundancy when similar information is retrieved\\nfrom multiple sources, leading to repetitive responses. Deter-\\nmining the significance and relevance of various passages and\\nensuring stylistic and tonal consistency add further complexity.\\nFacing complex issues, a single retrieval based on the original\\nquery may not suffice to acquire adequate context information.\\nMoreover, there’s a concern that generation models might\\noverly rely on augmented information, leading to outputs that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 2, 'page_label': '3', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='query may not suffice to acquire adequate context information.\\nMoreover, there’s a concern that generation models might\\noverly rely on augmented information, leading to outputs that\\nsimply echo retrieved content without adding insightful or\\nsynthesized information.\\nB. Advanced RAG\\nAdvanced RAG introduces specific improvements to over-\\ncome the limitations of Naive RAG. Focusing on enhancing re-\\ntrieval quality, it employs pre-retrieval and post-retrieval strate-\\ngies. To tackle the indexing issues, Advanced RAG refines\\nits indexing techniques through the use of a sliding window\\napproach, fine-grained segmentation, and the incorporation of\\nmetadata. Additionally, it incorporates several optimization\\nmethods to streamline the retrieval process [8].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='4\\nFig. 3. Comparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)\\nAdvanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a\\nchain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the\\nintroduction of multiple specific functional modules and the replacement of existing modules. The overall process is not limited to sequential retrieval and\\ngeneration; it includes methods such as iterative and adaptive retrieval.\\nPre-retrieval process. In this stage, the primary focus is\\non optimizing the indexing structure and the original query.\\nThe goal of optimizing indexing is to enhance the quality of\\nthe content being indexed. This involves strategies: enhancing\\ndata granularity, optimizing index structures, adding metadata,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='The goal of optimizing indexing is to enhance the quality of\\nthe content being indexed. This involves strategies: enhancing\\ndata granularity, optimizing index structures, adding metadata,\\nalignment optimization, and mixed retrieval. While the goal\\nof query optimization is to make the user’s original question\\nclearer and more suitable for the retrieval task. Common\\nmethods include query rewriting query transformation, query\\nexpansion and other techniques [7], [9]–[11].\\nPost-Retrieval Process. Once relevant context is retrieved,\\nit’s crucial to integrate it effectively with the query. The main\\nmethods in post-retrieval process include rerank chunks and\\ncontext compressing. Re-ranking the retrieved information to\\nrelocate the most relevant content to the edges of the prompt is\\na key strategy. This concept has been implemented in frame-\\nworks such as LlamaIndex 2, LangChain3, and HayStack [12].\\nFeeding all relevant documents directly into LLMs can lead'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='a key strategy. This concept has been implemented in frame-\\nworks such as LlamaIndex 2, LangChain3, and HayStack [12].\\nFeeding all relevant documents directly into LLMs can lead\\nto information overload, diluting the focus on key details with\\nirrelevant content.To mitigate this, post-retrieval efforts con-\\ncentrate on selecting the essential information, emphasizing\\ncritical sections, and shortening the context to be processed.\\n2https://www.llamaindex.ai\\n3https://www.langchain.com/\\nC. Modular RAG\\nThe modular RAG architecture advances beyond the for-\\nmer two RAG paradigms, offering enhanced adaptability and\\nversatility. It incorporates diverse strategies for improving its\\ncomponents, such as adding a search module for similarity\\nsearches and refining the retriever through fine-tuning. Inno-\\nvations like restructured RAG modules [13] and rearranged\\nRAG pipelines [14] have been introduced to tackle specific\\nchallenges. The shift towards a modular RAG approach is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='vations like restructured RAG modules [13] and rearranged\\nRAG pipelines [14] have been introduced to tackle specific\\nchallenges. The shift towards a modular RAG approach is\\nbecoming prevalent, supporting both sequential processing and\\nintegrated end-to-end training across its components. Despite\\nits distinctiveness, Modular RAG builds upon the foundational\\nprinciples of Advanced and Naive RAG, illustrating a progres-\\nsion and refinement within the RAG family.\\n1) New Modules: The Modular RAG framework introduces\\nadditional specialized components to enhance retrieval and\\nprocessing capabilities. The Search module adapts to spe-\\ncific scenarios, enabling direct searches across various data\\nsources like search engines, databases, and knowledge graphs,\\nusing LLM-generated code and query languages [15]. RAG-\\nFusion addresses traditional search limitations by employing\\na multi-query strategy that expands user queries into diverse'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 3, 'page_label': '4', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='using LLM-generated code and query languages [15]. RAG-\\nFusion addresses traditional search limitations by employing\\na multi-query strategy that expands user queries into diverse\\nperspectives, utilizing parallel vector searches and intelligent\\nre-ranking to uncover both explicit and transformative knowl-\\nedge [16]. The Memory module leverages the LLM’s memory\\nto guide retrieval, creating an unbounded memory pool that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='5\\naligns the text more closely with data distribution through iter-\\native self-enhancement [17], [18]. Routing in the RAG system\\nnavigates through diverse data sources, selecting the optimal\\npathway for a query, whether it involves summarization,\\nspecific database searches, or merging different information\\nstreams [19]. The Predict module aims to reduce redundancy\\nand noise by generating context directly through the LLM,\\nensuring relevance and accuracy [13]. Lastly, the Task Adapter\\nmodule tailors RAG to various downstream tasks, automating\\nprompt retrieval for zero-shot inputs and creating task-specific\\nretrievers through few-shot query generation [20], [21] .This\\ncomprehensive approach not only streamlines the retrieval pro-\\ncess but also significantly improves the quality and relevance\\nof the information retrieved, catering to a wide array of tasks\\nand queries with enhanced precision and flexibility.\\n2) New Patterns: Modular RAG offers remarkable adapt-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='of the information retrieved, catering to a wide array of tasks\\nand queries with enhanced precision and flexibility.\\n2) New Patterns: Modular RAG offers remarkable adapt-\\nability by allowing module substitution or reconfiguration\\nto address specific challenges. This goes beyond the fixed\\nstructures of Naive and Advanced RAG, characterized by a\\nsimple “Retrieve” and “Read” mechanism. Moreover, Modular\\nRAG expands this flexibility by integrating new modules or\\nadjusting interaction flow among existing ones, enhancing its\\napplicability across different tasks.\\nInnovations such as the Rewrite-Retrieve-Read [7]model\\nleverage the LLM’s capabilities to refine retrieval queries\\nthrough a rewriting module and a LM-feedback mechanism\\nto update rewriting model., improving task performance.\\nSimilarly, approaches like Generate-Read [13] replace tradi-\\ntional retrieval with LLM-generated content, while Recite-\\nRead [22] emphasizes retrieval from model weights, enhanc-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='Similarly, approaches like Generate-Read [13] replace tradi-\\ntional retrieval with LLM-generated content, while Recite-\\nRead [22] emphasizes retrieval from model weights, enhanc-\\ning the model’s ability to handle knowledge-intensive tasks.\\nHybrid retrieval strategies integrate keyword, semantic, and\\nvector searches to cater to diverse queries. Additionally, em-\\nploying sub-queries and hypothetical document embeddings\\n(HyDE) [11] seeks to improve retrieval relevance by focusing\\non embedding similarities between generated answers and real\\ndocuments.\\nAdjustments in module arrangement and interaction, such\\nas the Demonstrate-Search-Predict (DSP) [23] framework\\nand the iterative Retrieve-Read-Retrieve-Read flow of ITER-\\nRETGEN [14], showcase the dynamic use of module out-\\nputs to bolster another module’s functionality, illustrating a\\nsophisticated understanding of enhancing module synergy.\\nThe flexible orchestration of Modular RAG Flow showcases'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='puts to bolster another module’s functionality, illustrating a\\nsophisticated understanding of enhancing module synergy.\\nThe flexible orchestration of Modular RAG Flow showcases\\nthe benefits of adaptive retrieval through techniques such as\\nFLARE [24] and Self-RAG [25]. This approach transcends\\nthe fixed RAG retrieval process by evaluating the necessity\\nof retrieval based on different scenarios. Another benefit of\\na flexible architecture is that the RAG system can more\\neasily integrate with other technologies (such as fine-tuning\\nor reinforcement learning) [26]. For example, this can involve\\nfine-tuning the retriever for better retrieval results, fine-tuning\\nthe generator for more personalized outputs, or engaging in\\ncollaborative fine-tuning [27].\\nD. RAG vs Fine-tuning\\nThe augmentation of LLMs has attracted considerable atten-\\ntion due to their growing prevalence. Among the optimization\\nmethods for LLMs, RAG is often compared with Fine-tuning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='D. RAG vs Fine-tuning\\nThe augmentation of LLMs has attracted considerable atten-\\ntion due to their growing prevalence. Among the optimization\\nmethods for LLMs, RAG is often compared with Fine-tuning\\n(FT) and prompt engineering. Each method has distinct charac-\\nteristics as illustrated in Figure 4. We used a quadrant chart to\\nillustrate the differences among three methods in two dimen-\\nsions: external knowledge requirements and model adaption\\nrequirements. Prompt engineering leverages a model’s inherent\\ncapabilities with minimum necessity for external knowledge\\nand model adaption. RAG can be likened to providing a model\\nwith a tailored textbook for information retrieval, ideal for pre-\\ncise information retrieval tasks. In contrast, FT is comparable\\nto a student internalizing knowledge over time, suitable for\\nscenarios requiring replication of specific structures, styles, or\\nformats.\\nRAG excels in dynamic environments by offering real-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='to a student internalizing knowledge over time, suitable for\\nscenarios requiring replication of specific structures, styles, or\\nformats.\\nRAG excels in dynamic environments by offering real-\\ntime knowledge updates and effective utilization of external\\nknowledge sources with high interpretability. However, it\\ncomes with higher latency and ethical considerations regarding\\ndata retrieval. On the other hand, FT is more static, requiring\\nretraining for updates but enabling deep customization of the\\nmodel’s behavior and style. It demands significant compu-\\ntational resources for dataset preparation and training, and\\nwhile it can reduce hallucinations, it may face challenges with\\nunfamiliar data.\\nIn multiple evaluations of their performance on various\\nknowledge-intensive tasks across different topics, [28] re-\\nvealed that while unsupervised fine-tuning shows some im-\\nprovement, RAG consistently outperforms it, for both exist-\\ning knowledge encountered during training and entirely new'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='vealed that while unsupervised fine-tuning shows some im-\\nprovement, RAG consistently outperforms it, for both exist-\\ning knowledge encountered during training and entirely new\\nknowledge. Additionally, it was found that LLMs struggle\\nto learn new factual information through unsupervised fine-\\ntuning. The choice between RAG and FT depends on the\\nspecific needs for data dynamics, customization, and com-\\nputational capabilities in the application context. RAG and\\nFT are not mutually exclusive and can complement each\\nother, enhancing a model’s capabilities at different levels.\\nIn some instances, their combined use may lead to optimal\\nperformance. The optimization process involving RAG and FT\\nmay require multiple iterations to achieve satisfactory results.\\nIII. R ETRIEVAL\\nIn the context of RAG, it is crucial to efficiently retrieve\\nrelevant documents from the data source. There are several\\nkey issues involved, such as the retrieval source, retrieval'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 4, 'page_label': '5', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='III. R ETRIEVAL\\nIn the context of RAG, it is crucial to efficiently retrieve\\nrelevant documents from the data source. There are several\\nkey issues involved, such as the retrieval source, retrieval\\ngranularity, pre-processing of the retrieval, and selection of\\nthe corresponding embedding model.\\nA. Retrieval Source\\nRAG relies on external knowledge to enhance LLMs, while\\nthe type of retrieval source and the granularity of retrieval\\nunits both affect the final generation results.\\n1) Data Structure: Initially, text is s the mainstream source\\nof retrieval. Subsequently, the retrieval source expanded to in-\\nclude semi-structured data (PDF) and structured data (Knowl-\\nedge Graph, KG) for enhancement. In addition to retrieving\\nfrom original external sources, there is also a growing trend in\\nrecent researches towards utilizing content generated by LLMs\\nthemselves for retrieval and enhancement purposes.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='6\\nTABLE I\\nSUMMARY OF RAG METHODS\\nMethod Retrieval Source Retrieval\\nData Type\\nRetrieval\\nGranularity\\nAugmentation\\nStage\\nRetrieval\\nprocess\\nCoG [29] Wikipedia Text Phrase Pre-training Iterative\\nDenseX [30] FactoidWiki Text Proposition Inference Once\\nEAR [31] Dataset-base Text Sentence Tuning Once\\nUPRISE [20] Dataset-base Text Sentence Tuning Once\\nRAST [32] Dataset-base Text Sentence Tuning Once\\nSelf-Mem [17] Dataset-base Text Sentence Tuning Iterative\\nFLARE [24] Search Engine,Wikipedia Text Sentence Tuning Adaptive\\nPGRA [33] Wikipedia Text Sentence Inference Once\\nFILCO [34] Wikipedia Text Sentence Inference Once\\nRADA [35] Dataset-base Text Sentence Inference Once\\nFilter-rerank [36] Synthesized dataset Text Sentence Inference Once\\nR-GQA [37] Dataset-base Text Sentence Pair Tuning Once\\nLLM-R [38] Dataset-base Text Sentence Pair Inference Iterative\\nTIGER [39] Dataset-base Text Item-base Pre-training Once\\nLM-Indexer [40] Dataset-base Text Item-base Tuning Once'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='LLM-R [38] Dataset-base Text Sentence Pair Inference Iterative\\nTIGER [39] Dataset-base Text Item-base Pre-training Once\\nLM-Indexer [40] Dataset-base Text Item-base Tuning Once\\nBEQUE [9] Dataset-base Text Item-base Tuning Once\\nCT-RAG [41] Synthesized dataset Text Item-base Tuning Once\\nAtlas [42] Wikipedia, Common Crawl Text Chunk Pre-training Iterative\\nRA VEN [43] Wikipedia Text Chunk Pre-training Once\\nRETRO++ [44] Pre-training Corpus Text Chunk Pre-training Iterative\\nINSTRUCTRETRO [45] Pre-training corpus Text Chunk Pre-training Iterative\\nRRR [7] Search Engine Text Chunk Tuning Once\\nRA-e2e [46] Dataset-base Text Chunk Tuning Once\\nPROMPTAGATOR [21] BEIR Text Chunk Tuning Once\\nAAR [47] MSMARCO,Wikipedia Text Chunk Tuning Once\\nRA-DIT [27] Common Crawl,Wikipedia Text Chunk Tuning Once\\nRAG-Robust [48] Wikipedia Text Chunk Tuning Once\\nRA-Long-Form [49] Dataset-base Text Chunk Tuning Once\\nCoN [50] Wikipedia Text Chunk Tuning Once\\nSelf-RAG [25] Wikipedia Text Chunk Tuning Adaptive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='RAG-Robust [48] Wikipedia Text Chunk Tuning Once\\nRA-Long-Form [49] Dataset-base Text Chunk Tuning Once\\nCoN [50] Wikipedia Text Chunk Tuning Once\\nSelf-RAG [25] Wikipedia Text Chunk Tuning Adaptive\\nBGM [26] Wikipedia Text Chunk Inference Once\\nCoQ [51] Wikipedia Text Chunk Inference Iterative\\nToken-Elimination [52] Wikipedia Text Chunk Inference Once\\nPaperQA [53] Arxiv,Online Database,PubMed Text Chunk Inference Iterative\\nNoiseRAG [54] FactoidWiki Text Chunk Inference Once\\nIAG [55] Search Engine,Wikipedia Text Chunk Inference Once\\nNoMIRACL [56] Wikipedia Text Chunk Inference Once\\nToC [57] Search Engine,Wikipedia Text Chunk Inference Recursive\\nSKR [58] Dataset-base,Wikipedia Text Chunk Inference Adaptive\\nITRG [59] Wikipedia Text Chunk Inference Iterative\\nRAG-LongContext [60] Dataset-base Text Chunk Inference Once\\nITER-RETGEN [14] Wikipedia Text Chunk Inference Iterative\\nIRCoT [61] Wikipedia Text Chunk Inference Recursive\\nLLM-Knowledge-Boundary [62] Wikipedia Text Chunk Inference Once'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='ITER-RETGEN [14] Wikipedia Text Chunk Inference Iterative\\nIRCoT [61] Wikipedia Text Chunk Inference Recursive\\nLLM-Knowledge-Boundary [62] Wikipedia Text Chunk Inference Once\\nRAPTOR [63] Dataset-base Text Chunk Inference Recursive\\nRECITE [22] LLMs Text Chunk Inference Once\\nICRALM [64] Pile,Wikipedia Text Chunk Inference Iterative\\nRetrieve-and-Sample [65] Dataset-base Text Doc Tuning Once\\nZemi [66] C4 Text Doc Tuning Once\\nCRAG [67] Arxiv Text Doc Inference Once\\n1-PAGER [68] Wikipedia Text Doc Inference Iterative\\nPRCA [69] Dataset-base Text Doc Inference Once\\nQLM-Doc-ranking [70] Dataset-base Text Doc Inference Once\\nRecomp [71] Wikipedia Text Doc Inference Once\\nDSP [23] Wikipedia Text Doc Inference Iterative\\nRePLUG [72] Pile Text Doc Inference Once\\nARM-RAG [73] Dataset-base Text Doc Inference Iterative\\nGenRead [13] LLMs Text Doc Inference Iterative\\nUniMS-RAG [74] Dataset-base Text Multi Tuning Once\\nCREA-ICL [19] Dataset-base Crosslingual,Text Sentence Inference Once'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 5, 'page_label': '6', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='GenRead [13] LLMs Text Doc Inference Iterative\\nUniMS-RAG [74] Dataset-base Text Multi Tuning Once\\nCREA-ICL [19] Dataset-base Crosslingual,Text Sentence Inference Once\\nPKG [75] LLM Tabular,Text Chunk Inference Once\\nSANTA [76] Dataset-base Code,Text Item Pre-training Once\\nSURGE [77] Freebase KG Sub-Graph Tuning Once\\nMK-ToD [78] Dataset-base KG Entity Tuning Once\\nDual-Feedback-ToD [79] Dataset-base KG Entity Sequence Tuning Once\\nKnowledGPT [15] Dataset-base KG Triplet Inference Muti-time\\nFABULA [80] Dataset-base,Graph KG Entity Inference Once\\nHyKGE [81] CMeKG KG Entity Inference Once\\nKALMV [82] Wikipedia KG Triplet Inference Iterative\\nRoG [83] Freebase KG Triplet Inference Iterative\\nG-Retriever [84] Dataset-base TextGraph Sub-Graph Inference Once'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='7\\nFig. 4. RAG compared with other model optimization methods in the aspects of “External Knowledge Required” and “Model Adaption Required”. Prompt\\nEngineering requires low modifications to the model and external knowledge, focusing on harnessing the capabilities of LLMs themselves. Fine-tuning, on\\nthe other hand, involves further training the model. In the early stages of RAG (Naive RAG), there is a low demand for model modifications. As research\\nprogresses, Modular RAG has become more integrated with fine-tuning techniques.\\nUnstructured Data , such as text, is the most widely used\\nretrieval source, which are mainly gathered from corpus. For\\nopen-domain question-answering (ODQA) tasks, the primary\\nretrieval sources are Wikipedia Dump with the current major\\nversions including HotpotQA 4 (1st October , 2017), DPR5 (20\\nDecember, 2018). In addition to encyclopedic data, common\\nunstructured data includes cross-lingual text [19] and domain-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='versions including HotpotQA 4 (1st October , 2017), DPR5 (20\\nDecember, 2018). In addition to encyclopedic data, common\\nunstructured data includes cross-lingual text [19] and domain-\\nspecific data (such as medical [67]and legal domains [29]).\\nSemi-structured data. typically refers to data that contains a\\ncombination of text and table information, such as PDF. Han-\\ndling semi-structured data poses challenges for conventional\\nRAG systems due to two main reasons. Firstly, text splitting\\nprocesses may inadvertently separate tables, leading to data\\ncorruption during retrieval. Secondly, incorporating tables into\\nthe data can complicate semantic similarity searches. When\\ndealing with semi-structured data, one approach involves lever-\\naging the code capabilities of LLMs to execute Text-2-SQL\\nqueries on tables within databases, such as TableGPT [85].\\nAlternatively, tables can be transformed into text format for\\nfurther analysis using text-based methods [75]. However, both'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='queries on tables within databases, such as TableGPT [85].\\nAlternatively, tables can be transformed into text format for\\nfurther analysis using text-based methods [75]. However, both\\nof these methods are not optimal solutions, indicating substan-\\ntial research opportunities in this area.\\nStructured data , such as knowledge graphs (KGs) [86] ,\\nwhich are typically verified and can provide more precise in-\\nformation. KnowledGPT [15] generates KB search queries and\\nstores knowledge in a personalized base, enhancing the RAG\\nmodel’s knowledge richness. In response to the limitations of\\nLLMs in understanding and answering questions about textual\\ngraphs, G-Retriever [84] integrates Graph Neural Networks\\n4https://hotpotqa.github.io/wiki-readme.html\\n5https://github.com/facebookresearch/DPR\\n(GNNs), LLMs and RAG, enhancing graph comprehension\\nand question-answering capabilities through soft prompting\\nof the LLM, and employs the Prize-Collecting Steiner Tree'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='(GNNs), LLMs and RAG, enhancing graph comprehension\\nand question-answering capabilities through soft prompting\\nof the LLM, and employs the Prize-Collecting Steiner Tree\\n(PCST) optimization problem for targeted graph retrieval. On\\nthe contrary, it requires additional effort to build, validate,\\nand maintain structured databases. On the contrary, it requires\\nadditional effort to build, validate, and maintain structured\\ndatabases.\\nLLMs-Generated Content. Addressing the limitations of\\nexternal auxiliary information in RAG, some research has\\nfocused on exploiting LLMs’ internal knowledge. SKR [58]\\nclassifies questions as known or unknown, applying retrieval\\nenhancement selectively. GenRead [13] replaces the retriever\\nwith an LLM generator, finding that LLM-generated contexts\\noften contain more accurate answers due to better alignment\\nwith the pre-training objectives of causal language modeling.\\nSelfmem [17] iteratively creates an unbounded memory pool'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='often contain more accurate answers due to better alignment\\nwith the pre-training objectives of causal language modeling.\\nSelfmem [17] iteratively creates an unbounded memory pool\\nwith a retrieval-enhanced generator, using a memory selec-\\ntor to choose outputs that serve as dual problems to the\\noriginal question, thus self-enhancing the generative model.\\nThese methodologies underscore the breadth of innovative\\ndata source utilization in RAG, striving to improve model\\nperformance and task effectiveness.\\n2) Retrieval Granularity: Another important factor besides\\nthe data format of the retrieval source is the granularity of\\nthe retrieved data. Coarse-grained retrieval units theoretically\\ncan provide more relevant information for the problem, but\\nthey may also contain redundant content, which could distract\\nthe retriever and language models in downstream tasks [50],\\n[87]. On the other hand, fine-grained retrieval unit granularity'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 6, 'page_label': '7', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='they may also contain redundant content, which could distract\\nthe retriever and language models in downstream tasks [50],\\n[87]. On the other hand, fine-grained retrieval unit granularity\\nincreases the burden of retrieval and does not guarantee seman-\\ntic integrity and meeting the required knowledge. Choosing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='8\\nthe appropriate retrieval granularity during inference can be\\na simple and effective strategy to improve the retrieval and\\ndownstream task performance of dense retrievers.\\nIn text, retrieval granularity ranges from fine to coarse,\\nincluding Token, Phrase, Sentence, Proposition, Chunks, Doc-\\nument. Among them, DenseX [30]proposed the concept of\\nusing propositions as retrieval units. Propositions are defined\\nas atomic expressions in the text, each encapsulating a unique\\nfactual segment and presented in a concise, self-contained nat-\\nural language format. This approach aims to enhance retrieval\\nprecision and relevance. On the Knowledge Graph (KG),\\nretrieval granularity includes Entity, Triplet, and sub-Graph.\\nThe granularity of retrieval can also be adapted to downstream\\ntasks, such as retrieving Item IDs [40]in recommendation tasks\\nand Sentence pairs [38]. Detailed information is illustrated in\\nTable I.\\nB. Indexing Optimization\\nIn the Indexing phase, documents will be processed, seg-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='and Sentence pairs [38]. Detailed information is illustrated in\\nTable I.\\nB. Indexing Optimization\\nIn the Indexing phase, documents will be processed, seg-\\nmented, and transformed into Embeddings to be stored in a\\nvector database. The quality of index construction determines\\nwhether the correct context can be obtained in the retrieval\\nphase.\\n1) Chunking Strategy: The most common method is to split\\nthe document into chunks on a fixed number of tokens (e.g.,\\n100, 256, 512) [88]. Larger chunks can capture more context,\\nbut they also generate more noise, requiring longer processing\\ntime and higher costs. While smaller chunks may not fully\\nconvey the necessary context, they do have less noise. How-\\never, chunks leads to truncation within sentences, prompting\\nthe optimization of a recursive splits and sliding window meth-\\nods, enabling layered retrieval by merging globally related\\ninformation across multiple retrieval processes [89]. Never-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='the optimization of a recursive splits and sliding window meth-\\nods, enabling layered retrieval by merging globally related\\ninformation across multiple retrieval processes [89]. Never-\\ntheless, these approaches still cannot strike a balance between\\nsemantic completeness and context length. Therefore, methods\\nlike Small2Big have been proposed, where sentences (small)\\nare used as the retrieval unit, and the preceding and following\\nsentences are provided as (big) context to LLMs [90].\\n2) Metadata Attachments: Chunks can be enriched with\\nmetadata information such as page number, file name, au-\\nthor,category timestamp. Subsequently, retrieval can be filtered\\nbased on this metadata, limiting the scope of the retrieval.\\nAssigning different weights to document timestamps during\\nretrieval can achieve time-aware RAG, ensuring the freshness\\nof knowledge and avoiding outdated information.\\nIn addition to extracting metadata from the original doc-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='retrieval can achieve time-aware RAG, ensuring the freshness\\nof knowledge and avoiding outdated information.\\nIn addition to extracting metadata from the original doc-\\numents, metadata can also be artificially constructed. For\\nexample, adding summaries of paragraph, as well as intro-\\nducing hypothetical questions. This method is also known as\\nReverse HyDE. Specifically, using LLM to generate questions\\nthat can be answered by the document, then calculating the\\nsimilarity between the original question and the hypothetical\\nquestion during retrieval to reduce the semantic gap between\\nthe question and the answer.\\n3) Structural Index: One effective method for enhancing\\ninformation retrieval is to establish a hierarchical structure for\\nthe documents. By constructing In structure, RAG system can\\nexpedite the retrieval and processing of pertinent data.\\nHierarchical index structure . File are arranged in parent-\\nchild relationships, with chunks linked to them. Data sum-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='expedite the retrieval and processing of pertinent data.\\nHierarchical index structure . File are arranged in parent-\\nchild relationships, with chunks linked to them. Data sum-\\nmaries are stored at each node, aiding in the swift traversal\\nof data and assisting the RAG system in determining which\\nchunks to extract. This approach can also mitigate the illusion\\ncaused by block extraction issues.\\nKnowledge Graph index . Utilize KG in constructing the\\nhierarchical structure of documents contributes to maintaining\\nconsistency. It delineates the connections between different\\nconcepts and entities, markedly reducing the potential for\\nillusions. Another advantage is the transformation of the\\ninformation retrieval process into instructions that LLM can\\ncomprehend, thereby enhancing the accuracy of knowledge\\nretrieval and enabling LLM to generate contextually coherent\\nresponses, thus improving the overall efficiency of the RAG\\nsystem. To capture the logical relationship between document'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='retrieval and enabling LLM to generate contextually coherent\\nresponses, thus improving the overall efficiency of the RAG\\nsystem. To capture the logical relationship between document\\ncontent and structure, KGP [91] proposed a method of building\\nan index between multiple documents using KG. This KG\\nconsists of nodes (representing paragraphs or structures in the\\ndocuments, such as pages and tables) and edges (indicating\\nsemantic/lexical similarity between paragraphs or relationships\\nwithin the document structure), effectively addressing knowl-\\nedge retrieval and reasoning problems in a multi-document\\nenvironment.\\nC. Query Optimization\\nOne of the primary challenges with Naive RAG is its\\ndirect reliance on the user’s original query as the basis for\\nretrieval. Formulating a precise and clear question is difficult,\\nand imprudent queries result in subpar retrieval effectiveness.\\nSometimes, the question itself is complex, and the language'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='retrieval. Formulating a precise and clear question is difficult,\\nand imprudent queries result in subpar retrieval effectiveness.\\nSometimes, the question itself is complex, and the language\\nis not well-organized. Another difficulty lies in language\\ncomplexity ambiguity. Language models often struggle when\\ndealing with specialized vocabulary or ambiguous abbrevi-\\nations with multiple meanings. For instance, they may not\\ndiscern whether “LLM” refers to large language model or a\\nMaster of Laws in a legal context.\\n1) Query Expansion: Expanding a single query into mul-\\ntiple queries enriches the content of the query, providing\\nfurther context to address any lack of specific nuances, thereby\\nensuring the optimal relevance of the generated answers.\\nMulti-Query. By employing prompt engineering to expand\\nqueries via LLMs, these queries can then be executed in\\nparallel. The expansion of queries is not random, but rather\\nmeticulously designed.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 7, 'page_label': '8', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='Multi-Query. By employing prompt engineering to expand\\nqueries via LLMs, these queries can then be executed in\\nparallel. The expansion of queries is not random, but rather\\nmeticulously designed.\\nSub-Query. The process of sub-question planning represents\\nthe generation of the necessary sub-questions to contextualize\\nand fully answer the original question when combined. This\\nprocess of adding relevant context is, in principle, similar\\nto query expansion. Specifically, a complex question can be\\ndecomposed into a series of simpler sub-questions using the\\nleast-to-most prompting method [92].\\nChain-of-Verification(CoVe). The expanded queries undergo\\nvalidation by LLM to achieve the effect of reducing halluci-\\nnations. Validated expanded queries typically exhibit higher\\nreliability [93].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='9\\n2) Query Transformation: The core concept is to retrieve\\nchunks based on a transformed query instead of the user’s\\noriginal query.\\nQuery Rewrite.The original queries are not always optimal\\nfor LLM retrieval, especially in real-world scenarios. There-\\nfore, we can prompt LLM to rewrite the queries. In addition to\\nusing LLM for query rewriting, specialized smaller language\\nmodels, such as RRR (Rewrite-retrieve-read) [7]. The imple-\\nmentation of the query rewrite method in the Taobao, known\\nas BEQUE [9] has notably enhanced recall effectiveness for\\nlong-tail queries, resulting in a rise in GMV .\\nAnother query transformation method is to use prompt\\nengineering to let LLM generate a query based on the original\\nquery for subsequent retrieval. HyDE [11] construct hypothet-\\nical documents (assumed answers to the original query). It\\nfocuses on embedding similarity from answer to answer rather\\nthan seeking embedding similarity for the problem or query.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='ical documents (assumed answers to the original query). It\\nfocuses on embedding similarity from answer to answer rather\\nthan seeking embedding similarity for the problem or query.\\nUsing the Step-back Prompting method [10], the original\\nquery is abstracted to generate a high-level concept question\\n(step-back question). In the RAG system, both the step-back\\nquestion and the original query are used for retrieval, and both\\nthe results are utilized as the basis for language model answer\\ngeneration.\\n3) Query Routing: Based on varying queries, routing to\\ndistinct RAG pipeline,which is suitable for a versatile RAG\\nsystem designed to accommodate diverse scenarios.\\nMetadata Router/ Filter . The first step involves extracting\\nkeywords (entity) from the query, followed by filtering based\\non the keywords and metadata within the chunks to narrow\\ndown the search scope.\\nSemantic Router is another method of routing involves\\nleveraging the semantic information of the query. Specific'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='on the keywords and metadata within the chunks to narrow\\ndown the search scope.\\nSemantic Router is another method of routing involves\\nleveraging the semantic information of the query. Specific\\napprach see Semantic Router 6. Certainly, a hybrid routing\\napproach can also be employed, combining both semantic and\\nmetadata-based methods for enhanced query routing.\\nD. Embedding\\nIn RAG, retrieval is achieved by calculating the similarity\\n(e.g. cosine similarity) between the embeddings of the ques-\\ntion and document chunks, where the semantic representation\\ncapability of embedding models plays a key role. This mainly\\nincludes a sparse encoder (BM25) and a dense retriever (BERT\\narchitecture Pre-training language models). Recent research\\nhas introduced prominent embedding models such as AngIE,\\nV oyage, BGE,etc [94]–[96], which are benefit from multi-task\\ninstruct tuning. Hugging Face’s MTEB leaderboard 7 evaluates\\nembedding models across 8 tasks, covering 58 datasests. Ad-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='V oyage, BGE,etc [94]–[96], which are benefit from multi-task\\ninstruct tuning. Hugging Face’s MTEB leaderboard 7 evaluates\\nembedding models across 8 tasks, covering 58 datasests. Ad-\\nditionally, C-MTEB focuses on Chinese capability, covering\\n6 tasks and 35 datasets. There is no one-size-fits-all answer\\nto “which embedding model to use.” However, some specific\\nmodels are better suited for particular use cases.\\n1) Mix/hybrid Retrieval : Sparse and dense embedding\\napproaches capture different relevance features and can ben-\\nefit from each other by leveraging complementary relevance\\ninformation. For instance, sparse retrieval models can be used\\n6https://github.com/aurelio-labs/semantic-router\\n7https://huggingface.co/spaces/mteb/leaderboard\\nto provide initial search results for training dense retrieval\\nmodels. Additionally, pre-training language models (PLMs)\\ncan be utilized to learn term weights to enhance sparse\\nretrieval. Specifically, it also demonstrates that sparse retrieval'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='models. Additionally, pre-training language models (PLMs)\\ncan be utilized to learn term weights to enhance sparse\\nretrieval. Specifically, it also demonstrates that sparse retrieval\\nmodels can enhance the zero-shot retrieval capability of dense\\nretrieval models and assist dense retrievers in handling queries\\ncontaining rare entities, thereby improving robustness.\\n2) Fine-tuning Embedding Model: In instances where the\\ncontext significantly deviates from pre-training corpus, partic-\\nularly within highly specialized disciplines such as healthcare,\\nlegal practice, and other sectors replete with proprietary jargon,\\nfine-tuning the embedding model on your own domain dataset\\nbecomes essential to mitigate such discrepancies.\\nIn addition to supplementing domain knowledge, another\\npurpose of fine-tuning is to align the retriever and generator,\\nfor example, using the results of LLM as the supervision signal\\nfor fine-tuning, known as LSR (LM-supervised Retriever).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='purpose of fine-tuning is to align the retriever and generator,\\nfor example, using the results of LLM as the supervision signal\\nfor fine-tuning, known as LSR (LM-supervised Retriever).\\nPROMPTAGATOR [21] utilizes the LLM as a few-shot query\\ngenerator to create task-specific retrievers, addressing chal-\\nlenges in supervised fine-tuning, particularly in data-scarce\\ndomains. Another approach, LLM-Embedder [97], exploits\\nLLMs to generate reward signals across multiple downstream\\ntasks. The retriever is fine-tuned with two types of supervised\\nsignals: hard labels for the dataset and soft rewards from\\nthe LLMs. This dual-signal approach fosters a more effective\\nfine-tuning process, tailoring the embedding model to diverse\\ndownstream applications. REPLUG [72] utilizes a retriever\\nand an LLM to calculate the probability distributions of the\\nretrieved documents and then performs supervised training\\nby computing the KL divergence. This straightforward and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='and an LLM to calculate the probability distributions of the\\nretrieved documents and then performs supervised training\\nby computing the KL divergence. This straightforward and\\neffective training method enhances the performance of the\\nretrieval model by using an LM as the supervisory signal,\\neliminating the need for specific cross-attention mechanisms.\\nMoreover, inspired by RLHF (Reinforcement Learning from\\nHuman Feedback), utilizing LM-based feedback to reinforce\\nthe retriever through reinforcement learning.\\nE. Adapter\\nFine-tuning models may present challenges, such as in-\\ntegrating functionality through an API or addressing con-\\nstraints arising from limited local computational resources.\\nConsequently, some approaches opt to incorporate an external\\nadapter to aid in alignment.\\nTo optimize the multi-task capabilities of LLM, UP-\\nRISE [20] trained a lightweight prompt retriever that can\\nautomatically retrieve prompts from a pre-built prompt pool'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 8, 'page_label': '9', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='adapter to aid in alignment.\\nTo optimize the multi-task capabilities of LLM, UP-\\nRISE [20] trained a lightweight prompt retriever that can\\nautomatically retrieve prompts from a pre-built prompt pool\\nthat are suitable for a given zero-shot task input. AAR\\n(Augmentation-Adapted Retriver) [47] introduces a universal\\nadapter designed to accommodate multiple downstream tasks.\\nWhile PRCA [69] add a pluggable reward-driven contextual\\nadapter to enhance performance on specific tasks. BGM [26]\\nkeeps the retriever and LLM fixed,and trains a bridge Seq2Seq\\nmodel in between. The bridge model aims to transform the\\nretrieved information into a format that LLMs can work with\\neffectively, allowing it to not only rerank but also dynami-\\ncally select passages for each query, and potentially employ\\nmore advanced strategies like repetition. Furthermore, PKG'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='10\\nintroduces an innovative method for integrating knowledge\\ninto white-box models via directive fine-tuning [75]. In this\\napproach, the retriever module is directly substituted to gen-\\nerate relevant documents according to a query. This method\\nassists in addressing the difficulties encountered during the\\nfine-tuning process and enhances model performance.\\nIV. G ENERATION\\nAfter retrieval, it is not a good practice to directly input all\\nthe retrieved information to the LLM for answering questions.\\nFollowing will introduce adjustments from two perspectives:\\nadjusting the retrieved content and adjusting the LLM.\\nA. Context Curation\\nRedundant information can interfere with the final gener-\\nation of LLM, and overly long contexts can also lead LLM\\nto the “Lost in the middle” problem [98]. Like humans, LLM\\ntends to only focus on the beginning and end of long texts,\\nwhile forgetting the middle portion. Therefore, in the RAG\\nsystem, we typically need to further process the retrieved\\ncontent.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='tends to only focus on the beginning and end of long texts,\\nwhile forgetting the middle portion. Therefore, in the RAG\\nsystem, we typically need to further process the retrieved\\ncontent.\\n1) Reranking: Reranking fundamentally reorders document\\nchunks to highlight the most pertinent results first, effectively\\nreducing the overall document pool, severing a dual purpose\\nin information retrieval, acting as both an enhancer and a\\nfilter, delivering refined inputs for more precise language\\nmodel processing [70]. Reranking can be performed using\\nrule-based methods that depend on predefined metrics like\\nDiversity, Relevance, and MRR, or model-based approaches\\nlike Encoder-Decoder models from the BERT series (e.g.,\\nSpanBERT), specialized reranking models such as Cohere\\nrerank or bge-raranker-large, and general large language mod-\\nels like GPT [12], [99].\\n2) Context Selection/Compression: A common misconcep-\\ntion in the RAG process is the belief that retrieving as many'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='els like GPT [12], [99].\\n2) Context Selection/Compression: A common misconcep-\\ntion in the RAG process is the belief that retrieving as many\\nrelevant documents as possible and concatenating them to form\\na lengthy retrieval prompt is beneficial. However, excessive\\ncontext can introduce more noise, diminishing the LLM’s\\nperception of key information .\\n(Long) LLMLingua [100], [101] utilize small language\\nmodels (SLMs) such as GPT-2 Small or LLaMA-7B, to\\ndetect and remove unimportant tokens, transforming it into\\na form that is challenging for humans to comprehend but\\nwell understood by LLMs. This approach presents a direct\\nand practical method for prompt compression, eliminating the\\nneed for additional training of LLMs while balancing language\\nintegrity and compression ratio. PRCA tackled this issue by\\ntraining an information extractor [69]. Similarly, RECOMP\\nadopts a comparable approach by training an information\\ncondenser using contrastive learning [71]. Each training data'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='training an information extractor [69]. Similarly, RECOMP\\nadopts a comparable approach by training an information\\ncondenser using contrastive learning [71]. Each training data\\npoint consists of one positive sample and five negative sam-\\nples, and the encoder undergoes training using contrastive loss\\nthroughout this process [102] .\\nIn addition to compressing the context, reducing the num-\\nber of documents aslo helps improve the accuracy of the\\nmodel’s answers. Ma et al. [103] propose the “Filter-Reranker”\\nparadigm, which combines the strengths of LLMs and SLMs.\\nIn this paradigm, SLMs serve as filters, while LLMs function\\nas reordering agents. The research shows that instructing\\nLLMs to rearrange challenging samples identified by SLMs\\nleads to significant improvements in various Information\\nExtraction (IE) tasks. Another straightforward and effective\\napproach involves having the LLM evaluate the retrieved\\ncontent before generating the final answer. This allows the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='Extraction (IE) tasks. Another straightforward and effective\\napproach involves having the LLM evaluate the retrieved\\ncontent before generating the final answer. This allows the\\nLLM to filter out documents with poor relevance through LLM\\ncritique. For instance, in Chatlaw [104], the LLM is prompted\\nto self-suggestion on the referenced legal provisions to assess\\ntheir relevance.\\nB. LLM Fine-tuning\\nTargeted fine-tuning based on the scenario and data char-\\nacteristics on LLMs can yield better results. This is also one\\nof the greatest advantages of using on-premise LLMs. When\\nLLMs lack data in a specific domain, additional knowledge can\\nbe provided to the LLM through fine-tuning. Huggingface’s\\nfine-tuning data can also be used as an initial step.\\nAnother benefit of fine-tuning is the ability to adjust the\\nmodel’s input and output. For example, it can enable LLM to\\nadapt to specific data formats and generate responses in a par-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='Another benefit of fine-tuning is the ability to adjust the\\nmodel’s input and output. For example, it can enable LLM to\\nadapt to specific data formats and generate responses in a par-\\nticular style as instructed [37]. For retrieval tasks that engage\\nwith structured data, the SANTA framework [76] implements\\na tripartite training regimen to effectively encapsulate both\\nstructural and semantic nuances. The initial phase focuses on\\nthe retriever, where contrastive learning is harnessed to refine\\nthe query and document embeddings.\\nAligning LLM outputs with human or retriever preferences\\nthrough reinforcement learning is a potential approach. For\\ninstance, manually annotating the final generated answers\\nand then providing feedback through reinforcement learning.\\nIn addition to aligning with human preferences, it is also\\npossible to align with the preferences of fine-tuned models\\nand retrievers [79]. When circumstances prevent access to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='In addition to aligning with human preferences, it is also\\npossible to align with the preferences of fine-tuned models\\nand retrievers [79]. When circumstances prevent access to\\npowerful proprietary models or larger parameter open-source\\nmodels, a simple and effective method is to distill the more\\npowerful models(e.g. GPT-4). Fine-tuning of LLM can also\\nbe coordinated with fine-tuning of the retriever to align pref-\\nerences. A typical approach, such as RA-DIT [27], aligns the\\nscoring functions between Retriever and Generator using KL\\ndivergence.\\nV. A UGMENTATION PROCESS IN RAG\\nIn the domain of RAG, the standard practice often involves\\na singular (once) retrieval step followed by generation, which\\ncan lead to inefficiencies and sometimes is typically insuffi-\\ncient for complex problems demanding multi-step reasoning,\\nas it provides a limited scope of information [105]. Many\\nstudies have optimized the retrieval process in response to this\\nissue, and we have summarised them in Figure 5.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 9, 'page_label': '10', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='as it provides a limited scope of information [105]. Many\\nstudies have optimized the retrieval process in response to this\\nissue, and we have summarised them in Figure 5.\\nA. Iterative Retrieval\\nIterative retrieval is a process where the knowledge base\\nis repeatedly searched based on the initial query and the text\\ngenerated so far, providing a more comprehensive knowledge'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='11\\nFig. 5. In addition to the most common once retrieval, RAG also includes three types of retrieval augmentation processes. (left) Iterative retrieval involves\\nalternating between retrieval and generation, allowing for richer and more targeted context from the knowledge base at each step. (Middle) Recursive retrieval\\ninvolves gradually refining the user query and breaking down the problem into sub-problems, then continuously solving complex problems through retrieval\\nand generation. (Right) Adaptive retrieval focuses on enabling the RAG system to autonomously determine whether external knowledge retrieval is necessary\\nand when to stop retrieval and generation, often utilizing LLM-generated special tokens for control.\\nbase for LLMs. This approach has been shown to enhance\\nthe robustness of subsequent answer generation by offering\\nadditional contextual references through multiple retrieval\\niterations. However, it may be affected by semantic discon-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='the robustness of subsequent answer generation by offering\\nadditional contextual references through multiple retrieval\\niterations. However, it may be affected by semantic discon-\\ntinuity and the accumulation of irrelevant information. ITER-\\nRETGEN [14] employs a synergistic approach that lever-\\nages “retrieval-enhanced generation” alongside “generation-\\nenhanced retrieval” for tasks that necessitate the reproduction\\nof specific information. The model harnesses the content\\nrequired to address the input task as a contextual basis for\\nretrieving pertinent knowledge, which in turn facilitates the\\ngeneration of improved responses in subsequent iterations.\\nB. Recursive Retrieval\\nRecursive retrieval is often used in information retrieval and\\nNLP to improve the depth and relevance of search results.\\nThe process involves iteratively refining search queries based\\non the results obtained from previous searches. Recursive\\nRetrieval aims to enhance the search experience by gradu-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='The process involves iteratively refining search queries based\\non the results obtained from previous searches. Recursive\\nRetrieval aims to enhance the search experience by gradu-\\nally converging on the most pertinent information through a\\nfeedback loop. IRCoT [61] uses chain-of-thought to guide\\nthe retrieval process and refines the CoT with the obtained\\nretrieval results. ToC [57] creates a clarification tree that\\nsystematically optimizes the ambiguous parts in the Query. It\\ncan be particularly useful in complex search scenarios where\\nthe user’s needs are not entirely clear from the outset or where\\nthe information sought is highly specialized or nuanced. The\\nrecursive nature of the process allows for continuous learning\\nand adaptation to the user’s requirements, often resulting in\\nimproved satisfaction with the search outcomes.\\nTo address specific data scenarios, recursive retrieval and\\nmulti-hop retrieval techniques are utilized together. Recursive'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='improved satisfaction with the search outcomes.\\nTo address specific data scenarios, recursive retrieval and\\nmulti-hop retrieval techniques are utilized together. Recursive\\nretrieval involves a structured index to process and retrieve\\ndata in a hierarchical manner, which may include summarizing\\nsections of a document or lengthy PDF before performing a\\nretrieval based on this summary. Subsequently, a secondary\\nretrieval within the document refines the search, embodying\\nthe recursive nature of the process. In contrast, multi-hop\\nretrieval is designed to delve deeper into graph-structured data\\nsources, extracting interconnected information [106].\\nC. Adaptive Retrieval\\nAdaptive retrieval methods, exemplified by Flare [24] and\\nSelf-RAG [25], refine the RAG framework by enabling LLMs\\nto actively determine the optimal moments and content for\\nretrieval, thus enhancing the efficiency and relevance of the\\ninformation sourced.\\nThese methods are part of a broader trend wherein'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='to actively determine the optimal moments and content for\\nretrieval, thus enhancing the efficiency and relevance of the\\ninformation sourced.\\nThese methods are part of a broader trend wherein\\nLLMs employ active judgment in their operations, as seen\\nin model agents like AutoGPT, Toolformer, and Graph-\\nToolformer [107]–[109]. Graph-Toolformer, for instance, di-\\nvides its retrieval process into distinct steps where LLMs\\nproactively use retrievers, apply Self-Ask techniques, and em-\\nploy few-shot prompts to initiate search queries. This proactive\\nstance allows LLMs to decide when to search for necessary\\ninformation, akin to how an agent utilizes tools.\\nWebGPT [110] integrates a reinforcement learning frame-\\nwork to train the GPT-3 model in autonomously using a\\nsearch engine during text generation. It navigates this process\\nusing special tokens that facilitate actions such as search\\nengine queries, browsing results, and citing references, thereby'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 10, 'page_label': '11', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='search engine during text generation. It navigates this process\\nusing special tokens that facilitate actions such as search\\nengine queries, browsing results, and citing references, thereby\\nexpanding GPT-3’s capabilities through the use of external\\nsearch engines. Flare automates timing retrieval by monitoring\\nthe confidence of the generation process, as indicated by the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='12\\nprobability of generated terms [24]. When the probability falls\\nbelow a certain threshold would activates the retrieval system\\nto collect relevant information, thus optimizing the retrieval\\ncycle. Self-RAG [25] introduces “reflection tokens” that allow\\nthe model to introspect its outputs. These tokens come in\\ntwo varieties: “retrieve” and “critic”. The model autonomously\\ndecides when to activate retrieval, or alternatively, a predefined\\nthreshold may trigger the process. During retrieval, the gen-\\nerator conducts a fragment-level beam search across multiple\\nparagraphs to derive the most coherent sequence. Critic scores\\nare used to update the subdivision scores, with the flexibility\\nto adjust these weights during inference, tailoring the model’s\\nbehavior. Self-RAG’s design obviates the need for additional\\nclassifiers or reliance on Natural Language Inference (NLI)\\nmodels, thus streamlining the decision-making process for\\nwhen to engage retrieval mechanisms and improving the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='classifiers or reliance on Natural Language Inference (NLI)\\nmodels, thus streamlining the decision-making process for\\nwhen to engage retrieval mechanisms and improving the\\nmodel’s autonomous judgment capabilities in generating ac-\\ncurate responses.\\nVI. T ASK AND EVALUATION\\nThe rapid advancement and growing adoption of RAG\\nin the field of NLP have propelled the evaluation of RAG\\nmodels to the forefront of research in the LLMs community.\\nThe primary objective of this evaluation is to comprehend\\nand optimize the performance of RAG models across diverse\\napplication scenarios.This chapter will mainly introduce the\\nmain downstream tasks of RAG, datasets, and how to evaluate\\nRAG systems.\\nA. Downstream Task\\nThe core task of RAG remains Question Answering (QA),\\nincluding traditional single-hop/multi-hop QA, multiple-\\nchoice, domain-specific QA as well as long-form scenarios\\nsuitable for RAG. In addition to QA, RAG is continuously\\nbeing expanded into multiple downstream tasks, such as Infor-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='choice, domain-specific QA as well as long-form scenarios\\nsuitable for RAG. In addition to QA, RAG is continuously\\nbeing expanded into multiple downstream tasks, such as Infor-\\nmation Extraction (IE), dialogue generation, code search, etc.\\nThe main downstream tasks of RAG and their corresponding\\ndatasets are summarized in Table II.\\nB. Evaluation Target\\nHistorically, RAG models assessments have centered on\\ntheir execution in specific downstream tasks. These evaluations\\nemploy established metrics suitable to the tasks at hand. For\\ninstance, question answering evaluations might rely on EM\\nand F1 scores [7], [45], [59], [72], whereas fact-checking\\ntasks often hinge on Accuracy as the primary metric [4],\\n[14], [42]. BLEU and ROUGE metrics are also commonly\\nused to evaluate answer quality [26], [32], [52], [78]. Tools\\nlike RALLE, designed for the automatic evaluation of RAG\\napplications, similarly base their assessments on these task-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='used to evaluate answer quality [26], [32], [52], [78]. Tools\\nlike RALLE, designed for the automatic evaluation of RAG\\napplications, similarly base their assessments on these task-\\nspecific metrics [160]. Despite this, there is a notable paucity\\nof research dedicated to evaluating the distinct characteristics\\nof RAG models.The main evaluation objectives include:\\nRetrieval Quality. Evaluating the retrieval quality is crucial\\nfor determining the effectiveness of the context sourced by\\nthe retriever component. Standard metrics from the domains\\nof search engines, recommendation systems, and information\\nretrieval systems are employed to measure the performance of\\nthe RAG retrieval module. Metrics such as Hit Rate, MRR, and\\nNDCG are commonly utilized for this purpose [161], [162].\\nGeneration Quality . The assessment of generation quality\\ncenters on the generator’s capacity to synthesize coherent and\\nrelevant answers from the retrieved context. This evaluation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='Generation Quality . The assessment of generation quality\\ncenters on the generator’s capacity to synthesize coherent and\\nrelevant answers from the retrieved context. This evaluation\\ncan be categorized based on the content’s objectives: unlabeled\\nand labeled content. For unlabeled content, the evaluation\\nencompasses the faithfulness, relevance, and non-harmfulness\\nof the generated answers. In contrast, for labeled content,\\nthe focus is on the accuracy of the information produced by\\nthe model [161]. Additionally, both retrieval and generation\\nquality assessments can be conducted through manual or\\nautomatic evaluation methods [29], [161], [163].\\nC. Evaluation Aspects\\nContemporary evaluation practices of RAG models empha-\\nsize three primary quality scores and four essential abilities,\\nwhich collectively inform the evaluation of the two principal\\ntargets of the RAG model: retrieval and generation.\\n1) Quality Scores: Quality scores include context rele-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='which collectively inform the evaluation of the two principal\\ntargets of the RAG model: retrieval and generation.\\n1) Quality Scores: Quality scores include context rele-\\nvance, answer faithfulness, and answer relevance. These qual-\\nity scores evaluate the efficiency of the RAG model from\\ndifferent perspectives in the process of information retrieval\\nand generation [164]–[166].\\nContext Relevance evaluates the precision and specificity\\nof the retrieved context, ensuring relevance and minimizing\\nprocessing costs associated with extraneous content.\\nAnswer Faithfulness ensures that the generated answers\\nremain true to the retrieved context, maintaining consistency\\nand avoiding contradictions.\\nAnswer Relevance requires that the generated answers are\\ndirectly pertinent to the posed questions, effectively addressing\\nthe core inquiry.\\n2) Required Abilities: RAG evaluation also encompasses\\nfour abilities indicative of its adaptability and efficiency:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='the core inquiry.\\n2) Required Abilities: RAG evaluation also encompasses\\nfour abilities indicative of its adaptability and efficiency:\\nnoise robustness, negative rejection, information integration,\\nand counterfactual robustness [167], [168]. These abilities are\\ncritical for the model’s performance under various challenges\\nand complex scenarios, impacting the quality scores.\\nNoise Robustness appraises the model’s capability to man-\\nage noise documents that are question-related but lack sub-\\nstantive information.\\nNegative Rejection assesses the model’s discernment in\\nrefraining from responding when the retrieved documents do\\nnot contain the necessary knowledge to answer a question.\\nInformation Integration evaluates the model’s proficiency in\\nsynthesizing information from multiple documents to address\\ncomplex questions.\\nCounterfactual Robustness tests the model’s ability to rec-\\nognize and disregard known inaccuracies within documents,\\neven when instructed about potential misinformation.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 11, 'page_label': '12', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='complex questions.\\nCounterfactual Robustness tests the model’s ability to rec-\\nognize and disregard known inaccuracies within documents,\\neven when instructed about potential misinformation.\\nContext relevance and noise robustness are important for\\nevaluating the quality of retrieval, while answer faithfulness,\\nanswer relevance, negative rejection, information integration,\\nand counterfactual robustness are important for evaluating the\\nquality of generation.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='13\\nTABLE II\\nDOWNSTREAM TASKS AND DATASETS OF RAG\\nTask Sub Task Dataset Method\\nQA Single-hop Natural Qustion(NQ) [111]\\n[26], [30], [34], [42], [45], [50], [52], [59], [64], [82]\\n[3], [4], [22], [27], [40], [43], [54], [62], [71], [112]\\n[20], [44], [72]\\nTriviaQA(TQA) [113]\\n[13], [30], [34], [45], [50], [64]\\n[4], [27], [59], [62], [112]\\n[22], [25], [43], [44], [71], [72]\\nSQuAD [114] [20], [23], [30], [32], [45], [69], [112]\\nWeb Questions(WebQ) [115] [3], [4], [13], [30], [50], [68]\\nPopQA [116] [7], [25], [67]\\nMS MARCO [117] [4], [40], [52]\\nMulti-hop HotpotQA [118] [23], [26], [31], [34], [47], [51], [61], [82]\\n[7], [14], [22], [27], [59], [62], [69], [71], [91]\\n2WikiMultiHopQA [119] [14], [24], [48], [59], [61], [91]\\nMuSiQue [120] [14], [51], [61], [91]\\nLong-form QA ELI5 [121] [27], [34], [43], [49], [51]\\nNarrativeQA(NQA) [122] [45], [60], [63], [123]\\nASQA [124] [24], [57]\\nQMSum(QM) [125] [60], [123]\\nDomain QA Qasper [126] [60], [63]\\nCOVID-QA [127] [35], [46]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='NarrativeQA(NQA) [122] [45], [60], [63], [123]\\nASQA [124] [24], [57]\\nQMSum(QM) [125] [60], [123]\\nDomain QA Qasper [126] [60], [63]\\nCOVID-QA [127] [35], [46]\\nCMB [128],MMCU Medical [129] [81]\\nMulti-Choice QA QuALITY [130] [60], [63]\\nARC [131] [25], [67]\\nCommonsenseQA [132] [58], [66]\\nGraph QA GraphQA [84] [84]\\nDialog Dialog Generation Wizard of Wikipedia (WoW) [133] [13], [27], [34], [42]\\nPersonal Dialog KBP [134] [74], [135]\\nDuleMon [136] [74]\\nTask-oriented Dialog CamRest [137] [78], [79]\\nRecommendation Amazon(Toys,Sport,Beauty) [138] [39], [40]\\nIE Event Argument Extraction WikiEvent [139] [13], [27], [37], [42]\\nRAMS [140] [36], [37]\\nRelation Extraction T-REx [141],ZsRE [142] [27], [51]\\nReasoning Commonsense Reasoning HellaSwag [143] [20], [66]\\nCoT Reasoning CoT Reasoning [144] [27]\\nComplex Reasoning CSQA [145] [55]\\nOthers Language Understanding MMLU [146] [7], [27], [28], [42], [43], [47], [72]\\nLanguage Modeling WikiText-103 [147] [5], [29], [64], [71]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 12, 'page_label': '13', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='Complex Reasoning CSQA [145] [55]\\nOthers Language Understanding MMLU [146] [7], [27], [28], [42], [43], [47], [72]\\nLanguage Modeling WikiText-103 [147] [5], [29], [64], [71]\\nStrategyQA [148] [14], [24], [48], [51], [55], [58]\\nFact Checking/Verification FEVER [149] [4], [13], [27], [34], [42], [50]\\nPubHealth [150] [25], [67]\\nText Generation Biography [151] [67]\\nText Summarization WikiASP [152] [24]\\nXSum [153] [17]\\nText Classification VioLens [154] [19]\\nTREC [155] [33]\\nSentiment SST-2 [156] [20], [33], [38]\\nCode Search CodeSearchNet [157] [76]\\nRobustness Evaluation NoMIRACL [56] [56]\\nMath GSM8K [158] [73]\\nMachine Translation JRC-Acquis [159] [17]'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='14\\nTABLE III\\nSUMMARY OF METRICS APPLICABLE FOR EVALUATION ASPECTS OF RAG\\nContext\\nRelevance Faithfulness Answer\\nRelevance\\nNoise\\nRobustness\\nNegative\\nRejection\\nInformation\\nIntegration\\nCounterfactual\\nRobustness\\nAccuracy ✓ ✓ ✓ ✓ ✓ ✓ ✓\\nEM ✓\\nRecall ✓\\nPrecision ✓ ✓\\nR-Rate ✓\\nCosine Similarity ✓\\nHit Rate ✓\\nMRR ✓\\nNDCG ✓\\nBLEU ✓ ✓ ✓\\nROUGE/ROUGE-L ✓ ✓ ✓\\nThe specific metrics for each evaluation aspect are sum-\\nmarized in Table III. It is essential to recognize that these\\nmetrics, derived from related work, are traditional measures\\nand do not yet represent a mature or standardized approach for\\nquantifying RAG evaluation aspects. Custom metrics tailored\\nto the nuances of RAG models, though not included here, have\\nalso been developed in some evaluation studies.\\nD. Evaluation Benchmarks and Tools\\nA series of benchmark tests and tools have been proposed\\nto facilitate the evaluation of RAG.These instruments furnish\\nquantitative metrics that not only gauge RAG model perfor-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='A series of benchmark tests and tools have been proposed\\nto facilitate the evaluation of RAG.These instruments furnish\\nquantitative metrics that not only gauge RAG model perfor-\\nmance but also enhance comprehension of the model’s capabil-\\nities across various evaluation aspects. Prominent benchmarks\\nsuch as RGB, RECALL and CRUD [167]–[169] focus on\\nappraising the essential abilities of RAG models. Concur-\\nrently, state-of-the-art automated tools like RAGAS [164],\\nARES [165], and TruLens 8 employ LLMs to adjudicate the\\nquality scores. These tools and benchmarks collectively form\\na robust framework for the systematic evaluation of RAG\\nmodels, as summarized in Table IV.\\nVII. D ISCUSSION AND FUTURE PROSPECTS\\nDespite the considerable progress in RAG technology, sev-\\neral challenges persist that warrant in-depth research.This\\nchapter will mainly introduce the current challenges and future\\nresearch directions faced by RAG.\\nA. RAG vs Long Context'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='eral challenges persist that warrant in-depth research.This\\nchapter will mainly introduce the current challenges and future\\nresearch directions faced by RAG.\\nA. RAG vs Long Context\\nWith the deepening of related research, the context of LLMs\\nis continuously expanding [170]–[172]. Presently, LLMs can\\neffortlessly manage contexts exceeding 200,000 tokens 9. This\\ncapability signifies that long-document question answering,\\npreviously reliant on RAG, can now incorporate the entire\\ndocument directly into the prompt. This has also sparked\\ndiscussions on whether RAG is still necessary when LLMs\\n8https://www.trulens.org/trulens eval/core concepts rag triad/\\n9https://kimi.moonshot.cn\\nare not constrained by context. In fact, RAG still plays an\\nirreplaceable role. On one hand, providing LLMs with a\\nlarge amount of context at once will significantly impact its\\ninference speed, while chunked retrieval and on-demand input\\ncan significantly improve operational efficiency. On the other'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='large amount of context at once will significantly impact its\\ninference speed, while chunked retrieval and on-demand input\\ncan significantly improve operational efficiency. On the other\\nhand, RAG-based generation can quickly locate the original\\nreferences for LLMs to help users verify the generated an-\\nswers. The entire retrieval and reasoning process is observable,\\nwhile generation solely relying on long context remains a\\nblack box. Conversely, the expansion of context provides new\\nopportunities for the development of RAG, enabling it to\\naddress more complex problems and integrative or summary\\nquestions that require reading a large amount of material to\\nanswer [49]. Developing new RAG methods in the context of\\nsuper-long contexts is one of the future research trends.\\nB. RAG Robustness\\nThe presence of noise or contradictory information during\\nretrieval can detrimentally affect RAG’s output quality. This\\nsituation is figuratively referred to as “Misinformation can'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='B. RAG Robustness\\nThe presence of noise or contradictory information during\\nretrieval can detrimentally affect RAG’s output quality. This\\nsituation is figuratively referred to as “Misinformation can\\nbe worse than no information at all”. Improving RAG’s\\nresistance to such adversarial or counterfactual inputs is gain-\\ning research momentum and has become a key performance\\nmetric [48], [50], [82]. Cuconasu et al. [54] analyze which\\ntype of documents should be retrieved, evaluate the relevance\\nof the documents to the prompt, their position, and the\\nnumber included in the context. The research findings reveal\\nthat including irrelevant documents can unexpectedly increase\\naccuracy by over 30%, contradicting the initial assumption\\nof reduced quality. These results underscore the importance\\nof developing specialized strategies to integrate retrieval with\\nlanguage generation models, highlighting the need for further\\nresearch and exploration into the robustness of RAG.\\nC. Hybrid Approaches'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 13, 'page_label': '14', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='language generation models, highlighting the need for further\\nresearch and exploration into the robustness of RAG.\\nC. Hybrid Approaches\\nCombining RAG with fine-tuning is emerging as a leading\\nstrategy. Determining the optimal integration of RAG and\\nfine-tuning whether sequential, alternating, or through end-to-\\nend joint training—and how to harness both parameterized'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='15\\nTABLE IV\\nSUMMARY OF EVALUATION FRAMEWORKS\\nEvaluation Framework Evaluation Targets Evaluation Aspects Quantitative Metrics\\nRGB† Retrieval Quality\\nGeneration Quality\\nNoise Robustness\\nNegative Rejection\\nInformation Integration\\nCounterfactual Robustness\\nAccuracy\\nEM\\nAccuracy\\nAccuracy\\nRECALL† Generation Quality Counterfactual Robustness R-Rate (Reappearance Rate)\\nRAGAS‡ Retrieval Quality\\nGeneration Quality\\nContext Relevance\\nFaithfulness\\nAnswer Relevance\\n*\\n*\\nCosine Similarity\\nARES‡ Retrieval Quality\\nGeneration Quality\\nContext Relevance\\nFaithfulness\\nAnswer Relevance\\nAccuracy\\nAccuracy\\nAccuracy\\nTruLens‡ Retrieval Quality\\nGeneration Quality\\nContext Relevance\\nFaithfulness\\nAnswer Relevance\\n*\\n*\\n*\\nCRUD† Retrieval Quality\\nGeneration Quality\\nCreative Generation\\nKnowledge-intensive QA\\nError Correction\\nSummarization\\nBLEU\\nROUGE-L\\nBertScore\\nRAGQuestEval\\n† represents a benchmark, and ‡ represents a tool. * denotes customized quantitative metrics, which deviate from traditional'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='Error Correction\\nSummarization\\nBLEU\\nROUGE-L\\nBertScore\\nRAGQuestEval\\n† represents a benchmark, and ‡ represents a tool. * denotes customized quantitative metrics, which deviate from traditional\\nmetrics. Readers are encouraged to consult pertinent literature for the specific quantification formulas associated with these\\nmetrics, as required.\\nand non-parameterized advantages are areas ripe for explo-\\nration [27]. Another trend is to introduce SLMs with specific\\nfunctionalities into RAG and fine-tuned by the results of RAG\\nsystem. For example, CRAG [67] trains a lightweight retrieval\\nevaluator to assess the overall quality of the retrieved docu-\\nments for a query and triggers different knowledge retrieval\\nactions based on confidence levels.\\nD. Scaling laws of RAG\\nEnd-to-end RAG models and pre-trained models based\\non RAG are still one of the focuses of current re-\\nsearchers [173].The parameters of these models are one of\\nthe key factors.While scaling laws [174] are established for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='on RAG are still one of the focuses of current re-\\nsearchers [173].The parameters of these models are one of\\nthe key factors.While scaling laws [174] are established for\\nLLMs, their applicability to RAG remains uncertain. Initial\\nstudies like RETRO++ [44] have begun to address this, yet the\\nparameter count in RAG models still lags behind that of LLMs.\\nThe possibility of an Inverse Scaling Law 10, where smaller\\nmodels outperform larger ones, is particularly intriguing and\\nmerits further investigation.\\nE. Production-Ready RAG\\nRAG’s practicality and alignment with engineering require-\\nments have facilitated its adoption. However, enhancing re-\\ntrieval efficiency, improving document recall in large knowl-\\nedge bases, and ensuring data security—such as preventing\\n10https://github.com/inverse-scaling/prize\\ninadvertent disclosure of document sources or metadata by\\nLLMs—are critical engineering challenges that remain to be\\naddressed [175].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='10https://github.com/inverse-scaling/prize\\ninadvertent disclosure of document sources or metadata by\\nLLMs—are critical engineering challenges that remain to be\\naddressed [175].\\nThe development of the RAG ecosystem is greatly impacted\\nby the progression of its technical stack. Key tools like\\nLangChain and LLamaIndex have quickly gained popularity\\nwith the emergence of ChatGPT, providing extensive RAG-\\nrelated APIs and becoming essential in the realm of LLMs.The\\nemerging technology stack, while not as rich in features as\\nLangChain and LLamaIndex, stands out through its specialized\\nproducts. For example, Flowise AI prioritizes a low-code\\napproach, allowing users to deploy AI applications, including\\nRAG, through a user-friendly drag-and-drop interface. Other\\ntechnologies like HayStack, Meltano, and Cohere Coral are\\nalso gaining attention for their unique contributions to the field.\\nIn addition to AI-focused vendors, traditional software and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 14, 'page_label': '15', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='technologies like HayStack, Meltano, and Cohere Coral are\\nalso gaining attention for their unique contributions to the field.\\nIn addition to AI-focused vendors, traditional software and\\ncloud service providers are expanding their offerings to include\\nRAG-centric services. Weaviate’s Verba 11 is designed for\\npersonal assistant applications, while Amazon’s Kendra 12\\noffers intelligent enterprise search services, enabling users to\\nbrowse various content repositories using built-in connectors.\\nIn the development of RAG technology, there is a clear\\ntrend towards different specialization directions, such as: 1)\\nCustomization - tailoring RAG to meet specific requirements.\\n2) Simplification - making RAG easier to use to reduce the\\n11https://github.com/weaviate/Verba\\n12https://aws.amazon.com/cn/kendra/'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='16\\nFig. 6. Summary of RAG ecosystem\\ninitial learning curve. 3) Specialization - optimizing RAG to\\nbetter serve production environments.\\nThe mutual growth of RAG models and their technology\\nstacks is evident; technological advancements continuously\\nestablish new standards for existing infrastructure. In turn,\\nenhancements to the technology stack drive the development\\nof RAG capabilities. RAG toolkits are converging into a\\nfoundational technology stack, laying the groundwork for\\nadvanced enterprise applications. However, a fully integrated,\\ncomprehensive platform concept is still in the future, requiring\\nfurther innovation and development.\\nF . Multi-modal RAG\\nRAG has transcended its initial text-based question-\\nanswering confines, embracing a diverse array of modal data.\\nThis expansion has spawned innovative multimodal models\\nthat integrate RAG concepts across various domains:\\nImage. RA-CM3 [176] stands as a pioneering multimodal\\nmodel of both retrieving and generating text and images.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='that integrate RAG concepts across various domains:\\nImage. RA-CM3 [176] stands as a pioneering multimodal\\nmodel of both retrieving and generating text and images.\\nBLIP-2 [177] leverages frozen image encoders alongside\\nLLMs for efficient visual language pre-training, enabling zero-\\nshot image-to-text conversions. The “Visualize Before You\\nWrite” method [178] employs image generation to steer the\\nLM’s text generation, showing promise in open-ended text\\ngeneration tasks.\\nAudio and Video . The GSS method retrieves and stitches\\ntogether audio clips to convert machine-translated data into\\nspeech-translated data [179]. UEOP marks a significant ad-\\nvancement in end-to-end automatic speech recognition by\\nincorporating external, offline strategies for voice-to-text con-\\nversion [180]. Additionally, KNN-based attention fusion lever-\\nages audio embeddings and semantically related text embed-\\ndings to refine ASR, thereby accelerating domain adaptation.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='version [180]. Additionally, KNN-based attention fusion lever-\\nages audio embeddings and semantically related text embed-\\ndings to refine ASR, thereby accelerating domain adaptation.\\nVid2Seq augments language models with specialized temporal\\nmarkers, facilitating the prediction of event boundaries and\\ntextual descriptions within a unified output sequence [181].\\nCode. RBPS [182] excels in small-scale learning tasks by\\nretrieving code examples that align with developers’ objectives\\nthrough encoding and frequency analysis. This approach has\\ndemonstrated efficacy in tasks such as test assertion genera-\\ntion and program repair. For structured knowledge, the CoK\\nmethod [106] first extracts facts pertinent to the input query\\nfrom a knowledge graph, then integrates these facts as hints\\nwithin the input, enhancing performance in knowledge graph\\nquestion-answering tasks.\\nVIII. C ONCLUSION\\nThe summary of this paper, as depicted in Figure 6, empha-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='within the input, enhancing performance in knowledge graph\\nquestion-answering tasks.\\nVIII. C ONCLUSION\\nThe summary of this paper, as depicted in Figure 6, empha-\\nsizes RAG’s significant advancement in enhancing the capa-\\nbilities of LLMs by integrating parameterized knowledge from\\nlanguage models with extensive non-parameterized data from\\nexternal knowledge bases. The survey showcases the evolution\\nof RAG technologies and their application on many different\\ntasks. The analysis outlines three developmental paradigms\\nwithin the RAG framework: Naive, Advanced, and Modu-\\nlar RAG, each representing a progressive enhancement over\\nits predecessors. RAG’s technical integration with other AI\\nmethodologies, such as fine-tuning and reinforcement learning,\\nhas further expanded its capabilities. Despite the progress in\\nRAG technology, there are research opportunities to improve\\nits robustness and its ability to handle extended contexts.\\nRAG’s application scope is expanding into multimodal do-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 15, 'page_label': '16', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='RAG technology, there are research opportunities to improve\\nits robustness and its ability to handle extended contexts.\\nRAG’s application scope is expanding into multimodal do-\\nmains, adapting its principles to interpret and process diverse\\ndata forms like images, videos, and code. This expansion high-\\nlights RAG’s significant practical implications for AI deploy-\\nment, attracting interest from academic and industrial sectors.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='17\\nThe growing ecosystem of RAG is evidenced by the rise in\\nRAG-centric AI applications and the continuous development\\nof supportive tools. As RAG’s application landscape broadens,\\nthere is a need to refine evaluation methodologies to keep\\npace with its evolution. Ensuring accurate and representative\\nperformance assessments is crucial for fully capturing RAG’s\\ncontributions to the AI research and development community.\\nREFERENCES\\n[1] N. Kandpal, H. Deng, A. Roberts, E. Wallace, and C. Raffel, “Large\\nlanguage models struggle to learn long-tail knowledge,” in Interna-\\ntional Conference on Machine Learning . PMLR, 2023, pp. 15 696–\\n15 707.\\n[2] Y . Zhang, Y . Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,\\nY . Zhang, Y . Chenet al., “Siren’s song in the ai ocean: A survey on hal-\\nlucination in large language models,” arXiv preprint arXiv:2309.01219,\\n2023.\\n[3] D. Arora, A. Kini, S. R. Chowdhury, N. Natarajan, G. Sinha, and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='lucination in large language models,” arXiv preprint arXiv:2309.01219,\\n2023.\\n[3] D. Arora, A. Kini, S. R. Chowdhury, N. Natarajan, G. Sinha, and\\nA. Sharma, “Gar-meets-rag paradigm for zero-shot information re-\\ntrieval,” arXiv preprint arXiv:2310.20158 , 2023.\\n[4] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,\\nH. K ¨uttler, M. Lewis, W.-t. Yih, T. Rockt ¨aschel et al. , “Retrieval-\\naugmented generation for knowledge-intensive nlp tasks,” Advances in\\nNeural Information Processing Systems, vol. 33, pp. 9459–9474, 2020.\\n[5] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli-\\ncan, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clarket al.,\\n“Improving language models by retrieving from trillions of tokens,”\\nin International conference on machine learning . PMLR, 2022, pp.\\n2206–2240.\\n[6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al. , “Training language'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='2206–2240.\\n[6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al. , “Training language\\nmodels to follow instructions with human feedback,” Advances in\\nneural information processing systems , vol. 35, pp. 27 730–27 744,\\n2022.\\n[7] X. Ma, Y . Gong, P. He, H. Zhao, and N. Duan, “Query rewrit-\\ning for retrieval-augmented large language models,” arXiv preprint\\narXiv:2305.14283, 2023.\\n[8] I. ILIN, “Advanced rag techniques: an il-\\nlustrated overview,” https://pub.towardsai.net/\\nadvanced-rag-techniques-an-illustrated-overview-04d193d8fec6,\\n2023.\\n[9] W. Peng, G. Li, Y . Jiang, Z. Wang, D. Ou, X. Zeng, E. Chen et al. ,\\n“Large language model based long-tail query rewriting in taobao\\nsearch,” arXiv preprint arXiv:2311.03758 , 2023.\\n[10] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V . Le,\\nand D. Zhou, “Take a step back: Evoking reasoning via abstraction in\\nlarge language models,” arXiv preprint arXiv:2310.06117 , 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='and D. Zhou, “Take a step back: Evoking reasoning via abstraction in\\nlarge language models,” arXiv preprint arXiv:2310.06117 , 2023.\\n[11] L. Gao, X. Ma, J. Lin, and J. Callan, “Precise zero-shot dense retrieval\\nwithout relevance labels,” arXiv preprint arXiv:2212.10496 , 2022.\\n[12] V . Blagojevi, “Enhancing rag pipelines in haystack: Introducing diver-\\nsityranker and lostinthemiddleranker,” https://towardsdatascience.com/\\nenhancing-rag-pipelines-in-haystack-45f14e2bc9f5, 2023.\\n[13] W. Yu, D. Iter, S. Wang, Y . Xu, M. Ju, S. Sanyal, C. Zhu, M. Zeng,\\nand M. Jiang, “Generate rather than retrieve: Large language models\\nare strong context generators,” arXiv preprint arXiv:2209.10063, 2022.\\n[14] Z. Shao, Y . Gong, Y . Shen, M. Huang, N. Duan, and W. Chen,\\n“Enhancing retrieval-augmented large language models with iterative\\nretrieval-generation synergy,” arXiv preprint arXiv:2305.15294 , 2023.\\n[15] X. Wang, Q. Yang, Y . Qiu, J. Liang, Q. He, Z. Gu, Y . Xiao,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='retrieval-generation synergy,” arXiv preprint arXiv:2305.15294 , 2023.\\n[15] X. Wang, Q. Yang, Y . Qiu, J. Liang, Q. He, Z. Gu, Y . Xiao,\\nand W. Wang, “Knowledgpt: Enhancing large language models with\\nretrieval and storage access on knowledge bases,” arXiv preprint\\narXiv:2308.11761, 2023.\\n[16] A. H. Raudaschl, “Forget rag, the future\\nis rag-fusion,” https://towardsdatascience.com/\\nforget-rag-the-future-is-rag-fusion-1147298d8ad1, 2023.\\n[17] X. Cheng, D. Luo, X. Chen, L. Liu, D. Zhao, and R. Yan, “Lift\\nyourself up: Retrieval-augmented text generation with self memory,”\\narXiv preprint arXiv:2305.02437 , 2023.\\n[18] S. Wang, Y . Xu, Y . Fang, Y . Liu, S. Sun, R. Xu, C. Zhu, and\\nM. Zeng, “Training data is more valuable than you think: A simple\\nand effective method by retrieving from training data,” arXiv preprint\\narXiv:2203.08773, 2022.\\n[19] X. Li, E. Nie, and S. Liang, “From classification to generation:\\nInsights into crosslingual retrieval augmented icl,” arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='arXiv:2203.08773, 2022.\\n[19] X. Li, E. Nie, and S. Liang, “From classification to generation:\\nInsights into crosslingual retrieval augmented icl,” arXiv preprint\\narXiv:2311.06595, 2023.\\n[20] D. Cheng, S. Huang, J. Bi, Y . Zhan, J. Liu, Y . Wang, H. Sun,\\nF. Wei, D. Deng, and Q. Zhang, “Uprise: Universal prompt retrieval\\nfor improving zero-shot evaluation,” arXiv preprint arXiv:2303.08518,\\n2023.\\n[21] Z. Dai, V . Y . Zhao, J. Ma, Y . Luan, J. Ni, J. Lu, A. Bakalov, K. Guu,\\nK. B. Hall, and M.-W. Chang, “Promptagator: Few-shot dense retrieval\\nfrom 8 examples,” arXiv preprint arXiv:2209.11755 , 2022.\\n[22] Z. Sun, X. Wang, Y . Tay, Y . Yang, and D. Zhou, “Recitation-augmented\\nlanguage models,” arXiv preprint arXiv:2210.01296 , 2022.\\n[23] O. Khattab, K. Santhanam, X. L. Li, D. Hall, P. Liang, C. Potts,\\nand M. Zaharia, “Demonstrate-search-predict: Composing retrieval\\nand language models for knowledge-intensive nlp,” arXiv preprint\\narXiv:2212.14024, 2022.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='and M. Zaharia, “Demonstrate-search-predict: Composing retrieval\\nand language models for knowledge-intensive nlp,” arXiv preprint\\narXiv:2212.14024, 2022.\\n[24] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y . Yang,\\nJ. Callan, and G. Neubig, “Active retrieval augmented generation,”\\narXiv preprint arXiv:2305.06983 , 2023.\\n[25] A. Asai, Z. Wu, Y . Wang, A. Sil, and H. Hajishirzi, “Self-rag:\\nLearning to retrieve, generate, and critique through self-reflection,”\\narXiv preprint arXiv:2310.11511 , 2023.\\n[26] Z. Ke, W. Kong, C. Li, M. Zhang, Q. Mei, and M. Bendersky,\\n“Bridging the preference gap between retrievers and llms,” arXiv\\npreprint arXiv:2401.06954, 2024.\\n[27] X. V . Lin, X. Chen, M. Chen, W. Shi, M. Lomeli, R. James, P. Ro-\\ndriguez, J. Kahn, G. Szilvasy, M. Lewis et al. , “Ra-dit: Retrieval-\\naugmented dual instruction tuning,” arXiv preprint arXiv:2310.01352 ,\\n2023.\\n[28] O. Ovadia, M. Brief, M. Mishaeli, and O. Elisha, “Fine-tuning or'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='augmented dual instruction tuning,” arXiv preprint arXiv:2310.01352 ,\\n2023.\\n[28] O. Ovadia, M. Brief, M. Mishaeli, and O. Elisha, “Fine-tuning or\\nretrieval? comparing knowledge injection in llms,” arXiv preprint\\narXiv:2312.05934, 2023.\\n[29] T. Lan, D. Cai, Y . Wang, H. Huang, and X.-L. Mao, “Copy is all\\nyou need,” in The Eleventh International Conference on Learning\\nRepresentations, 2022.\\n[30] T. Chen, H. Wang, S. Chen, W. Yu, K. Ma, X. Zhao, D. Yu, and\\nH. Zhang, “Dense x retrieval: What retrieval granularity should we\\nuse?” arXiv preprint arXiv:2312.06648 , 2023.\\n[31] F. Luo and M. Surdeanu, “Divide & conquer for entailment-aware\\nmulti-hop evidence retrieval,” arXiv preprint arXiv:2311.02616 , 2023.\\n[32] Q. Gou, Z. Xia, B. Yu, H. Yu, F. Huang, Y . Li, and N. Cam-Tu,\\n“Diversify question generation with retrieval-augmented style transfer,”\\narXiv preprint arXiv:2310.14503 , 2023.\\n[33] Z. Guo, S. Cheng, Y . Wang, P. Li, and Y . Liu, “Prompt-guided re-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='“Diversify question generation with retrieval-augmented style transfer,”\\narXiv preprint arXiv:2310.14503 , 2023.\\n[33] Z. Guo, S. Cheng, Y . Wang, P. Li, and Y . Liu, “Prompt-guided re-\\ntrieval augmentation for non-knowledge-intensive tasks,”arXiv preprint\\narXiv:2305.17653, 2023.\\n[34] Z. Wang, J. Araki, Z. Jiang, M. R. Parvez, and G. Neubig, “Learning\\nto filter context for retrieval-augmented generation,” arXiv preprint\\narXiv:2311.08377, 2023.\\n[35] M. Seo, J. Baek, J. Thorne, and S. J. Hwang, “Retrieval-augmented\\ndata augmentation for low-resource domain tasks,” arXiv preprint\\narXiv:2402.13482, 2024.\\n[36] Y . Ma, Y . Cao, Y . Hong, and A. Sun, “Large language model is not\\na good few-shot information extractor, but a good reranker for hard\\nsamples!” arXiv preprint arXiv:2303.08559 , 2023.\\n[37] X. Du and H. Ji, “Retrieval-augmented generative question answering\\nfor event argument extraction,” arXiv preprint arXiv:2211.07067, 2022.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='samples!” arXiv preprint arXiv:2303.08559 , 2023.\\n[37] X. Du and H. Ji, “Retrieval-augmented generative question answering\\nfor event argument extraction,” arXiv preprint arXiv:2211.07067, 2022.\\n[38] L. Wang, N. Yang, and F. Wei, “Learning to retrieve in-context\\nexamples for large language models,”arXiv preprint arXiv:2307.07164,\\n2023.\\n[39] S. Rajput, N. Mehta, A. Singh, R. H. Keshavan, T. Vu, L. Heldt,\\nL. Hong, Y . Tay, V . Q. Tran, J. Samostet al., “Recommender systems\\nwith generative retrieval,” arXiv preprint arXiv:2305.05065 , 2023.\\n[40] B. Jin, H. Zeng, G. Wang, X. Chen, T. Wei, R. Li, Z. Wang, Z. Li,\\nY . Li, H. Lu et al. , “Language models as semantic indexers,” arXiv\\npreprint arXiv:2310.07815, 2023.\\n[41] R. Anantha, T. Bethi, D. V odianik, and S. Chappidi, “Context tuning\\nfor retrieval augmented generation,” arXiv preprint arXiv:2312.05708 ,\\n2023.\\n[42] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 16, 'page_label': '17', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='for retrieval augmented generation,” arXiv preprint arXiv:2312.05708 ,\\n2023.\\n[42] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,\\nJ. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, “Few-shot\\nlearning with retrieval augmented language models,” arXiv preprint\\narXiv:2208.03299, 2022.\\n[43] J. Huang, W. Ping, P. Xu, M. Shoeybi, K. C.-C. Chang, and B. Catan-\\nzaro, “Raven: In-context learning with retrieval augmented encoder-\\ndecoder language models,” arXiv preprint arXiv:2308.07922 , 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='18\\n[44] B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi, Y . Dong,\\nO. Kuchaiev, B. Li, C. Xiao et al. , “Shall we pretrain autoregressive\\nlanguage models with retrieval? a comprehensive study,”arXiv preprint\\narXiv:2304.06762, 2023.\\n[45] B. Wang, W. Ping, L. McAfee, P. Xu, B. Li, M. Shoeybi, and B. Catan-\\nzaro, “Instructretro: Instruction tuning post retrieval-augmented pre-\\ntraining,” arXiv preprint arXiv:2310.07713 , 2023.\\n[46] S. Siriwardhana, R. Weerasekera, E. Wen, T. Kaluarachchi, R. Rana,\\nand S. Nanayakkara, “Improving the domain adaptation of retrieval\\naugmented generation (rag) models for open domain question answer-\\ning,” Transactions of the Association for Computational Linguistics ,\\nvol. 11, pp. 1–17, 2023.\\n[47] Z. Yu, C. Xiong, S. Yu, and Z. Liu, “Augmentation-adapted retriever\\nimproves generalization of language models as generic plug-in,” arXiv\\npreprint arXiv:2305.17331, 2023.\\n[48] O. Yoran, T. Wolfson, O. Ram, and J. Berant, “Making retrieval-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='improves generalization of language models as generic plug-in,” arXiv\\npreprint arXiv:2305.17331, 2023.\\n[48] O. Yoran, T. Wolfson, O. Ram, and J. Berant, “Making retrieval-\\naugmented language models robust to irrelevant context,” arXiv\\npreprint arXiv:2310.01558, 2023.\\n[49] H.-T. Chen, F. Xu, S. A. Arora, and E. Choi, “Understanding re-\\ntrieval augmentation for long-form question answering,” arXiv preprint\\narXiv:2310.12150, 2023.\\n[50] W. Yu, H. Zhang, X. Pan, K. Ma, H. Wang, and D. Yu, “Chain-of-note:\\nEnhancing robustness in retrieval-augmented language models,” arXiv\\npreprint arXiv:2311.09210, 2023.\\n[51] S. Xu, L. Pang, H. Shen, X. Cheng, and T.-S. Chua, “Search-in-the-\\nchain: Towards accurate, credible and traceable large language models\\nfor knowledgeintensive tasks,” CoRR, vol. abs/2304.14732 , 2023.\\n[52] M. Berchansky, P. Izsak, A. Caciularu, I. Dagan, and M. Wasserblat,\\n“Optimizing retrieval-augmented reader models via token elimination,”\\narXiv preprint arXiv:2310.13682 , 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='[52] M. Berchansky, P. Izsak, A. Caciularu, I. Dagan, and M. Wasserblat,\\n“Optimizing retrieval-augmented reader models via token elimination,”\\narXiv preprint arXiv:2310.13682 , 2023.\\n[53] J. L ´ala, O. O’Donoghue, A. Shtedritski, S. Cox, S. G. Rodriques,\\nand A. D. White, “Paperqa: Retrieval-augmented generative agent for\\nscientific research,” arXiv preprint arXiv:2312.07559 , 2023.\\n[54] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano,\\nY . Maarek, N. Tonellotto, and F. Silvestri, “The power of noise:\\nRedefining retrieval for rag systems,” arXiv preprint arXiv:2401.14887,\\n2024.\\n[55] Z. Zhang, X. Zhang, Y . Ren, S. Shi, M. Han, Y . Wu, R. Lai, and\\nZ. Cao, “Iag: Induction-augmented generation framework for answer-\\ning reasoning questions,” in Proceedings of the 2023 Conference on\\nEmpirical Methods in Natural Language Processing , 2023, pp. 1–14.\\n[56] N. Thakur, L. Bonifacio, X. Zhang, O. Ogundepo, E. Kamalloo,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='ing reasoning questions,” in Proceedings of the 2023 Conference on\\nEmpirical Methods in Natural Language Processing , 2023, pp. 1–14.\\n[56] N. Thakur, L. Bonifacio, X. Zhang, O. Ogundepo, E. Kamalloo,\\nD. Alfonso-Hermelo, X. Li, Q. Liu, B. Chen, M. Rezagholizadeh et al.,\\n“Nomiracl: Knowing when you don’t know for robust multilingual\\nretrieval-augmented generation,” arXiv preprint arXiv:2312.11361 ,\\n2023.\\n[57] G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, “Tree of clarifica-\\ntions: Answering ambiguous questions with retrieval-augmented large\\nlanguage models,” arXiv preprint arXiv:2310.14696 , 2023.\\n[58] Y . Wang, P. Li, M. Sun, and Y . Liu, “Self-knowledge guided\\nretrieval augmentation for large language models,” arXiv preprint\\narXiv:2310.05002, 2023.\\n[59] Z. Feng, X. Feng, D. Zhao, M. Yang, and B. Qin, “Retrieval-\\ngeneration synergy augmented large language models,” arXiv preprint\\narXiv:2310.05149, 2023.\\n[60] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='generation synergy augmented large language models,” arXiv preprint\\narXiv:2310.05149, 2023.\\n[60] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian,\\nE. Bakhturina, M. Shoeybi, and B. Catanzaro, “Retrieval meets long\\ncontext large language models,” arXiv preprint arXiv:2310.03025 ,\\n2023.\\n[61] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Interleav-\\ning retrieval with chain-of-thought reasoning for knowledge-intensive\\nmulti-step questions,” arXiv preprint arXiv:2212.10509 , 2022.\\n[62] R. Ren, Y . Wang, Y . Qu, W. X. Zhao, J. Liu, H. Tian, H. Wu, J.-\\nR. Wen, and H. Wang, “Investigating the factual knowledge boundary\\nof large language models with retrieval augmentation,” arXiv preprint\\narXiv:2307.11019, 2023.\\n[63] P. Sarthi, S. Abdullah, A. Tuli, S. Khanna, A. Goldie, and C. D.\\nManning, “Raptor: Recursive abstractive processing for tree-organized\\nretrieval,” arXiv preprint arXiv:2401.18059 , 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='[63] P. Sarthi, S. Abdullah, A. Tuli, S. Khanna, A. Goldie, and C. D.\\nManning, “Raptor: Recursive abstractive processing for tree-organized\\nretrieval,” arXiv preprint arXiv:2401.18059 , 2024.\\n[64] O. Ram, Y . Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-\\nBrown, and Y . Shoham, “In-context retrieval-augmented language\\nmodels,” arXiv preprint arXiv:2302.00083 , 2023.\\n[65] Y . Ren, Y . Cao, P. Guo, F. Fang, W. Ma, and Z. Lin, “Retrieve-and-\\nsample: Document-level event argument extraction via hybrid retrieval\\naugmentation,” in Proceedings of the 61st Annual Meeting of the\\nAssociation for Computational Linguistics (Volume 1: Long Papers) ,\\n2023, pp. 293–306.\\n[66] Z. Wang, X. Pan, D. Yu, D. Yu, J. Chen, and H. Ji, “Zemi: Learning\\nzero-shot semi-parametric language models from multiple tasks,” arXiv\\npreprint arXiv:2210.00185, 2022.\\n[67] S.-Q. Yan, J.-C. Gu, Y . Zhu, and Z.-H. Ling, “Corrective retrieval\\naugmented generation,” arXiv preprint arXiv:2401.15884 , 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='preprint arXiv:2210.00185, 2022.\\n[67] S.-Q. Yan, J.-C. Gu, Y . Zhu, and Z.-H. Ling, “Corrective retrieval\\naugmented generation,” arXiv preprint arXiv:2401.15884 , 2024.\\n[68] P. Jain, L. B. Soares, and T. Kwiatkowski, “1-pager: One pass answer\\ngeneration and evidence retrieval,” arXiv preprint arXiv:2310.16568 ,\\n2023.\\n[69] H. Yang, Z. Li, Y . Zhang, J. Wang, N. Cheng, M. Li, and J. Xiao, “Prca:\\nFitting black-box large language models for retrieval question answer-\\ning via pluggable reward-driven contextual adapter,” arXiv preprint\\narXiv:2310.18347, 2023.\\n[70] S. Zhuang, B. Liu, B. Koopman, and G. Zuccon, “Open-source large\\nlanguage models are strong zero-shot query likelihood models for\\ndocument ranking,” arXiv preprint arXiv:2310.13243 , 2023.\\n[71] F. Xu, W. Shi, and E. Choi, “Recomp: Improving retrieval-augmented\\nlms with compression and selective augmentation,” arXiv preprint\\narXiv:2310.04408, 2023.\\n[72] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='lms with compression and selective augmentation,” arXiv preprint\\narXiv:2310.04408, 2023.\\n[72] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-\\nmoyer, and W.-t. Yih, “Replug: Retrieval-augmented black-box lan-\\nguage models,” arXiv preprint arXiv:2301.12652 , 2023.\\n[73] E. Melz, “Enhancing llm intelligence with arm-rag: Auxiliary ra-\\ntionale memory for retrieval augmented generation,” arXiv preprint\\narXiv:2311.04177, 2023.\\n[74] H. Wang, W. Huang, Y . Deng, R. Wang, Z. Wang, Y . Wang, F. Mi,\\nJ. Z. Pan, and K.-F. Wong, “Unims-rag: A unified multi-source\\nretrieval-augmented generation for personalized dialogue systems,”\\narXiv preprint arXiv:2401.13256 , 2024.\\n[75] Z. Luo, C. Xu, P. Zhao, X. Geng, C. Tao, J. Ma, Q. Lin, and D. Jiang,\\n“Augmented large language models with parametric knowledge guid-\\ning,” arXiv preprint arXiv:2305.04757 , 2023.\\n[76] X. Li, Z. Liu, C. Xiong, S. Yu, Y . Gu, Z. Liu, and G. Yu, “Structure-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='“Augmented large language models with parametric knowledge guid-\\ning,” arXiv preprint arXiv:2305.04757 , 2023.\\n[76] X. Li, Z. Liu, C. Xiong, S. Yu, Y . Gu, Z. Liu, and G. Yu, “Structure-\\naware language model pretraining improves dense retrieval on struc-\\ntured data,” arXiv preprint arXiv:2305.19912 , 2023.\\n[77] M. Kang, J. M. Kwak, J. Baek, and S. J. Hwang, “Knowledge\\ngraph-augmented language models for knowledge-grounded dialogue\\ngeneration,” arXiv preprint arXiv:2305.18846 , 2023.\\n[78] W. Shen, Y . Gao, C. Huang, F. Wan, X. Quan, and W. Bi, “Retrieval-\\ngeneration alignment for end-to-end task-oriented dialogue system,”\\narXiv preprint arXiv:2310.08877 , 2023.\\n[79] T. Shi, L. Li, Z. Lin, T. Yang, X. Quan, and Q. Wang, “Dual-feedback\\nknowledge retrieval for task-oriented dialogue systems,” arXiv preprint\\narXiv:2310.14528, 2023.\\n[80] P. Ranade and A. Joshi, “Fabula: Intelligence report generation\\nusing retrieval-augmented narrative construction,” arXiv preprint\\narXiv:2310.13848, 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='arXiv:2310.14528, 2023.\\n[80] P. Ranade and A. Joshi, “Fabula: Intelligence report generation\\nusing retrieval-augmented narrative construction,” arXiv preprint\\narXiv:2310.13848, 2023.\\n[81] X. Jiang, R. Zhang, Y . Xu, R. Qiu, Y . Fang, Z. Wang, J. Tang,\\nH. Ding, X. Chu, J. Zhao et al. , “Think and retrieval: A hypothesis\\nknowledge graph enhanced medical large language models,” arXiv\\npreprint arXiv:2312.15883, 2023.\\n[82] J. Baek, S. Jeong, M. Kang, J. C. Park, and S. J. Hwang,\\n“Knowledge-augmented language model verification,” arXiv preprint\\narXiv:2310.12836, 2023.\\n[83] L. Luo, Y .-F. Li, G. Haffari, and S. Pan, “Reasoning on graphs: Faithful\\nand interpretable large language model reasoning,” arXiv preprint\\narXiv:2310.01061, 2023.\\n[84] X. He, Y . Tian, Y . Sun, N. V . Chawla, T. Laurent, Y . LeCun,\\nX. Bresson, and B. Hooi, “G-retriever: Retrieval-augmented generation\\nfor textual graph understanding and question answering,”arXiv preprint\\narXiv:2402.07630, 2024.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='X. Bresson, and B. Hooi, “G-retriever: Retrieval-augmented generation\\nfor textual graph understanding and question answering,”arXiv preprint\\narXiv:2402.07630, 2024.\\n[85] L. Zha, J. Zhou, L. Li, R. Wang, Q. Huang, S. Yang, J. Yuan, C. Su,\\nX. Li, A. Su et al., “Tablegpt: Towards unifying tables, nature language\\nand commands into one gpt,” arXiv preprint arXiv:2307.08674 , 2023.\\n[86] M. Gaur, K. Gunaratna, V . Srinivasan, and H. Jin, “Iseeq: Information\\nseeking question generation using dynamic meta-information retrieval\\nand knowledge graphs,” in Proceedings of the AAAI Conference on\\nArtificial Intelligence, vol. 36, no. 10, 2022, pp. 10 672–10 680.\\n[87] F. Shi, X. Chen, K. Misra, N. Scales, D. Dohan, E. H. Chi, N. Sch ¨arli,\\nand D. Zhou, “Large language models can be easily distracted by\\nirrelevant context,” in International Conference on Machine Learning .\\nPMLR, 2023, pp. 31 210–31 227.\\n[88] R. Teja, “Evaluating the ideal chunk size for a rag'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 17, 'page_label': '18', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='irrelevant context,” in International Conference on Machine Learning .\\nPMLR, 2023, pp. 31 210–31 227.\\n[88] R. Teja, “Evaluating the ideal chunk size for a rag\\nsystem using llamaindex,” https://www.llamaindex.ai/blog/\\nevaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5,\\n2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='19\\n[89] Langchain, “Recursively split by character,” https://python.langchain.\\ncom/docs/modules/data connection/document transformers/recursive\\ntext splitter, 2023.\\n[90] S. Yang, “Advanced rag 01: Small-to-\\nbig retrieval,” https://towardsdatascience.com/\\nadvanced-rag-01-small-to-big-retrieval-172181b396d4, 2023.\\n[91] Y . Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and T. Derr,\\n“Knowledge graph prompting for multi-document question answering,”\\narXiv preprint arXiv:2308.11730 , 2023.\\n[92] D. Zhou, N. Sch ¨arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schu-\\nurmans, C. Cui, O. Bousquet, Q. Le et al., “Least-to-most prompting\\nenables complex reasoning in large language models,” arXiv preprint\\narXiv:2205.10625, 2022.\\n[93] S. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz,\\nand J. Weston, “Chain-of-verification reduces hallucination in large\\nlanguage models,” arXiv preprint arXiv:2309.11495 , 2023.\\n[94] X. Li and J. Li, “Angle-optimized text embeddings,” arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='and J. Weston, “Chain-of-verification reduces hallucination in large\\nlanguage models,” arXiv preprint arXiv:2309.11495 , 2023.\\n[94] X. Li and J. Li, “Angle-optimized text embeddings,” arXiv preprint\\narXiv:2309.12871, 2023.\\n[95] V oyageAI, “V oyage’s embedding models,” https://docs.voyageai.com/\\nembeddings/, 2023.\\n[96] BAAI, “Flagembedding,” https://github.com/FlagOpen/\\nFlagEmbedding, 2023.\\n[97] P. Zhang, S. Xiao, Z. Liu, Z. Dou, and J.-Y . Nie, “Retrieve anything\\nto augment large language models,” arXiv preprint arXiv:2310.07554 ,\\n2023.\\n[98] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni,\\nand P. Liang, “Lost in the middle: How language models use long\\ncontexts,” arXiv preprint arXiv:2307.03172 , 2023.\\n[99] Y . Gao, T. Sheng, Y . Xiang, Y . Xiong, H. Wang, and J. Zhang, “Chat-\\nrec: Towards interactive and explainable llms-augmented recommender\\nsystem,” arXiv preprint arXiv:2303.14524 , 2023.\\n[100] N. Anderson, C. Wilson, and S. D. Richardson, “Lingua: Addressing'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='rec: Towards interactive and explainable llms-augmented recommender\\nsystem,” arXiv preprint arXiv:2303.14524 , 2023.\\n[100] N. Anderson, C. Wilson, and S. D. Richardson, “Lingua: Addressing\\nscenarios for live interpretation and automatic dubbing,” inProceedings\\nof the 15th Biennial Conference of the Association for Machine\\nTranslation in the Americas (Volume 2: Users and Providers Track\\nand Government Track) , J. Campbell, S. Larocca, J. Marciano,\\nK. Savenkov, and A. Yanishevsky, Eds. Orlando, USA: Association\\nfor Machine Translation in the Americas, Sep. 2022, pp. 202–209.\\n[Online]. Available: https://aclanthology.org/2022.amta-upg.14\\n[101] H. Jiang, Q. Wu, X. Luo, D. Li, C.-Y . Lin, Y . Yang, and L. Qiu,\\n“Longllmlingua: Accelerating and enhancing llms in long context\\nscenarios via prompt compression,” arXiv preprint arXiv:2310.06839 ,\\n2023.\\n[102] V . Karpukhin, B. O ˘guz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen,\\nand W.-t. Yih, “Dense passage retrieval for open-domain question'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='2023.\\n[102] V . Karpukhin, B. O ˘guz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen,\\nand W.-t. Yih, “Dense passage retrieval for open-domain question\\nanswering,” arXiv preprint arXiv:2004.04906 , 2020.\\n[103] Y . Ma, Y . Cao, Y . Hong, and A. Sun, “Large language model is\\nnot a good few-shot information extractor, but a good reranker for\\nhard samples!” ArXiv, vol. abs/2303.08559, 2023. [Online]. Available:\\nhttps://api.semanticscholar.org/CorpusID:257532405\\n[104] J. Cui, Z. Li, Y . Yan, B. Chen, and L. Yuan, “Chatlaw: Open-source\\nlegal large language model with integrated external knowledge bases,”\\narXiv preprint arXiv:2306.16092 , 2023.\\n[105] O. Yoran, T. Wolfson, O. Ram, and J. Berant, “Making retrieval-\\naugmented language models robust to irrelevant context,” arXiv\\npreprint arXiv:2310.01558, 2023.\\n[106] X. Li, R. Zhao, Y . K. Chia, B. Ding, L. Bing, S. Joty, and S. Poria,\\n“Chain of knowledge: A framework for grounding large language mod-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='preprint arXiv:2310.01558, 2023.\\n[106] X. Li, R. Zhao, Y . K. Chia, B. Ding, L. Bing, S. Joty, and S. Poria,\\n“Chain of knowledge: A framework for grounding large language mod-\\nels with structured knowledge bases,”arXiv preprint arXiv:2305.13269,\\n2023.\\n[107] H. Yang, S. Yue, and Y . He, “Auto-gpt for online decision\\nmaking: Benchmarks and additional opinions,” arXiv preprint\\narXiv:2306.02224, 2023.\\n[108] T. Schick, J. Dwivedi-Yu, R. Dess `ı, R. Raileanu, M. Lomeli, L. Zettle-\\nmoyer, N. Cancedda, and T. Scialom, “Toolformer: Language models\\ncan teach themselves to use tools,” arXiv preprint arXiv:2302.04761 ,\\n2023.\\n[109] J. Zhang, “Graph-toolformer: To empower llms with graph rea-\\nsoning ability via prompt augmented by chatgpt,” arXiv preprint\\narXiv:2304.11116, 2023.\\n[110] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,\\nC. Hesse, S. Jain, V . Kosaraju, W. Saunders et al., “Webgpt: Browser-\\nassisted question-answering with human feedback,” arXiv preprint'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='C. Hesse, S. Jain, V . Kosaraju, W. Saunders et al., “Webgpt: Browser-\\nassisted question-answering with human feedback,” arXiv preprint\\narXiv:2112.09332, 2021.\\n[111] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh,\\nC. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee et al., “Natural\\nquestions: a benchmark for question answering research,” Transactions\\nof the Association for Computational Linguistics , vol. 7, pp. 453–466,\\n2019.\\n[112] Y . Liu, S. Yavuz, R. Meng, M. Moorthy, S. Joty, C. Xiong, and Y . Zhou,\\n“Exploring the integration strategies of retriever and large language\\nmodels,” arXiv preprint arXiv:2308.12574 , 2023.\\n[113] M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer, “Triviaqa: A large\\nscale distantly supervised challenge dataset for reading comprehen-\\nsion,” arXiv preprint arXiv:1705.03551 , 2017.\\n[114] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad: 100,000+\\nquestions for machine comprehension of text,” arXiv preprint\\narXiv:1606.05250, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='[114] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad: 100,000+\\nquestions for machine comprehension of text,” arXiv preprint\\narXiv:1606.05250, 2016.\\n[115] J. Berant, A. Chou, R. Frostig, and P. Liang, “Semantic parsing on\\nfreebase from question-answer pairs,” in Proceedings of the 2013\\nconference on empirical methods in natural language processing, 2013,\\npp. 1533–1544.\\n[116] A. Mallen, A. Asai, V . Zhong, R. Das, H. Hajishirzi, and D. Khashabi,\\n“When not to trust language models: Investigating effectiveness and\\nlimitations of parametric and non-parametric memories,” arXiv preprint\\narXiv:2212.10511, 2022.\\n[117] T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder,\\nand L. Deng, “Ms marco: A human-generated machine reading com-\\nprehension dataset,” 2016.\\n[118] Z. Yang, P. Qi, S. Zhang, Y . Bengio, W. W. Cohen, R. Salakhutdi-\\nnov, and C. D. Manning, “Hotpotqa: A dataset for diverse, explain-\\nable multi-hop question answering,” arXiv preprint arXiv:1809.09600,\\n2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='nov, and C. D. Manning, “Hotpotqa: A dataset for diverse, explain-\\nable multi-hop question answering,” arXiv preprint arXiv:1809.09600,\\n2018.\\n[119] X. Ho, A.-K. D. Nguyen, S. Sugawara, and A. Aizawa, “Constructing a\\nmulti-hop qa dataset for comprehensive evaluation of reasoning steps,”\\narXiv preprint arXiv:2011.01060 , 2020.\\n[120] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Musique:\\nMultihop questions via single-hop question composition,” Transactions\\nof the Association for Computational Linguistics , vol. 10, pp. 539–554,\\n2022.\\n[121] A. Fan, Y . Jernite, E. Perez, D. Grangier, J. Weston, and M. Auli, “Eli5:\\nLong form question answering,” arXiv preprint arXiv:1907.09190 ,\\n2019.\\n[122] T. Ko ˇcisk`y, J. Schwarz, P. Blunsom, C. Dyer, K. M. Hermann, G. Melis,\\nand E. Grefenstette, “The narrativeqa reading comprehension chal-\\nlenge,” Transactions of the Association for Computational Linguistics ,\\nvol. 6, pp. 317–328, 2018.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='and E. Grefenstette, “The narrativeqa reading comprehension chal-\\nlenge,” Transactions of the Association for Computational Linguistics ,\\nvol. 6, pp. 317–328, 2018.\\n[123] K.-H. Lee, X. Chen, H. Furuta, J. Canny, and I. Fischer, “A human-\\ninspired reading agent with gist memory of very long contexts,” arXiv\\npreprint arXiv:2402.09727, 2024.\\n[124] I. Stelmakh, Y . Luan, B. Dhingra, and M.-W. Chang, “Asqa: Factoid\\nquestions meet long-form answers,” arXiv preprint arXiv:2204.06092 ,\\n2022.\\n[125] M. Zhong, D. Yin, T. Yu, A. Zaidi, M. Mutuma, R. Jha, A. H.\\nAwadallah, A. Celikyilmaz, Y . Liu, X. Qiu et al. , “Qmsum: A new\\nbenchmark for query-based multi-domain meeting summarization,”\\narXiv preprint arXiv:2104.05938 , 2021.\\n[126] P. Dasigi, K. Lo, I. Beltagy, A. Cohan, N. A. Smith, and M. Gardner,\\n“A dataset of information-seeking questions and answers anchored in\\nresearch papers,” arXiv preprint arXiv:2105.03011 , 2021.\\n[127] T. M ¨oller, A. Reina, R. Jayakumar, and M. Pietsch, “Covid-qa: A'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='“A dataset of information-seeking questions and answers anchored in\\nresearch papers,” arXiv preprint arXiv:2105.03011 , 2021.\\n[127] T. M ¨oller, A. Reina, R. Jayakumar, and M. Pietsch, “Covid-qa: A\\nquestion answering dataset for covid-19,” in ACL 2020 Workshop on\\nNatural Language Processing for COVID-19 (NLP-COVID) , 2020.\\n[128] X. Wang, G. H. Chen, D. Song, Z. Zhang, Z. Chen, Q. Xiao, F. Jiang,\\nJ. Li, X. Wan, B. Wang et al. , “Cmb: A comprehensive medical\\nbenchmark in chinese,” arXiv preprint arXiv:2308.08833 , 2023.\\n[129] H. Zeng, “Measuring massive multitask chinese understanding,” arXiv\\npreprint arXiv:2304.12986, 2023.\\n[130] R. Y . Pang, A. Parrish, N. Joshi, N. Nangia, J. Phang, A. Chen, V . Pad-\\nmakumar, J. Ma, J. Thompson, H. He et al. , “Quality: Question an-\\nswering with long input texts, yes!” arXiv preprint arXiv:2112.08608 ,\\n2021.\\n[131] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\\nand O. Tafjord, “Think you have solved question answering? try arc,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 18, 'page_label': '19', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='2021.\\n[131] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,\\nand O. Tafjord, “Think you have solved question answering? try arc,\\nthe ai2 reasoning challenge,” arXiv preprint arXiv:1803.05457 , 2018.\\n[132] A. Talmor, J. Herzig, N. Lourie, and J. Berant, “Commonsenseqa:\\nA question answering challenge targeting commonsense knowledge,”\\narXiv preprint arXiv:1811.00937 , 2018.\\n[133] E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, and J. Weston,\\n“Wizard of wikipedia: Knowledge-powered conversational agents,”\\narXiv preprint arXiv:1811.01241 , 2018.\\n[134] H. Wang, M. Hu, Y . Deng, R. Wang, F. Mi, W. Wang, Y . Wang, W.-\\nC. Kwan, I. King, and K.-F. Wong, “Large language models as source'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='20\\nplanner for personalized knowledge-grounded dialogue,” arXiv preprint\\narXiv:2310.08840, 2023.\\n[135] ——, “Large language models as source planner for personal-\\nized knowledge-grounded dialogue,” arXiv preprint arXiv:2310.08840,\\n2023.\\n[136] X. Xu, Z. Gou, W. Wu, Z.-Y . Niu, H. Wu, H. Wang, and S. Wang,\\n“Long time no see! open-domain conversation with long-term persona\\nmemory,” arXiv preprint arXiv:2203.05797 , 2022.\\n[137] T.-H. Wen, M. Gasic, N. Mrksic, L. M. Rojas-Barahona, P.-H.\\nSu, S. Ultes, D. Vandyke, and S. Young, “Conditional generation\\nand snapshot learning in neural dialogue systems,” arXiv preprint\\narXiv:1606.03352, 2016.\\n[138] R. He and J. McAuley, “Ups and downs: Modeling the visual evolution\\nof fashion trends with one-class collaborative filtering,” in proceedings\\nof the 25th international conference on world wide web , 2016, pp.\\n507–517.\\n[139] S. Li, H. Ji, and J. Han, “Document-level event argument extraction'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='of the 25th international conference on world wide web , 2016, pp.\\n507–517.\\n[139] S. Li, H. Ji, and J. Han, “Document-level event argument extraction\\nby conditional generation,” arXiv preprint arXiv:2104.05919 , 2021.\\n[140] S. Ebner, P. Xia, R. Culkin, K. Rawlins, and B. Van Durme, “Multi-\\nsentence argument linking,” arXiv preprint arXiv:1911.03766 , 2019.\\n[141] H. Elsahar, P. V ougiouklis, A. Remaci, C. Gravier, J. Hare, F. Laforest,\\nand E. Simperl, “T-rex: A large scale alignment of natural language\\nwith knowledge base triples,” in Proceedings of the Eleventh Inter-\\nnational Conference on Language Resources and Evaluation (LREC\\n2018), 2018.\\n[142] O. Levy, M. Seo, E. Choi, and L. Zettlemoyer, “Zero-shot relation ex-\\ntraction via reading comprehension,” arXiv preprint arXiv:1706.04115,\\n2017.\\n[143] R. Zellers, A. Holtzman, Y . Bisk, A. Farhadi, and Y . Choi, “Hel-\\nlaswag: Can a machine really finish your sentence?” arXiv preprint\\narXiv:1905.07830, 2019.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='2017.\\n[143] R. Zellers, A. Holtzman, Y . Bisk, A. Farhadi, and Y . Choi, “Hel-\\nlaswag: Can a machine really finish your sentence?” arXiv preprint\\narXiv:1905.07830, 2019.\\n[144] S. Kim, S. J. Joo, D. Kim, J. Jang, S. Ye, J. Shin, and M. Seo,\\n“The cot collection: Improving zero-shot and few-shot learning of\\nlanguage models via chain-of-thought fine-tuning,” arXiv preprint\\narXiv:2305.14045, 2023.\\n[145] A. Saha, V . Pahuja, M. Khapra, K. Sankaranarayanan, and S. Chandar,\\n“Complex sequential question answering: Towards learning to converse\\nover linked question answer pairs with a knowledge graph,” inProceed-\\nings of the AAAI conference on artificial intelligence , vol. 32, no. 1,\\n2018.\\n[146] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and\\nJ. Steinhardt, “Measuring massive multitask language understanding,”\\narXiv preprint arXiv:2009.03300 , 2020.\\n[147] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer sentinel'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='J. Steinhardt, “Measuring massive multitask language understanding,”\\narXiv preprint arXiv:2009.03300 , 2020.\\n[147] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer sentinel\\nmixture models,” arXiv preprint arXiv:1609.07843 , 2016.\\n[148] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant,\\n“Did aristotle use a laptop? a question answering benchmark with\\nimplicit reasoning strategies,” Transactions of the Association for\\nComputational Linguistics, vol. 9, pp. 346–361, 2021.\\n[149] J. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal, “Fever: a\\nlarge-scale dataset for fact extraction and verification,” arXiv preprint\\narXiv:1803.05355, 2018.\\n[150] N. Kotonya and F. Toni, “Explainable automated fact-checking for\\npublic health claims,” arXiv preprint arXiv:2010.09926 , 2020.\\n[151] R. Lebret, D. Grangier, and M. Auli, “Neural text generation from\\nstructured data with application to the biography domain,” arXiv\\npreprint arXiv:1603.07771, 2016.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='[151] R. Lebret, D. Grangier, and M. Auli, “Neural text generation from\\nstructured data with application to the biography domain,” arXiv\\npreprint arXiv:1603.07771, 2016.\\n[152] H. Hayashi, P. Budania, P. Wang, C. Ackerson, R. Neervannan,\\nand G. Neubig, “Wikiasp: A dataset for multi-domain aspect-based\\nsummarization,” Transactions of the Association for Computational\\nLinguistics, vol. 9, pp. 211–225, 2021.\\n[153] S. Narayan, S. B. Cohen, and M. Lapata, “Don’t give me the details,\\njust the summary! topic-aware convolutional neural networks for ex-\\ntreme summarization,” arXiv preprint arXiv:1808.08745 , 2018.\\n[154] S. Saha, J. A. Junaed, M. Saleki, A. S. Sharma, M. R. Rifat, M. Rahouti,\\nS. I. Ahmed, N. Mohammed, and M. R. Amin, “Vio-lens: A novel\\ndataset of annotated social network posts leading to different forms\\nof communal violence and its evaluation,” in Proceedings of the First\\nWorkshop on Bangla Language Processing (BLP-2023), 2023, pp. 72–\\n84.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='of communal violence and its evaluation,” in Proceedings of the First\\nWorkshop on Bangla Language Processing (BLP-2023), 2023, pp. 72–\\n84.\\n[155] X. Li and D. Roth, “Learning question classifiers,” in COLING 2002:\\nThe 19th International Conference on Computational Linguistics, 2002.\\n[156] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y . Ng,\\nand C. Potts, “Recursive deep models for semantic compositionality\\nover a sentiment treebank,” in Proceedings of the 2013 conference on\\nempirical methods in natural language processing , 2013, pp. 1631–\\n1642.\\n[157] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,\\n“Codesearchnet challenge: Evaluating the state of semantic code\\nsearch,” arXiv preprint arXiv:1909.09436 , 2019.\\n[158] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,\\nM. Plappert, J. Tworek, J. Hilton, R. Nakano et al., “Training verifiers\\nto solve math word problems,” arXiv preprint arXiv:2110.14168, 2021.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='M. Plappert, J. Tworek, J. Hilton, R. Nakano et al., “Training verifiers\\nto solve math word problems,” arXiv preprint arXiv:2110.14168, 2021.\\n[159] R. Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Erjavec, D. Tufis,\\nand D. Varga, “The jrc-acquis: A multilingual aligned parallel corpus\\nwith 20+ languages,” arXiv preprint cs/0609058 , 2006.\\n[160] Y . Hoshi, D. Miyashita, Y . Ng, K. Tatsuno, Y . Morioka, O. Torii,\\nand J. Deguchi, “Ralle: A framework for developing and eval-\\nuating retrieval-augmented large language models,” arXiv preprint\\narXiv:2308.10633, 2023.\\n[161] J. Liu, “Building production-ready rag applications,” https://www.ai.\\nengineer/summit/schedule/building-production-ready-rag-applications,\\n2023.\\n[162] I. Nguyen, “Evaluating rag part i: How to evaluate document retrieval,”\\nhttps://www.deepset.ai/blog/rag-evaluation-retrieval, 2023.\\n[163] Q. Leng, K. Uhlenhuth, and A. Polyzotis, “Best practices for\\nllm evaluation of rag applications,” https://www.databricks.com/blog/'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='https://www.deepset.ai/blog/rag-evaluation-retrieval, 2023.\\n[163] Q. Leng, K. Uhlenhuth, and A. Polyzotis, “Best practices for\\nllm evaluation of rag applications,” https://www.databricks.com/blog/\\nLLM-auto-eval-best-practices-RAG, 2023.\\n[164] S. Es, J. James, L. Espinosa-Anke, and S. Schockaert, “Ragas: Au-\\ntomated evaluation of retrieval augmented generation,” arXiv preprint\\narXiv:2309.15217, 2023.\\n[165] J. Saad-Falcon, O. Khattab, C. Potts, and M. Zaharia, “Ares: An\\nautomated evaluation framework for retrieval-augmented generation\\nsystems,” arXiv preprint arXiv:2311.09476 , 2023.\\n[166] C. Jarvis and J. Allard, “A survey of techniques for\\nmaximizing llm performance,” https://community.openai.\\ncom/t/openai-dev-day-2023-breakout-sessions/505213#\\na-survey-of-techniques-for-maximizing-llm-performance-2, 2023.\\n[167] J. Chen, H. Lin, X. Han, and L. Sun, “Benchmarking large lan-\\nguage models in retrieval-augmented generation,” arXiv preprint\\narXiv:2309.01431, 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='[167] J. Chen, H. Lin, X. Han, and L. Sun, “Benchmarking large lan-\\nguage models in retrieval-augmented generation,” arXiv preprint\\narXiv:2309.01431, 2023.\\n[168] Y . Liu, L. Huang, S. Li, S. Chen, H. Zhou, F. Meng, J. Zhou, and\\nX. Sun, “Recall: A benchmark for llms robustness against external\\ncounterfactual knowledge,” arXiv preprint arXiv:2311.08147 , 2023.\\n[169] Y . Lyu, Z. Li, S. Niu, F. Xiong, B. Tang, W. Wang, H. Wu, H. Liu,\\nT. Xu, and E. Chen, “Crud-rag: A comprehensive chinese benchmark\\nfor retrieval-augmented generation of large language models,” arXiv\\npreprint arXiv:2401.17043, 2024.\\n[170] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian,\\nE. Bakhturina, M. Shoeybi, and B. Catanzaro, “Retrieval meets long\\ncontext large language models,” arXiv preprint arXiv:2310.03025 ,\\n2023.\\n[171] C. Packer, V . Fang, S. G. Patil, K. Lin, S. Wooders, and J. E. Gon-\\nzalez, “Memgpt: Towards llms as operating systems,” arXiv preprint\\narXiv:2310.08560, 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='2023.\\n[171] C. Packer, V . Fang, S. G. Patil, K. Lin, S. Wooders, and J. E. Gon-\\nzalez, “Memgpt: Towards llms as operating systems,” arXiv preprint\\narXiv:2310.08560, 2023.\\n[172] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis, “Efficient\\nstreaming language models with attention sinks,” arXiv preprint\\narXiv:2309.17453, 2023.\\n[173] T. Zhang, S. G. Patil, N. Jain, S. Shen, M. Zaharia, I. Stoica, and J. E.\\nGonzalez, “Raft: Adapting language model to domain specific rag,”\\narXiv preprint arXiv:2403.10131 , 2024.\\n[174] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,\\nR. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws\\nfor neural language models,” arXiv preprint arXiv:2001.08361 , 2020.\\n[175] U. Alon, F. Xu, J. He, S. Sengupta, D. Roth, and G. Neubig, “Neuro-\\nsymbolic language modeling with automaton-augmented retrieval,” in\\nInternational Conference on Machine Learning . PMLR, 2022, pp.\\n468–485.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='symbolic language modeling with automaton-augmented retrieval,” in\\nInternational Conference on Machine Learning . PMLR, 2022, pp.\\n468–485.\\n[176] M. Yasunaga, A. Aghajanyan, W. Shi, R. James, J. Leskovec, P. Liang,\\nM. Lewis, L. Zettlemoyer, and W.-t. Yih, “Retrieval-augmented multi-\\nmodal language modeling,” arXiv preprint arXiv:2211.12561 , 2022.\\n[177] J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language-\\nimage pre-training with frozen image encoders and large language\\nmodels,” arXiv preprint arXiv:2301.12597 , 2023.\\n[178] W. Zhu, A. Yan, Y . Lu, W. Xu, X. E. Wang, M. Eckstein, and W. Y .\\nWang, “Visualize before you write: Imagination-guided open-ended\\ntext generation,” arXiv preprint arXiv:2210.03765 , 2022.\\n[179] J. Zhao, G. Haffar, and E. Shareghi, “Generating synthetic speech from\\nspokenvocab for speech translation,” arXiv preprint arXiv:2210.08174,\\n2022.\\n[180] D. M. Chan, S. Ghosh, A. Rastrow, and B. Hoffmeister, “Using external'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 19, 'page_label': '20', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='spokenvocab for speech translation,” arXiv preprint arXiv:2210.08174,\\n2022.\\n[180] D. M. Chan, S. Ghosh, A. Rastrow, and B. Hoffmeister, “Using external\\noff-policy speech-to-text mappings in contextual end-to-end automated\\nspeech recognition,” arXiv preprint arXiv:2301.02736 , 2023.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-03-28T00:54:45+00:00', 'author': '', 'keywords': '', 'moddate': '2024-03-28T00:54:45+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf', 'total_pages': 21, 'page': 20, 'page_label': '21', 'source_file': 'RAG_research_paper.pdf', 'file_type': 'pdf'}, page_content='21\\n[181] A. Yang, A. Nagrani, P. H. Seo, A. Miech, J. Pont-Tuset, I. Laptev,\\nJ. Sivic, and C. Schmid, “Vid2seq: Large-scale pretraining of a visual\\nlanguage model for dense video captioning,” in Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\\n2023, pp. 10 714–10 726.\\n[182] N. Nashid, M. Sintaha, and A. Mesbah, “Retrieval-based prompt\\nselection for code-related few-shot learning,” in 2023 IEEE/ACM 45th\\nInternational Conference on Software Engineering (ICSE) , 2023, pp.\\n2450–2462.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64915254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 391 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 13/13 [00:28<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (391, 384)\n",
      "Adding 391 documents to vector store...\n",
      "Successfully added 391 documents to vector store\n",
      "Total documents in collection: 782\n"
     ]
    }
   ],
   "source": [
    "### Convert the text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate the Embeddings\n",
    "\n",
    "embeddings=embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "##store int he vector dtaabase\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fa9851",
   "metadata": {},
   "source": [
    "Retriever Pipeline From VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af5a68cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60659a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x25f3a5a5e80>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0d739f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'what is Modular RAG'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_50c7ec86_265',\n",
       "  'content': 'a key strategy. This concept has been implemented in frame-\\nworks such as LlamaIndex 2, LangChain3, and HayStack [12].\\nFeeding all relevant documents directly into LLMs can lead\\nto information overload, diluting the focus on key details with\\nirrelevant content.To mitigate this, post-retrieval efforts con-\\ncentrate on selecting the essential information, emphasizing\\ncritical sections, and shortening the context to be processed.\\n2https://www.llamaindex.ai\\n3https://www.langchain.com/\\nC. Modular RAG\\nThe modular RAG architecture advances beyond the for-\\nmer two RAG paradigms, offering enhanced adaptability and\\nversatility. It incorporates diverse strategies for improving its\\ncomponents, such as adding a search module for similarity\\nsearches and refining the retriever through fine-tuning. Inno-\\nvations like restructured RAG modules [13] and rearranged\\nRAG pipelines [14] have been introduced to tackle specific\\nchallenges. The shift towards a modular RAG approach is',\n",
       "  'metadata': {'content_length': 972,\n",
       "   'subject': '',\n",
       "   'page_label': '4',\n",
       "   'total_pages': 21,\n",
       "   'author': '',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'source_file': 'RAG_research_paper.pdf',\n",
       "   'keywords': '',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'doc_index': 265,\n",
       "   'title': '',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'page': 3,\n",
       "   'creationdate': '2024-03-28T00:54:45+00:00',\n",
       "   'file_type': 'pdf',\n",
       "   'moddate': '2024-03-28T00:54:45+00:00',\n",
       "   'trapped': '/False',\n",
       "   'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf'},\n",
       "  'similarity_score': 0.13069558143615723,\n",
       "  'distance': 0.8693044185638428,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_b4334af7_265',\n",
       "  'content': 'a key strategy. This concept has been implemented in frame-\\nworks such as LlamaIndex 2, LangChain3, and HayStack [12].\\nFeeding all relevant documents directly into LLMs can lead\\nto information overload, diluting the focus on key details with\\nirrelevant content.To mitigate this, post-retrieval efforts con-\\ncentrate on selecting the essential information, emphasizing\\ncritical sections, and shortening the context to be processed.\\n2https://www.llamaindex.ai\\n3https://www.langchain.com/\\nC. Modular RAG\\nThe modular RAG architecture advances beyond the for-\\nmer two RAG paradigms, offering enhanced adaptability and\\nversatility. It incorporates diverse strategies for improving its\\ncomponents, such as adding a search module for similarity\\nsearches and refining the retriever through fine-tuning. Inno-\\nvations like restructured RAG modules [13] and rearranged\\nRAG pipelines [14] have been introduced to tackle specific\\nchallenges. The shift towards a modular RAG approach is',\n",
       "  'metadata': {'title': '',\n",
       "   'keywords': '',\n",
       "   'trapped': '/False',\n",
       "   'doc_index': 265,\n",
       "   'subject': '',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'creationdate': '2024-03-28T00:54:45+00:00',\n",
       "   'source_file': 'RAG_research_paper.pdf',\n",
       "   'content_length': 972,\n",
       "   'page_label': '4',\n",
       "   'author': '',\n",
       "   'file_type': 'pdf',\n",
       "   'total_pages': 21,\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'page': 3,\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf',\n",
       "   'moddate': '2024-03-28T00:54:45+00:00'},\n",
       "  'similarity_score': 0.13069558143615723,\n",
       "  'distance': 0.8693044185638428,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_ead5d622_257',\n",
       "  'content': 'to the user’s query. These articles, combined with the original\\nquestion, form a comprehensive prompt that empowers LLMs\\nto generate a well-informed answer.\\nThe RAG research paradigm is continuously evolving, and\\nwe categorize it into three stages: Naive RAG, Advanced\\nRAG, and Modular RAG, as showed in Figure 3. Despite\\nRAG method are cost-effective and surpass the performance\\nof the native LLM, they also exhibit several limitations.\\nThe development of Advanced RAG and Modular RAG is\\na response to these specific shortcomings in Naive RAG.\\nA. Naive RAG\\nThe Naive RAG research paradigm represents the earli-\\nest methodology, which gained prominence shortly after the',\n",
       "  'metadata': {'producer': 'pdfTeX-1.40.25',\n",
       "   'doc_index': 257,\n",
       "   'keywords': '',\n",
       "   'author': '',\n",
       "   'total_pages': 21,\n",
       "   'content_length': 670,\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'file_type': 'pdf',\n",
       "   'creationdate': '2024-03-28T00:54:45+00:00',\n",
       "   'title': '',\n",
       "   'page_label': '2',\n",
       "   'subject': '',\n",
       "   'trapped': '/False',\n",
       "   'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf',\n",
       "   'page': 1,\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'source_file': 'RAG_research_paper.pdf',\n",
       "   'moddate': '2024-03-28T00:54:45+00:00'},\n",
       "  'similarity_score': 0.10269713401794434,\n",
       "  'distance': 0.8973028659820557,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_9beeefb2_257',\n",
       "  'content': 'to the user’s query. These articles, combined with the original\\nquestion, form a comprehensive prompt that empowers LLMs\\nto generate a well-informed answer.\\nThe RAG research paradigm is continuously evolving, and\\nwe categorize it into three stages: Naive RAG, Advanced\\nRAG, and Modular RAG, as showed in Figure 3. Despite\\nRAG method are cost-effective and surpass the performance\\nof the native LLM, they also exhibit several limitations.\\nThe development of Advanced RAG and Modular RAG is\\na response to these specific shortcomings in Naive RAG.\\nA. Naive RAG\\nThe Naive RAG research paradigm represents the earli-\\nest methodology, which gained prominence shortly after the',\n",
       "  'metadata': {'title': '',\n",
       "   'author': '',\n",
       "   'page': 1,\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf',\n",
       "   'keywords': '',\n",
       "   'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'total_pages': 21,\n",
       "   'doc_index': 257,\n",
       "   'subject': '',\n",
       "   'creationdate': '2024-03-28T00:54:45+00:00',\n",
       "   'page_label': '2',\n",
       "   'source_file': 'RAG_research_paper.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 670,\n",
       "   'trapped': '/False',\n",
       "   'moddate': '2024-03-28T00:54:45+00:00',\n",
       "   'creator': 'LaTeX with hyperref'},\n",
       "  'similarity_score': 0.10269713401794434,\n",
       "  'distance': 0.8973028659820557,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_62cd233a_252',\n",
       "  'content': 'could clarify its broader trajectory. This survey endeavors to\\nfill this gap by mapping out the RAG process and charting\\nits evolution and anticipated future paths, with a focus on the\\nintegration of RAG within LLMs. This paper considers both\\ntechnical paradigms and research methods, summarizing three\\nmain research paradigms from over 100 RAG studies, and\\nanalyzing key technologies in the core stages of “Retrieval,”\\n“Generation,” and “Augmentation.” On the other hand, current\\nresearch tends to focus more on methods, lacking analysis and\\nsummarization of how to evaluate RAG. This paper compre-\\nhensively reviews the downstream tasks, datasets, benchmarks,\\nand evaluation methods applicable to RAG. Overall, this\\npaper sets out to meticulously compile and categorize the\\nfoundational technical concepts, historical progression, and\\nthe spectrum of RAG methodologies and applications that\\nhave emerged post-LLMs. It is designed to equip readers and',\n",
       "  'metadata': {'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       "   'subject': '',\n",
       "   'creationdate': '2024-03-28T00:54:45+00:00',\n",
       "   'doc_index': 252,\n",
       "   'title': '',\n",
       "   'source': '..\\\\data\\\\pdf\\\\RAG_research_paper.pdf',\n",
       "   'keywords': '',\n",
       "   'creator': 'LaTeX with hyperref',\n",
       "   'producer': 'pdfTeX-1.40.25',\n",
       "   'file_type': 'pdf',\n",
       "   'total_pages': 21,\n",
       "   'source_file': 'RAG_research_paper.pdf',\n",
       "   'moddate': '2024-03-28T00:54:45+00:00',\n",
       "   'content_length': 952,\n",
       "   'author': '',\n",
       "   'trapped': '/False',\n",
       "   'page': 0,\n",
       "   'page_label': '1'},\n",
       "  'similarity_score': 0.07119452953338623,\n",
       "  'distance': 0.9288054704666138,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"what is Modular RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94895a22",
   "metadata": {},
   "source": [
    "RAG Pipeline- VectorDB To LLM Output Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc1b5ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfa06db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "# from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0f9b0a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GroqLLM:\n",
    "    def __init__(self, model_name: str = \"llama-3.3-70b-versatile\", api_key: str = None):\n",
    "        \"\"\"\n",
    "        Initialize Groq LLM\n",
    "        \n",
    "        Args:\n",
    "            model_name: Groq model name (qwen2-72b-instruct, llama3-70b-8192, etc.)\n",
    "            api_key: Groq API key (or set GROQ_API_KEY environment variable)\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.api_key = api_key or os.environ.get(\"GROQ_API_KEY\")\n",
    "        \n",
    "        if not self.api_key:\n",
    "            raise ValueError(\n",
    "                \"Groq API key is required. \"\n",
    "                \"Set GROQ_API_KEY environment variable or pass api_key parameter.\"\n",
    "            )\n",
    "        \n",
    "        self.llm = ChatGroq(\n",
    "            groq_api_key=self.api_key,\n",
    "            model=self.model_name,\n",
    "            temperature=0.1,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        \n",
    "        print(f\"Initialized Groq LLM with model: {self.model_name}\")\n",
    "\n",
    "    def generate_response(self, query: str, context: str, max_length: int = 500) -> str:\n",
    "        \"\"\"\n",
    "        Generate response using retrieved context\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context: Retrieved document context\n",
    "            max_length: Maximum response length\n",
    "            \n",
    "        Returns:\n",
    "            Generated response string\n",
    "        \"\"\"\n",
    "        # Create prompt string directly\n",
    "        prompt = f\"\"\"You are a helpful AI assistant. Use the following context to answer the question accurately and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: Provide a clear and informative answer based on the context above. \n",
    "If the context doesn't contain enough information to answer the question, say so.\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Generate response\n",
    "            response = self.llm.invoke(prompt)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)}\"\n",
    "\n",
    "    def generate_response_simple(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Simple response generation without complex prompting\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            context: Retrieved context\n",
    "            \n",
    "        Returns:\n",
    "            Generated response\n",
    "        \"\"\"\n",
    "        simple_prompt = f\"\"\"Based on this context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        try:\n",
    "            response = self.llm.invoke(simple_prompt)\n",
    "            return response.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e8408a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Groq LLM with model: llama-3.3-70b-versatile\n",
      "Groq LLM initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    groq_llm = GroqLLM(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "    print(\"Groq LLM initialized successfully!\")\n",
    "except ValueError as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "    print(\"Please set your GROQ_API_KEY environment variable to use the LLM.\")\n",
    "    groq_llm = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4887f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple RAG pipeline with Groq LLM\n",
    "\n",
    "### Initialize the Groq LLM (set your GROQ_API_KEY in environment)\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"llama-3.3-70b-versatile\",temperature=0.3,max_tokens=1024)\n",
    "\n",
    "## 2. Simple RAG function: retrieve context + generate response\n",
    "def rag_simple(query,retriever,llm,top_k=5):\n",
    "    ## retriever the context\n",
    "    results=retriever.retrieve(query,top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "    \n",
    "    ## generate the answwer using GROQ LLM\n",
    "    prompt=f\"\"\"Use the following context to answer the question concisely.\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    \n",
    "    response=llm.invoke([prompt.format(context=context,query=query)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "071d43f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'NLP pipeline for smart healthcare'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n",
      " and  Zhiwen Yu \n",
      "Abstract\n",
      "This paper reviews the applications of natural language \n",
      "processing (NLP) in smart healthcare. It discusses the appli-\n",
      "cations of NLP in smart healthcare from both technical and \n",
      "application perspectives. After that, it discusses two speciﬁc \n",
      "medical issues, i.e., coronavirus disease 2019 (COVID-19) \n",
      "pandemic and mental health, in which NLP-driven smart \n",
      "healthcare plays an important role. Finally it discusses the \n",
      "limitations of existing works, identify the future directions \n",
      "of applying NLP to smart healthcare, and close the review \n",
      "with some conclusions.\n",
      "II. NLP FOR SMART HEALTHCARE FROM TECHNICAL \n",
      "PERSPECTIVE\n",
      "NLP has been undergoing continuous development since \n",
      "the 1950s. Studies on NLP for smart healthcare have also \n",
      "been conducted for decades and have attracted increased \n",
      "attention in recent years with the advancement of artiﬁcial \n",
      "intelligence and general NLP. To connect existing works \n",
      "from technical perspective, in this section, we ﬁrst \n",
      "introduce the three kinds of NLP approaches and their \n",
      "representative algorithms, and then introduce the NLP \n",
      "pipeline for smart healthcare to show how NLP can be \n",
      "applied to smart healthcare.\n",
      "III. NLP FOR SMART HEALTHCARE FROM APPLICATION \n",
      "PERSPECTIVE\n",
      "In this section, we discuss the applications of NLP in \n",
      "smart healthcare, including clinical decision support, \n",
      "patient data analysis, disease diagnosis, patient \n",
      "monitoring, and etc.\n",
      "What is the main topic of the paper?\n",
      "The main topic of the paper is the application of Natural Language Processing (NLP) in Smart Healthcare.\n"
     ]
    }
   ],
   "source": [
    "answer=rag_simple(\"NLP pipeline for smart healthcare\",rag_retriever,llm)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
